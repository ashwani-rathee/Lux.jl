{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lux.jl: Explicitly Parameterized Neural Networks","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the documentation of Lux!</p> <p></p> <p></p>"},{"location":"#what-is-lux","title":"What is Lux?","text":"<p><code>Lux</code> is a julia deep learning framework which decouples models and parameterization using deeply nested named tuples.</p> <ul> <li>Functional Design \u2013 Pure Functions and Deterministic Function Calls.</li> <li>No more implicit parameterization.</li> <li>Compiler and AD-friendly Neural Networks</li> </ul> <p></p> <p></p>"},{"location":"#installation-guide","title":"Installation Guide","text":"<p>Install julia v1.6 or above.</p> <pre><code>using Pkg\nPkg.add(\"Lux\")\n</code></pre> <p></p> <p></p>"},{"location":"#resources-to-get-started","title":"Resources to Get Started","text":"<ul> <li>Go through the Quickstart Example.</li> <li>Read the introductory tutorials on julia and Lux</li> <li>Go through the examples sorted based on their complexity in the documentation</li> </ul> <p>Tip</p> <p>For usage related questions, please use Github Discussions or JuliaLang Discourse (machine learning domain) which allows questions and answers to be indexed. To report bugs use github issues or even better send in a pull request.</p> <p></p> <p></p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Tip</p> <p>You need to install <code>Optimisers</code> and <code>Zygote</code> if not done already.</p> <p><code>Pkg.add([\"Optimisers\", \"Zygote\"])</code></p> <pre><code>using Lux, Random, Optimisers, Zygote\n</code></pre> <p>We take randomness very seriously</p> <pre><code># Seeding\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n</code></pre> <p>Build the model</p> <pre><code># Construct the layer\nmodel = Chain(BatchNorm(128), Dense(128, 256, tanh), BatchNorm(256),\nChain(Dense(256, 1, tanh), Dense(1, 10)))\n</code></pre> <p>Models don't hold parameters and states so initialize them. From there on, we just use our standard AD and Optimisers API.</p> <pre><code># Parameter and State Variables\nps, st = Lux.setup(rng, model) .|&gt; gpu\n\n# Dummy Input\nx = rand(rng, Float32, 128, 2) |&gt; gpu\n\n# Run the model\ny, st = Lux.apply(model, x, ps, st)\n\n# Gradients\n## Pullback API to capture change in state\n(l, st_), pb = pullback(p -&gt; Lux.apply(model, x, p, st), ps)\ngs = pb((one.(l), nothing))[1]\n\n# Optimization\nst_opt = Optimisers.setup(Optimisers.ADAM(0.0001), ps)\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\n</code></pre> <p></p> <p></p>"},{"location":"#how-the-documentation-is-structured","title":"How the documentation is structured","text":"<p>Having a high-level overview of how this documentation is structured will help you know where to look for certain things.</p> <ul> <li><code>Introduction</code> \u2013 Talks about why we wrote Lux and has pointers to frameworks in the extended julia ecosystem which might help users to get started with deep learning</li> <li><code>Tutorials</code> \u2013 Contain tutorials of varying complexity. These contain worked examples of solving problems with Lux. Start here if you are new to Lux, or you have a particular problem class you want to model.</li> <li><code>Manual</code> \u2013 Contains guides to some common problems encountered by users.</li> <li><code>API Reference</code> \u2013 Contains a complete list of the functions you can use in Lux. Look here if you want to know how to use a particular function.</li> <li><code>Development Documentation</code> \u2013 Contains information for people contributing to Lux development or writing Lux extensions. Don't worry about this section if you are using Lux to formulate and solve problems as a user.</li> </ul> <p></p> <p></p>"},{"location":"#citation","title":"Citation","text":"<p>If you found this library to be useful in academic work, then please cite:</p> <pre><code>@misc{pal2022lux,\nauthor = {Pal, Avik},\ntitle = {Lux: Explicit Parameterization of Deep Neural Networks in Julia},\nyear = {2022},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/avik-pal/Lux.jl/}}\n}\n</code></pre> <p>Also consider starring our github repo</p>"},{"location":"api/contrib/","title":"Experimental","text":"<p>All features listed on this page are experimental which means:</p> <ol> <li>No SemVer Guarantees. We use code here to iterate fast and most users should wait for these features to be marked non-experimental.</li> <li>The code will probably be moved into a separate repository in the future.</li> <li>Expect edge-cases and report them. It will help us move these features out of experimental sooner.</li> <li>None of the features are exported.</li> </ol> <p></p> <p></p>"},{"location":"api/contrib/#training","title":"Training","text":"<p>Note</p> <p>This module will be moved into a separate package in the near future.</p> <p>Helper Functions making it easier to train <code>Lux.jl</code> models.</p> <p>Lux.Training is meant to be simple and provide extremely basic functionality. We provide basic building blocks which can be seamlessly composed to create complex training pipelines.</p> <p># <code>Lux.Training.AbstractVJP</code> \u2014 Type.</p> <pre><code>AbstractVJP\n</code></pre> <p>Base Type for all Vector-Jacobian Product Backends.</p> <p>source</p> <p># <code>Lux.Training.backend</code> \u2014 Function.</p> <pre><code>backend(::AbstractVJP)\n</code></pre> <p>Package used to compute the VJP.</p> <p>source</p> <p># <code>Lux.Training.EnzymeVJP</code> \u2014 Type.</p> <pre><code>EnzymeVJP &lt;: AbstractVJP\n</code></pre> <p>Vector-Jacobian Product using Enzyme.</p> <p>source</p> <p># <code>Lux.Training.YotaVJP</code> \u2014 Type.</p> <pre><code>YotaVJP &lt;: AbstractVJP\n</code></pre> <p>Vector-Jacobian Product using Yota.</p> <p>source</p> <p># <code>Lux.Training.ZygoteVJP</code> \u2014 Type.</p> <pre><code>ZygoteVJP &lt;: AbstractVJP\n</code></pre> <p>Vector-Jacobian Product using Zygote.</p> <p>source</p> <p># <code>Lux.Training.TrainState</code> \u2014 Type.</p> <pre><code>TrainState\n</code></pre> <p>Training State containing:</p> <ul> <li><code>model</code>: <code>Lux</code> model.</li> <li><code>parameters</code>: Trainable Variables of the <code>model</code>.</li> <li><code>states</code>: Non-trainable Variables of the <code>model</code>.</li> <li><code>optimizer_state</code>: Optimizer State.</li> <li><code>step</code>: Number of updates of the parameters made.</li> </ul> <p>source</p> <p># <code>Lux.Training.compute_gradients</code> \u2014 Function.</p> <pre><code>compute_gradients(vjp::AbstractVJP, objective_function::Function, data, ts::TrainState)\n</code></pre> <p>Compute the gradients of the objective function wrt parameters stored in <code>ts</code>.</p> <p>Arguments</p> <ul> <li><code>vjp</code>: Backend used to compute the gradients. See <code>AbstractVJP</code>.</li> <li><code>objective_function</code>: Objective function. The function must take 4 inputs \u2013 model, parameters, states and data. The function must return 3 values \u2013 loss, updated_state, and any computed statistics.</li> <li><code>data</code>: Data used to compute the gradients.</li> <li><code>ts</code>: Current Training State. See <code>TrainState</code>.</li> </ul> <p>Return</p> <p>A 4-Tuple containing:</p> <ul> <li><code>grads</code>: Computed Gradients.</li> <li><code>loss</code>: Loss from the objective function.</li> <li><code>stats</code>: Any computed statistics from the objective function.</li> <li><code>ts</code>: Updated Training State.</li> </ul> <p>source</p> <p># <code>Lux.Training.apply_gradients</code> \u2014 Function.</p> <pre><code>apply_gradients(ts::TrainState, grads)\n</code></pre> <p>Update the parameters stored in <code>ts</code> using the gradients <code>grads</code>.</p> <p>Arguments</p> <ul> <li><code>ts</code>: <code>TrainState</code> object.</li> <li><code>grads</code>: Gradients of the loss function wrt <code>ts.params</code>.</li> </ul> <p>Returns</p> <p>Updated <code>TrainState</code> object.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/contrib/#why-vjp-rules","title":"Why VJP rules ?","text":"<p>In the long term, the goal is to just use AbstractDifferentiation.jl? directly. However, there are current refactors being planned (See this issue). Once the package is stable and we have the necessary backend support, we will be dropping the VJP rules in this module.</p> <p></p> <p></p>"},{"location":"api/contrib/#parameter-freezing","title":"Parameter Freezing","text":"<p>Note</p> <p>In the long term, this will be supported via Optimisers.jl.</p> <p># <code>Lux.FrozenLayer</code> \u2014 Type.</p> <pre><code>FrozenLayer(l::AbstractExplicitLayer, which_params::Union{Tuple, Nothing})\n</code></pre> <p>Freeze the parameters with name <code>which_params</code> of the layer <code>l</code>.</p> <p>Tip</p> <p>It is always recommended to use the <code>Lux.freeze</code> function instead of directly using the <code>FrozenLayer</code> constructor.</p> <p>Warning</p> <p>There are no checks for <code>which_params</code>. For example, if the original layer has parameters named <code>(:weight, :bias)``, and</code>which_params<code>is set to</code>(:myweight,)` then none of the parameters are frozen and no error is thrown.</p> <p>Arguments</p> <ul> <li><code>l</code>: Lux AbstractExplicitLayer.</li> <li><code>which_params</code>: Parameter Names to be Frozen. Can be set to <code>nothing</code>, in which case all parameters are frozen.</li> </ul> <p>Input</p> <ul> <li><code>x</code>: Input to the layer <code>l</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of the inner layer <code>l</code></li> <li>Updated State</li> </ul> <p>Parameters</p> <ul> <li>Parameters of the layer <code>l</code> excluding <code>which_params</code>.</li> </ul> <p>States</p> <ul> <li><code>frozen_params</code>: Parameters that are frozen, i.e., <code>which_params</code>.</li> <li><code>states</code>: The state of the inner layer <code>l</code>.</li> </ul> <p>Note on Internal Layer Implementation</p> <p>The inner layer should work with <code>NamedTuple</code> parameters. In order to support custom parameter types, users need to implement <code>Lux._merge(::CustomParamType, ::NamedTuple)</code>.</p> <p>Example</p> <pre><code>m = Lux.FrozenLayer(Dense(2 =&gt; 2), (:weight,))\n</code></pre> <p>See also <code>Lux.freeze</code>, <code>Lux.unfreeze</code>.</p> <p>source</p> <p># <code>Lux.freeze</code> \u2014 Function.</p> <pre><code>freeze(l::AbstractExplicitLayer, which_params::Union{Tuple, Nothing} = nothing)\n</code></pre> <p>Constructs a version of <code>l</code> with <code>which_params</code> frozen. If <code>which_params</code> is nothing, then all parameters are frozen.</p> <p>source</p> <pre><code>freeze(l::AbstractExplicitLayer, ps, st::NamedTuple,\n       which_params::Union{Tuple, Nothing} = nothing)\n</code></pre> <p>Construct a <code>Lux.FrozenLayer</code> for <code>l</code> with the current parameters and states. If <code>which_params</code> is nothing, then all parameters are frozen.</p> <p>source</p> <p># <code>Lux.unfreeze</code> \u2014 Function.</p> <pre><code>unfreeze(l::FrozenLayer)\n</code></pre> <p>Unfreezes the layer <code>l</code>.</p> <p>source</p> <pre><code>unfreeze(l::FrozenLayer, ps, st::NamedTuple)\n</code></pre> <p>Unwraps a <code>Lux.FrozenLayer</code> <code>l</code> with the current parameters and states.</p> <p>source</p> <p>For detailed usage example look at the manual page.</p> <p></p> <p></p>"},{"location":"api/contrib/#map-over-layer","title":"Map over Layer","text":"<p># <code>Lux.layer_map</code> \u2014 Function.</p> <pre><code>layer_map(f::Function, l::AbstractExplicitLayer, ps, st::NamedTuple,\nname::String=\"model\")\n</code></pre> <p>Map the function <code>f</code> over the model <code>l</code>, with the parameters <code>ps</code> and states <code>st</code>. This is different from <code>Functors.fmap</code> since it zips the layers, parameters, and states and invokes the function on all of them together.</p> <p>Call Signature for <code>f</code></p> <ul> <li>Must take 4 inputs \u2013 <code>AbstractExplicitLayer</code>, Corresponding Parameters, Corresponding States, and the name of the layer.</li> <li>Must return a tuple of 3 elements \u2013 <code>AbstractExplicitLayer</code>, new parameters and the new states.</li> </ul> <p>Tip</p> <p>We recommend using the macro <code>Lux.@layer_map</code> instead of this function. It automatically sets the <code>name</code> of the layer to be the variable name.</p> <p>Example</p> <pre><code>using Lux, Random, Setfield\n\nc = Parallel(+; chain=Chain(; dense_1=Dense(2 =&gt; 3), bn=BatchNorm(3),\ndense_2=Dense(3 =&gt; 5)),\ndense_3=Dense(5 =&gt; 1))\n\nrng = Random.default_rng()\nps, st = Lux.setup(rng, c)\n\n# Makes parameters of Dense Layers inside Chain zero\nfunction zero_dense_params(l, ps, st, name)\nif l isa Dense\nprintln(\"zeroing params of name\")\n@set! ps.weight = zero.(ps.weight)\n@set! ps.bias = zero.(ps.bias)\nend\nreturn l, ps, st\nend\n\nLux.layer_map(zero_dense_params, c, ps, st)\n</code></pre> <p>source</p> <p># <code>Lux.@layer_map</code> \u2014 Macro.</p> <pre><code>@layer_map func layer ps st\n</code></pre> <p>See the documentation of <code>Lux.layer_map</code> for more details. This macro eliminates the need to the set the layer name, and uses the variable name as the starting point.</p> <p>Example</p> <pre><code>using Lux, Random, Setfield\n\nc = Parallel(+; chain=Chain(; dense_1=Dense(2 =&gt; 3), bn=BatchNorm(3),\ndense_2=Dense(3 =&gt; 5)),\ndense_3=Dense(5 =&gt; 1))\n\nrng = Random.default_rng()\nps, st = Lux.setup(rng, c)\n\n# Makes parameters of Dense Layers inside Chain zero\nfunction zero_dense_params(l, ps, st, name)\nif l isa Dense\nprintln(\"zeroing params of name\")\n@set! ps.weight = zero.(ps.weight)\n@set! ps.bias = zero.(ps.bias)\nend\nreturn l, ps, st\nend\n\nLux.@layer_map zero_dense_params c ps st\n</code></pre> <p>source</p> <p></p> <p></p>"},{"location":"api/contrib/#tied-parameters","title":"Tied Parameters","text":"<p># <code>Lux.share_parameters</code> \u2014 Function.</p> <pre><code>share_parameters(ps, sharing)\nshare_parameters(ps, sharing, new_parameters)\n</code></pre> <p>Updates the parameters in <code>ps</code> with a common set of parameters <code>new_parameters</code> that are shared between each list in the nested list <code>sharing</code>. (That was kind of a mouthful, the example should make it clear).</p> <p>Arguments</p> <ul> <li><code>ps</code>: Original parameters.</li> <li><code>sharing</code>: A nested list of lists of accessors of <code>ps</code> which need to shate the parameters (See the example for details). (Each list in the list must be disjoint)</li> <li><code>new_parameters</code>: If passed the length of <code>new_parameters</code> must be equal to the length of <code>sharing</code>. For each vector in <code>sharing</code> the corresponding parameter in <code>new_parameters</code> will be used. (If not passed, the parameters corresponding to the first element of each vector in <code>sharing</code> will be used).</li> </ul> <p>Returns</p> <p>Updated Parameters having the same structure as <code>ps</code>.</p> <p>Example</p> <pre><code>model = Chain(; d1=Dense(2 =&gt; 4, tanh), d3=Chain(; l1=Dense(4 =&gt; 2), l2=Dense(2 =&gt; 4)),\nd2=Dense(4 =&gt; 2))\n\nps, st = Lux.setup(Xoshiro(0), model)\n\n# share parameters of (d1 and d3.l1) and (d3.l2 and d2)\nps = Lux.share_parameters(ps, ((\"d3.l2\", \"d1\"), (\"d2\", \"d3.l1\")))\n</code></pre> <p>source</p> <p></p> <p></p>"},{"location":"api/contrib/#index","title":"Index","text":"<ul> <li><code>Lux.FrozenLayer</code></li> <li><code>Lux.Training.AbstractVJP</code></li> <li><code>Lux.Training.EnzymeVJP</code></li> <li><code>Lux.Training.TrainState</code></li> <li><code>Lux.Training.YotaVJP</code></li> <li><code>Lux.Training.ZygoteVJP</code></li> <li><code>Lux.Training.apply_gradients</code></li> <li><code>Lux.Training.backend</code></li> <li><code>Lux.Training.compute_gradients</code></li> <li><code>Lux.freeze</code></li> <li><code>Lux.layer_map</code></li> <li><code>Lux.share_parameters</code></li> <li><code>Lux.unfreeze</code></li> <li><code>Lux.@layer_map</code></li> </ul>"},{"location":"api/core/","title":"Core","text":"<p>The documentation for this page has been moved to LuxCore.jl. However, this the functionality has in no way been deprecated and there is no plan to deprecated access to these functionalities in the future. The change was made merely to allow lighter dependencies for users extending <code>Lux.jl</code>.</p>"},{"location":"api/functional/","title":"Functional","text":""},{"location":"api/functional/#functional-layers","title":"Functional Layers","text":"<p>Note</p> <p>These functions expose the backend of <code>Lux.jl</code>. In the long-term we plan to move these into NNlib</p> <p># <code>Lux.dropout</code> \u2014 Function.</p> <pre><code>dropout(rng::AbstractRNG, x, p, q, dims, ::Val{training})\ndropout(rng::AbstractRNG, x, mask, p, q, dims, t::Val{training}, ::Val{update_mask})\n</code></pre> <p>If <code>training</code> then dropout is applied on <code>x</code> with probability <code>p</code> along <code>dims</code>. If <code>mask</code> is passed it is used if <code>update_mask</code> is false. If <code>update_mask</code> is true then the mask is generated and used.</p> <p>Warning</p> <p>This function has been deprecated and will be removed in v0.5. Use <code>LuxLib.dropout</code> instead.</p> <p>source</p> <p># <code>Lux.normalization</code> \u2014 Function.</p> <pre><code>normalization(x, running_mean, running_var, scale, bias, activation, reduce_dims,\n::Val{training}, momentum, epsilon)\n</code></pre> <p>Performs BatchNorm/GroupNorm based on input configuration</p> <p>Warning</p> <p>This function has been deprecated and will be removed in v0.5. Use <code>LuxLib.(batch/group)norm</code> instead.</p> <p>source</p>"},{"location":"api/layers/","title":"Layers","text":""},{"location":"api/layers/#containers","title":"Containers","text":"<p># <code>Lux.BranchLayer</code> \u2014 Type.</p> <pre><code>BranchLayer(layers...)\nBranchLayer(; layers...)\n</code></pre> <p>Takes an input <code>x</code> and passes it through all the <code>layers</code> and returns a tuple of the outputs.</p> <p>Arguments</p> <ul> <li> <p>Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Will be directly passed to each of the <code>layers</code></li> </ul> <p>Returns</p> <ul> <li>Tuple: <code>(layer_1(x), layer_2(x), ..., layer_N(x))</code>  (naming changes if using the kwargs API)</li> <li>Updated state of the <code>layers</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>Comparison with Parallel</p> <p>This is slightly different from <code>Parallel(nothing, layers...)</code></p> <ul> <li>If the input is a tuple, <code>Parallel</code> will pass each element individually to each layer.</li> <li><code>BranchLayer</code> essentially assumes 1 input comes in and is branched out into <code>N</code> outputs.</li> </ul> <p>Example</p> <p>An easy way to replicate an input to an NTuple is to do</p> <pre><code>l = BranchLayer(NoOpLayer(), NoOpLayer(), NoOpLayer())\n</code></pre> <p>source</p> <p># <code>Lux.Chain</code> \u2014 Type.</p> <pre><code>Chain(layers...; disable_optimizations::Bool = false)\nChain(; layers..., disable_optimizations::Bool = false)\n</code></pre> <p>Collects multiple layers / functions to be called in sequence on a given input.</p> <p>Arguments</p> <ul> <li> <p>Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Keyword Arguments</p> <ul> <li><code>disable_optimizations</code>: Prevents any structural optimization</li> </ul> <p>Inputs</p> <p>Input <code>x</code> is passed sequentially to each layer, and must conform to the input requirements of the internal layers.</p> <p>Returns</p> <ul> <li>Output after sequentially applying all the layers to <code>x</code></li> <li>Updated model states</li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>Optimizations</p> <p>Performs a few optimizations to generate reasonable architectures. Can be disabled using keyword argument <code>disable_optimizations</code>.</p> <ul> <li>All sublayers are recursively optimized.</li> <li>If a function <code>f</code> is passed as a layer and it doesn't take 3 inputs, it is converted to a <code>WrappedFunction</code>(<code>f</code>) which takes only one input.</li> <li>If the layer is a Chain, it is flattened.</li> <li><code>NoOpLayer</code>s are removed.</li> <li>If there is only 1 layer (left after optimizations), then it is returned without the <code>Chain</code> wrapper.</li> <li>If there are no layers (left after optimizations), a <code>NoOpLayer</code> is returned.</li> </ul> <p>Miscellaneous Properties</p> <ul> <li>Allows indexing. We can access the <code>i</code>th layer using <code>m[i]</code>. We can also index using ranges or arrays.</li> </ul> <p>Example</p> <pre><code>c = Chain(Dense(2, 3, relu), BatchNorm(3), Dense(3, 2))\n</code></pre> <p>source</p> <p># <code>Lux.PairwiseFusion</code> \u2014 Type.</p> <pre><code>PairwiseFusion(connection, layers...)\nPairwiseFusion(connection; layers...)\n</code></pre> <pre><code>x1 \u2192 layer1 \u2192 y1 \u2198\n                  connection \u2192 layer2 \u2192 y2 \u2198\n              x2 \u2197                          connection \u2192 y3\n                                        x3 \u2197\n</code></pre> <p>Arguments</p> <ul> <li><code>connection</code>: Takes 2 inputs and combines them</li> <li> <p><code>layers</code>: <code>AbstractExplicitLayer</code>s. Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Inputs</p> <p>Layer behaves differently based on input type:</p> <ol> <li>If the input <code>x</code> is a tuple of length <code>N + 1</code>, then the <code>layers</code> must be a tuple of length <code>N</code>. The computation is as follows</li> </ol> <pre><code>y = x[1]\nfor i in 1:N\ny = connection(x[i + 1], layers[i](y))\nend\n</code></pre> <ol> <li>Any other kind of input</li> </ol> <pre><code>y = x\nfor i in 1:N\ny = connection(x, layers[i](y))\nend\n</code></pre> <p>Returns</p> <ul> <li>See Inputs section for how the return value is computed</li> <li>Updated model state for all the contained layers</li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>source</p> <p># <code>Lux.Parallel</code> \u2014 Type.</p> <pre><code>Parallel(connection, layers...)\nParallel(connection; layers...)\n</code></pre> <p>Create a layer which passes an input to each path in <code>layers</code>, before reducing the output with <code>connection</code>.</p> <p>Arguments</p> <ul> <li><code>connection</code>: An <code>N</code>-argument function that is called after passing the input through each layer. If <code>connection = nothing</code>, we return a tuple <code>Parallel(nothing, f, g)(x, y) = (f(x), g(y))</code></li> <li> <p>Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: If <code>x</code> is not a tuple, then return is computed as <code>connection([l(x) for l in layers]...)</code>. Else one is passed to each layer, thus <code>Parallel(+, f, g)(x, y) = f(x) + g(y)</code>.</li> </ul> <p>Returns</p> <ul> <li>See the Inputs section for how the output is computed</li> <li>Updated state of the <code>layers</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>See also <code>SkipConnection</code> which is <code>Parallel</code> with one identity.</p> <p>source</p> <p># <code>Lux.SkipConnection</code> \u2014 Type.</p> <pre><code>SkipConnection(layer, connection)\n</code></pre> <p>Create a skip connection which consists of a layer or <code>Chain</code> of consecutive layers and a shortcut connection linking the block's input to the output through a user-supplied 2-argument callable. The first argument to the callable will be propagated through the given <code>layer</code> while the second is the unchanged, \"skipped\" input.</p> <p>The simplest \"ResNet\"-type connection is just <code>SkipConnection(layer, +)</code>.</p> <p>Arguments</p> <ul> <li><code>layer</code>: Layer or <code>Chain</code> of layers to be applied to the input</li> <li> <p><code>connection</code>:</p> <ul> <li>A 2-argument function that takes <code>layer(input)</code> and the input OR</li> <li>An AbstractExplicitLayer that takes <code>(layer(input), input)</code> as input</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Will be passed directly to <code>layer</code></li> </ul> <p>Returns</p> <ul> <li>Output of <code>connection(layer(input), input)</code></li> <li>Updated state of <code>layer</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of <code>layer</code> OR</li> <li>If <code>connection</code> is an AbstractExplicitLayer, then NamedTuple with fields <code>:layers</code> and <code>:connection</code></li> </ul> <p>States</p> <ul> <li>States of <code>layer</code> OR</li> <li>If <code>connection</code> is an AbstractExplicitLayer, then NamedTuple with fields <code>:layers</code> and <code>:connection</code></li> </ul> <p>See <code>Parallel</code> for a more general implementation.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#convolutional-layers","title":"Convolutional Layers","text":"<p># <code>Lux.Conv</code> \u2014 Type.</p> <pre><code>Conv(k::NTuple{N,Integer}, (in_chs =&gt; out_chs)::Pair{&lt;:Integer,&lt;:Integer},\nactivation=identity; init_weight=glorot_uniform, init_bias=zeros32, stride=1,\npad=0, dilation=1, groups=1, use_bias=true)\n</code></pre> <p>Standard convolutional layer.</p> <p>Image data should be stored in WHCN order (width, height, channels, batch). In other words, a <code>100 x 100</code> RGB image would be a <code>100 x 100 x 3 x 1</code> array, and a batch of 50 would be a <code>100 x 100 x 3 x 50</code> array. This has <code>N = 2</code> spatial dimensions, and needs a kernel size like <code>(5, 5)</code>, a 2-tuple of integers. To take convolutions along <code>N</code> feature dimensions, this layer expects as input an array with <code>ndims(x) == N + 2</code>, where <code>size(x, N + 1) == in_chs</code> is the number of input channels, and <code>size(x, ndims(x))</code> is the number of observations in a batch.</p> <p>Note</p> <p>Frameworks like <code>Pytorch</code> perform cross-correlation in their convolution layers</p> <p>Arguments</p> <ul> <li><code>k</code>: Tuple of integers specifying the size of the convolutional kernel. Eg, for 2D      convolutions <code>length(k) == 2</code></li> <li><code>in_chs</code>: Number of input channels</li> <li><code>out_chs</code>: Number of input and output channels</li> <li><code>activation</code>: Activation Function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: Controls the initialization of the weight parameter</li> <li><code>init_bias</code>: Controls the initialization of the bias parameter</li> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li><code>dilation</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> <li><code>groups</code>: Expected to be an <code>Int</code>. It specifies the number of groups to divide a           convolution into (set <code>groups = in_chs</code> for Depthwise Convolutions). <code>in_chs</code>           and <code>out_chs</code> must be divisible by <code>groups</code>.</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code>.</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2 &amp;&amp; size(x, N - 1) == in_chs</code>, i.e.      <code>size(x) = (I_N, ..., I_1, C_in, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the convolution <code>y</code> of size <code>(O_N, ..., O_1, C_out, N)</code> where</li> </ul> \\[ O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Convolution kernel</li> <li><code>bias</code>: Bias (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.ConvTranspose</code> \u2014 Type.</p> <pre><code>ConvTranspose(k::NTuple{N,Integer}, (in_chs =&gt; out_chs)::Pair{&lt;:Integer,&lt;:Integer},\nactivation=identity; init_weight=glorot_uniform, init_bias=zeros32,\nstride=1, pad=0, dilation=1, groups=1, use_bias=true)\n</code></pre> <p>Standard convolutional transpose layer.</p> <p>Arguments</p> <ul> <li><code>k</code>: Tuple of integers specifying the size of the convolutional kernel. Eg, for 2D      convolutions <code>length(k) == 2</code></li> <li><code>in_chs</code>: Number of input channels</li> <li><code>out_chs</code>: Number of input and output channels</li> <li><code>activation</code>: Activation Function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: Controls the initialization of the weight parameter</li> <li><code>init_bias</code>: Controls the initialization of the bias parameter</li> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li><code>dilation</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) * stride</code> (possibly rounded) for each spatial dimension.</li> <li><code>groups</code>: Expected to be an <code>Int</code>. It specifies the number of groups to divide a           convolution into (set <code>groups = in_chs</code> for Depthwise Convolutions). <code>in_chs</code>           and <code>out_chs</code> must be divisible by <code>groups</code>.</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code>.</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2 &amp;&amp; size(x, N - 1) == in_chs</code>, i.e.      <code>size(x) = (I_N, ..., I_1, C_in, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the convolution transpose <code>y</code> of size <code>(O_N, ..., O_1, C_out, N)</code> where</li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Convolution Transpose kernel</li> <li><code>bias</code>: Bias (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.CrossCor</code> \u2014 Type.</p> <pre><code>CrossCor(k::NTuple{N,Integer}, (in_chs =&gt; out_chs)::Pair{&lt;:Integer,&lt;:Integer},\nactivation=identity; init_weight=glorot_uniform, init_bias=zeros32, stride=1,\npad=0, dilation=1, use_bias=true)\n</code></pre> <p>Cross Correlation layer.</p> <p>Image data should be stored in WHCN order (width, height, channels, batch). In other words, a <code>100 x 100</code> RGB image would be a <code>100 x 100 x 3 x 1</code> array, and a batch of 50 would be a <code>100 x 100 x 3 x 50</code> array. This has <code>N = 2</code> spatial dimensions, and needs a kernel size like <code>(5, 5)</code>, a 2-tuple of integers. To take convolutions along <code>N</code> feature dimensions, this layer expects as input an array with <code>ndims(x) == N + 2</code>, where <code>size(x, N + 1) == in_chs</code> is the number of input channels, and <code>size(x, ndims(x))</code> is the number of observations in a batch.</p> <p>Arguments</p> <ul> <li><code>k</code>: Tuple of integers specifying the size of the convolutional kernel. Eg, for 2D      convolutions <code>length(k) == 2</code></li> <li><code>in_chs</code>: Number of input channels</li> <li><code>out_chs</code>: Number of input and output channels</li> <li><code>activation</code>: Activation Function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: Controls the initialization of the weight parameter</li> <li><code>init_bias</code>: Controls the initialization of the bias parameter</li> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li><code>dilation</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code>.</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2 &amp;&amp; size(x, N - 1) == in_chs</code>, i.e.      <code>size(x) = (I_N, ..., I_1, C_in, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the convolution <code>y</code> of size <code>(O_N, ..., O_1, C_out, N)</code> where</li> </ul> \\[ O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Convolution kernel</li> <li><code>bias</code>: Bias (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#dropout-layers","title":"Dropout Layers","text":"<p># <code>Lux.Dropout</code> \u2014 Type.</p> <pre><code>Dropout(p; dims=:)\n</code></pre> <p>Dropout layer.</p> <p>Arguments</p> <ul> <li><code>p</code>: Probability of Dropout (if <code>p = 0</code> then <code>NoOpLayer</code> is returned)</li> </ul> <p>Keyword Arguments</p> <ul> <li>To apply dropout along certain dimension(s), specify the <code>dims</code> keyword. e.g. <code>Dropout(p; dims = 3)</code> will randomly zero out entire channels on WHCN input (also called 2D dropout).</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Must be an AbstractArray</li> </ul> <p>Returns</p> <ul> <li><code>x</code> with dropout mask applied if <code>training=Val(true)</code> else just <code>x</code></li> <li>State with updated <code>rng</code></li> </ul> <p>States</p> <ul> <li><code>rng</code>: Pseudo Random Number Generator</li> <li><code>training</code>: Used to check if training/inference mode</li> </ul> <p>Call <code>Lux.testmode</code> to switch to test mode.</p> <p>See also <code>AlphaDropout</code>, <code>VariationalHiddenDropout</code></p> <p>source</p> <p># <code>Lux.VariationalHiddenDropout</code> \u2014 Type.</p> <pre><code>VariationalHiddenDropout(p; dims=:)\n</code></pre> <p>VariationalHiddenDropout layer. The only difference from Dropout is that the <code>mask</code> is retained until <code>Lux.update_state(l, :update_mask, Val(true))</code> is called.</p> <p>Arguments</p> <ul> <li><code>p</code>: Probability of Dropout (if <code>p = 0</code> then <code>NoOpLayer</code> is returned)</li> </ul> <p>Keyword Arguments</p> <ul> <li>To apply dropout along certain dimension(s), specify the <code>dims</code> keyword. e.g. <code>VariationalHiddenDropout(p; dims = 3)</code> will randomly zero out entire channels on WHCN input (also called 2D dropout).</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Must be an AbstractArray</li> </ul> <p>Returns</p> <ul> <li><code>x</code> with dropout mask applied if <code>training=Val(true)</code> else just <code>x</code></li> <li>State with updated <code>rng</code></li> </ul> <p>States</p> <ul> <li><code>rng</code>: Pseudo Random Number Generator</li> <li><code>training</code>: Used to check if training/inference mode</li> <li><code>mask</code>: Dropout mask. Initilly set to nothing. After every run, contains the mask applied in that call</li> <li><code>update_mask</code>: Stores whether new mask needs to be generated in the current call</li> </ul> <p>Call <code>Lux.testmode</code> to switch to test mode.</p> <p>See also <code>AlphaDropout</code>, <code>Dropout</code></p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#pooling-layers","title":"Pooling Layers","text":"<p># <code>Lux.AdaptiveMaxPool</code> \u2014 Type.</p> <pre><code>AdaptiveMaxPool(out::NTuple)\n</code></pre> <p>Adaptive Max Pooling layer. Calculates the necessary window size such that its output has <code>size(y)[1:N] == out</code>.</p> <p>Arguments</p> <ul> <li><code>out</code>: Size of the first <code>N</code> dimensions for the output</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Expects as input an array with <code>ndims(x) == N+2</code>, i.e. channel and batch dimensions, after the <code>N</code> feature dimensions, where <code>N = length(out)</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of size <code>(out..., C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MaxPool</code>, <code>AdaptiveMeanPool</code>.</p> <p>source</p> <p># <code>Lux.AdaptiveMeanPool</code> \u2014 Type.</p> <pre><code>AdaptiveMeanPool(out::NTuple)\n</code></pre> <p>Adaptive Mean Pooling layer. Calculates the necessary window size such that its output has <code>size(y)[1:N] == out</code>.</p> <p>Arguments</p> <ul> <li><code>out</code>: Size of the first <code>N</code> dimensions for the output</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Expects as input an array with <code>ndims(x) == N+2</code>, i.e. channel and batch dimensions, after the <code>N</code> feature dimensions, where <code>N = length(out)</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of size <code>(out..., C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MeanPool</code>, <code>AdaptiveMaxPool</code>.</p> <p>source</p> <p># <code>Lux.GlobalMaxPool</code> \u2014 Type.</p> <pre><code>GlobalMaxPool()\n</code></pre> <p>Global Mean Pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing max pooling on the complete (w,h)-shaped feature maps.</p> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) &gt; 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(1, ..., 1, C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MaxPool</code>, <code>AdaptiveMaxPool</code>, <code>GlobalMeanPool</code></p> <p>source</p> <p># <code>Lux.GlobalMeanPool</code> \u2014 Type.</p> <pre><code>GlobalMeanPool()\n</code></pre> <p>Global Mean Pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing mean pooling on the complete (w,h)-shaped feature maps.</p> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) &gt; 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(1, ..., 1, C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MeanPool</code>, <code>AdaptiveMeanPool</code>, <code>GlobalMaxPool</code></p> <p>source</p> <p># <code>Lux.MaxPool</code> \u2014 Type.</p> <pre><code>MaxPool(window::NTuple; pad=0, stride=window)\n</code></pre> <p>Max pooling layer, which replaces all pixels in a block of size <code>window</code> with the maximum value.</p> <p>Arguments</p> <ul> <li><code>window</code>: Tuple of integers specifying the size of the window. Eg, for 2D pooling           <code>length(window) == 2</code></li> </ul> <p>Keyword Arguments</p> <ul> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(O_N, ..., O_1, C, N)</code> where</li> </ul> \\[   O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>Conv</code>, <code>MeanPool</code>, <code>GlobalMaxPool</code>, <code>AdaptiveMaxPool</code></p> <p>source</p> <p># <code>Lux.MeanPool</code> \u2014 Type.</p> <pre><code>MeanPool(window::NTuple; pad=0, stride=window)\n</code></pre> <p>Mean pooling layer, which replaces all pixels in a block of size <code>window</code> with the mean value.</p> <p>Arguments</p> <ul> <li><code>window</code>: Tuple of integers specifying the size of the window. Eg, for 2D pooling           <code>length(window) == 2</code></li> </ul> <p>Keyword Arguments</p> <ul> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(O_N, ..., O_1, C, N)</code> where</li> </ul> \\[   O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>Conv</code>, <code>MaxPool</code>, <code>GlobalMeanPool</code>, <code>AdaptiveMeanPool</code></p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#recurrent-layers","title":"Recurrent Layers","text":"<p># <code>Lux.GRUCell</code> \u2014 Type.</p> <pre><code>GRUCell((in_dims, out_dims)::Pair{&lt;:Int,&lt;:Int}; use_bias=true, train_state::Bool=false,\ninit_weight::Tuple{Function,Function,Function}=(glorot_uniform, glorot_uniform,\nglorot_uniform),\ninit_bias::Tuple{Function,Function,Function}=(zeros32, zeros32, zeros32),\ninit_state::Function=zeros32)\n</code></pre> <p>Gated Recurrent Unit (GRU) Cell</p> \\[ \\begin{align}   r &amp;= \\sigma(W_{ir} \\times x + W_{hr} \\times h_{prev} + b_{hr})\\\\   z &amp;= \\sigma(W_{iz} \\times x + W_{hz} \\times h_{prev} + b_{hz})\\\\   n &amp;= \\sigma(W_{in} \\times x + b_{in} + r \\cdot (W_{hn} \\times h_{prev} + b_{hn}))\\\\   h_{new} &amp;= (1 - z) \\cdot n + z \\cdot h_{prev} \\end{align} \\] <p>Arguments</p> <ul> <li><code>in_dims</code>: Input Dimension</li> <li><code>out_dims</code>: Output (Hidden State) Dimension</li> <li><code>use_bias</code>: Set to false to deactivate bias</li> <li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li> <li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 3 functions</li> <li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 3 functions</li> <li><code>init_state</code>: Initializer for hidden state</li> </ul> <p>Inputs</p> <ul> <li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li> <li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li> <li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the          updated hidden state is returned.</li> </ul> <p>Returns</p> <ul> <li> <p>Tuple containing</p> <ul> <li>Output \\(h_{new}\\) of shape <code>(out_dims, batch_size)</code></li> <li>Tuple containing new hidden state \\(h_{new}\\)</li> <li>Updated model state</li> </ul> </li> </ul> <p>Parameters</p> <ul> <li><code>weight_i</code>: Concatenated Weights to map from input space             \\(\\\\left\\\\{ W_{ir}, W_{iz}, W_{in} \\\\right\\\\}\\).</li> <li><code>weight_h</code>: Concatenated Weights to map from hidden space             \\(\\\\left\\\\{ W_{hr}, W_{hz}, W_{hn} \\\\right\\\\}\\)</li> <li><code>bias_i</code>: Bias vector (\\(b_{in}\\); not present if <code>use_bias=false</code>)</li> <li><code>bias_h</code>: Concatenated Bias vector for the hidden space           \\(\\\\left\\\\{ b_{hr}, b_{hz}, b_{hn} \\\\right\\\\}\\) (not present if           <code>use_bias=false</code>)</li> <li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)           \\(\\\\left\\\\{ b_{hr}, b_{hz}, b_{hn} \\\\right\\\\}\\)</li> </ul> <p>States</p> <ul> <li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li> </ul> <p>source</p> <p># <code>Lux.LSTMCell</code> \u2014 Type.</p> <pre><code>LSTMCell(in_dims =&gt; out_dims; use_bias::Bool=true, train_state::Bool=false,\ntrain_memory::Bool=false,\ninit_weight=(glorot_uniform, glorot_uniform, glorot_uniform, glorot_uniform),\ninit_bias=(zeros32, zeros32, ones32, zeros32), init_state=zeros32,\ninit_memory=zeros32)\n</code></pre> <p>Long Short-Term (LSTM) Cell</p> \\[ \\begin{align}   i &amp;= \\sigma(W_{ii} \\times x + W_{hi} \\times h_{prev} + b_{i})\\\\   f &amp;= \\sigma(W_{if} \\times x + W_{hf} \\times h_{prev} + b_{f})\\\\   g &amp;= tanh(W_{ig} \\times x + W_{hg} \\times h_{prev} + b_{g})\\\\   o &amp;= \\sigma(W_{io} \\times x + W_{ho} \\times h_{prev} + b_{o})\\\\   c_{new} &amp;= f \\cdot c_{prev} + i \\cdot g\\\\   h_{new} &amp;= o \\cdot tanh(c_{new}) \\end{align} \\] <p>Arguments</p> <ul> <li><code>in_dims</code>: Input Dimension</li> <li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li> <li><code>use_bias</code>: Set to false to deactivate bias</li> <li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li> <li><code>train_memory</code>: Trainable initial memory can be activated by setting this to <code>true</code></li> <li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions</li> <li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions</li> <li><code>init_state</code>: Initializer for hidden state</li> <li><code>init_memory</code>: Initializer for memory</li> </ul> <p>Inputs</p> <ul> <li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li> <li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li> <li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li> <li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li> <li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li> </ul> <p>Returns</p> <ul> <li> <p>Tuple Containing</p> <ul> <li>Output \\(h_{new}\\) of shape <code>(out_dims, batch_size)</code></li> <li>Tuple containing new hidden state \\(h_{new}\\) and new memory \\(c_{new}\\)</li> <li>Updated model state</li> </ul> </li> </ul> <p>Parameters</p> <ul> <li><code>weight_i</code>: Concatenated Weights to map from input space             \\(\\left\\{ W_{ii}, W_{if}, W_{ig}, W_{io} \\right\\}\\).</li> <li><code>weight_h</code>: Concatenated Weights to map from hidden space             \\(\\left\\{ W_{hi}, W_{hf}, W_{hg}, W_{ho} \\right\\}\\)</li> <li><code>bias</code>: Bias vector (not present if <code>use_bias=false</code>)</li> <li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li> <li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li> </ul> <p>States</p> <ul> <li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li> </ul> <p>source</p> <p># <code>Lux.RNNCell</code> \u2014 Type.</p> <pre><code>RNNCell(in_dims =&gt; out_dims, activation=tanh; bias::Bool=true,\ntrain_state::Bool=false, init_bias=zeros32, init_weight=glorot_uniform,\ninit_state=ones32)\n</code></pre> <p>An Elman RNNCell cell with <code>activation</code> (typically set to <code>tanh</code> or <code>relu</code>).</p> <p>\\(h_{new} = activation(weight_{ih} \\times x + weight_{hh} \\times h_{prev} + bias)\\)</p> <p>Arguments</p> <ul> <li><code>in_dims</code>: Input Dimension</li> <li><code>out_dims</code>: Output (Hidden State) Dimension</li> <li><code>activation</code>: Activation function</li> <li><code>bias</code>: Set to false to deactivate bias</li> <li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li> <li><code>init_bias</code>: Initializer for bias</li> <li><code>init_weight</code>: Initializer for weight</li> <li><code>init_state</code>: Initializer for hidden state</li> </ul> <p>Inputs</p> <ul> <li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li> <li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li> <li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the updated hidden state is returned.</li> </ul> <p>Returns</p> <ul> <li> <p>Tuple containing</p> <ul> <li>Output \\(h_{new}\\) of shape <code>(out_dims, batch_size)</code></li> <li>Tuple containing new hidden state \\(h_{new}\\)</li> <li>Updated model state</li> </ul> </li> </ul> <p>Parameters</p> <ul> <li><code>weight_ih</code>: Maps the input to the hidden state.</li> <li><code>weight_hh</code>: Maps the hidden state to the hidden state.</li> <li><code>bias</code>: Bias vector (not present if <code>bias=false</code>)</li> <li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li> </ul> <p>States</p> <ul> <li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li> </ul> <p>source</p> <p># <code>Lux.Recurrence</code> \u2014 Type.</p> <pre><code>Recurrence(cell)\n</code></pre> <p>Wraps a recurrent cell (like <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>) to automatically operate over a sequence of inputs.</p> <p>Warning</p> <p>This is completely distinct from <code>Flux.Recur</code>. It doesn't make the <code>cell</code> stateful, rather allows operating on an entire sequence of inputs at once. See <code>StatefulRecurrentCell</code> for functionality similar to <code>Flux.Recur</code>.</p> <p>Arguments</p> <ul> <li><code>cell</code>: A recurrent cell. See <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>, for how the inputs/outputs of a recurrent cell must be structured.</li> </ul> <p>Inputs</p> <ul> <li> <p>If <code>x</code> is a</p> <ul> <li>Tuple or Vector: Each element is fed to the <code>cell</code> sequentially.</li> <li>Array (except a Vector): It is spliced along the penultimate dimension and each slice is fed to the <code>cell</code> sequentially.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>Output of the <code>cell</code> for the entire sequence.</li> <li>Update state of the <code>cell</code>.</li> </ul> <p>Parameters</p> <ul> <li>Same as <code>cell</code>.</li> </ul> <p>States</p> <ul> <li>Same as <code>cell</code>.</li> </ul> <p>source</p> <p># <code>Lux.StatefulRecurrentCell</code> \u2014 Type.</p> <pre><code>StatefulRecurrentCell(cell)\n</code></pre> <p>Wraps a recurrent cell (like <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>) and makes it stateful.</p> <p>Tip</p> <p>This is very similar to <code>Flux.Recur</code></p> <p>To avoid undefined behavior, once the processing of a single sequence of data is complete, update the state with <code>Lux.update_state(st, :carry, nothing)</code>.</p> <p>Arguments</p> <ul> <li><code>cell</code>: A recurrent cell. See <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>, for how the inputs/outputs of a recurrent cell must be structured.</li> </ul> <p>Inputs</p> <ul> <li>Input to the <code>cell</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of the <code>cell</code> for the entire sequence.</li> <li>Update state of the <code>cell</code> and updated <code>carry</code>.</li> </ul> <p>Parameters</p> <ul> <li>Same as <code>cell</code>.</li> </ul> <p>States</p> <ul> <li> <p>NamedTuple containing:</p> <ul> <li><code>cell</code>: Same as <code>cell</code>.</li> <li><code>carry</code>: The carry state of the <code>cell</code>.</li> </ul> </li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#linear-layers","title":"Linear Layers","text":"<p># <code>Lux.Bilinear</code> \u2014 Type.</p> <pre><code>Bilinear((in1_dims, in2_dims) =&gt; out, activation=identity; init_weight=glorot_uniform,\ninit_bias=zeros32, use_bias::Bool=true, allow_fast_activation::Bool=true)\nBilinear(in12_dims =&gt; out, activation=identity; init_weight=glorot_uniform,\ninit_bias=zeros32, use_bias::Bool=true, allow_fast_activation::Bool=true)\n</code></pre> <p>Create a fully connected layer between two inputs and an output, and otherwise similar to <code>Dense</code>. Its output, given vectors <code>x</code> &amp; <code>y</code>, is another vector <code>z</code> with, for all <code>i in 1:out</code>:</p> <p><code>z[i] = activation(x' * W[i, :, :] * y + bias[i])</code></p> <p>If <code>x</code> and <code>y</code> are matrices, then each column of the output <code>z = B(x, y)</code> is of this form, with <code>B</code> the Bilinear layer.</p> <p>Arguments</p> <ul> <li><code>in1_dims</code>: number of input dimensions of <code>x</code></li> <li><code>in2_dims</code>: number of input dimensions of <code>y</code></li> <li><code>in12_dims</code>: If specified, then <code>in1_dims = in2_dims = in12_dims</code></li> <li><code>out</code>: number of output dimensions</li> <li><code>activation</code>: activation function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in1_dims, in2_dims)</code>)</li> <li><code>init_bias</code>: initializer for the bias vector (ignored if <code>use_bias=false</code>)</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code></li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> <p>Input</p> <ul> <li> <p>A 2-Tuple containing</p> <ul> <li><code>x</code> must be an AbstractArray with <code>size(x, 1) == in1_dims</code></li> <li><code>y</code> must be an AbstractArray with <code>size(x, 1) == in2_dims</code></li> <li>If the input is an AbstractArray, then <code>x = y</code></li> </ul> </li> </ul> <p>Returns</p> <ul> <li>AbstractArray with dimensions <code>(out_dims, size(x, 2))</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Weight Matrix of size <code>(out_dims, in1_dims, in2_dims)</code></li> <li><code>bias</code>: Bias of size <code>(out_dims, 1)</code> (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.Dense</code> \u2014 Type.</p> <pre><code>Dense(in_dims =&gt; out_dims, activation=identity; init_weight=glorot_uniform,\ninit_bias=zeros32, bias::Bool=true)\n</code></pre> <p>Create a traditional fully connected layer, whose forward pass is given by: <code>y = activation.(weight * x .+ bias)</code></p> <p>Arguments</p> <ul> <li><code>in_dims</code>: number of input dimensions</li> <li><code>out_dims</code>: number of output dimensions</li> <li><code>activation</code>: activation function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in_dims)</code>)</li> <li><code>init_bias</code>: initializer for the bias vector (ignored if <code>use_bias=false</code>)</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code></li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> <p>Input</p> <ul> <li><code>x</code> must be an AbstractArray with <code>size(x, 1) == in_dims</code></li> </ul> <p>Returns</p> <ul> <li>AbstractArray with dimensions <code>(out_dims, ...)</code> where <code>...</code> are the dimensions of <code>x</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Weight Matrix of size <code>(out_dims, in_dims)</code></li> <li><code>bias</code>: Bias of size <code>(out_dims, 1)</code> (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.Embedding</code> \u2014 Type.</p> <pre><code>Embedding(in_dims =&gt; out_dims; init_weight=randn32)\n</code></pre> <p>A lookup table that stores embeddings of dimension <code>out_dims</code> for a vocabulary of size <code>in_dims</code>.</p> <p>This layer is often used to store word embeddings and retrieve them using indices.</p> <p>Warning</p> <p>Unlike <code>Flux.Embedding</code>, this layer does not support using <code>OneHotArray</code> as an input.</p> <p>Arguments</p> <ul> <li><code>in_dims</code>: number of input dimensions</li> <li><code>out_dims</code>: number of output dimensions</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in_dims)</code>)</li> </ul> <p>Input</p> <ul> <li>Integer OR</li> <li>Abstract Vector of Integers OR</li> <li>Abstract Array of Integers</li> </ul> <p>Returns</p> <ul> <li>Returns the embedding corresponding to each index in the input. For an N dimensional input, an N + 1 dimensional output is returned.</li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.Scale</code> \u2014 Type.</p> <pre><code>Scale(dims, activation=identity; init_weight=ones32, init_bias=zeros32, bias::Bool=true)\n</code></pre> <p>Create a Sparsely Connected Layer with a very specific structure (only Diagonal Elements are non-zero). The forward pass is given by: <code>y = activation.(weight .* x .+ bias)</code></p> <p>Arguments</p> <ul> <li><code>dims</code>: size of the learnable scale and bias parameters.</li> <li><code>activation</code>: activation function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in_dims)</code>)</li> <li><code>init_bias</code>: initializer for the bias vector (ignored if <code>use_bias=false</code>)</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code></li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> <p>Input</p> <ul> <li><code>x</code> must be an Array of size <code>(dims..., B)</code> or <code>(dims...[0], ..., dims[k])</code> for <code>k \u2264 size(dims)</code></li> </ul> <p>Returns</p> <ul> <li>Array of size <code>(dims..., B)</code> or <code>(dims...[0], ..., dims[k])</code> for <code>k \u2264 size(dims)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Weight Array of size <code>(dims...)</code></li> <li><code>bias</code>: Bias of size <code>(dims...)</code></li> </ul> <p>Lux 0.4.3</p> <p><code>Scale</code> with multiple dimensions requires at least Lux 0.4.3.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#misc-helper-layers","title":"Misc. Helper Layers","text":"<p># <code>Lux.ActivationFunction</code> \u2014 Function.</p> <pre><code>ActivationFunction(f)\n</code></pre> <p>Broadcast <code>f</code> on the input.</p> <p>Arguments</p> <ul> <li><code>f</code>: Activation function</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Any array type s.t. <code>f</code> can be broadcasted over it</li> </ul> <p>Returns</p> <ul> <li>Broadcasted Activation <code>f.(x)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Warning</p> <p>This layer is deprecated and will be removed in v0.5. Use <code>WrappedFunction</code> with manual broadcasting</p> <p>source</p> <p># <code>Lux.FlattenLayer</code> \u2014 Type.</p> <pre><code>FlattenLayer()\n</code></pre> <p>Flattens the passed array into a matrix.</p> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray</li> </ul> <p>Returns</p> <ul> <li>AbstractMatrix of size <code>(:, size(x, ndims(x)))</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.Maxout</code> \u2014 Type.</p> <pre><code>Maxout(layers...)\nMaxout(; layers...)\nMaxout(f::Function, n_alts::Int)\n</code></pre> <p>This contains a number of internal layers, each of which receives the same input. Its output is the elementwise maximum of the the internal layers' outputs.</p> <p>Maxout over linear dense layers satisfies the univeral approximation theorem. See [1].</p> <p>See also <code>Parallel</code> to reduce with other operators.</p> <p>Arguments</p> <ul> <li> <p>Layers can be specified in three formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> <li>A no argument function <code>f</code> and an integer <code>n_alts</code> which specifies the number of layers.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Input that is passed to each of the layers</li> </ul> <p>Returns</p> <ul> <li>Output is computed by taking elementwise <code>max</code> of the outputs of the individual layers.</li> <li>Updated state of the <code>layers</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>References</p> <p>[1] Goodfellow, Warde-Farley, Mirza, Courville &amp; Bengio \"Maxout Networks\" https://arxiv.org/abs/1302.4389</p> <p>source</p> <p># <code>Lux.NoOpLayer</code> \u2014 Type.</p> <pre><code>NoOpLayer()\n</code></pre> <p>As the name suggests does nothing but allows pretty printing of layers. Whatever input is passed is returned.</p> <p>source</p> <p># <code>Lux.ReshapeLayer</code> \u2014 Type.</p> <pre><code>ReshapeLayer(dims)\n</code></pre> <p>Reshapes the passed array to have a size of <code>(dims..., :)</code></p> <p>Arguments</p> <ul> <li><code>dims</code>: The new dimensions of the array (excluding the last dimension).</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray of any shape which can be reshaped in <code>(dims..., size(x, ndims(x)))</code></li> </ul> <p>Returns</p> <ul> <li>AbstractArray of size <code>(dims..., size(x, ndims(x)))</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.SelectDim</code> \u2014 Type.</p> <pre><code>SelectDim(dim, i)\n</code></pre> <p>Return a view of all the data of the input <code>x</code> where the index for dimension <code>dim</code> equals <code>i</code>. Equivalent to <code>view(x,:,:,...,i,:,:,...)</code> where <code>i</code> is in position <code>d</code>.</p> <p>Arguments</p> <ul> <li><code>dim</code>: Dimension for indexing</li> <li><code>i</code>: Index for dimension <code>dim</code></li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray that can be indexed with <code>view(x,:,:,...,i,:,:,...)</code></li> </ul> <p>Returns</p> <ul> <li><code>view(x,:,:,...,i,:,:,...)</code> where <code>i</code> is in position <code>d</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.WrappedFunction</code> \u2014 Type.</p> <pre><code>WrappedFunction(f)\n</code></pre> <p>Wraps a stateless and parameter less function. Might be used when a function is added to <code>Chain</code>. For example, <code>Chain(x -&gt; relu.(x))</code> would not work and the right thing to do would be <code>Chain((x, ps, st) -&gt; (relu.(x), st))</code>. An easier thing to do would be <code>Chain(WrappedFunction(Base.Fix1(broadcast, relu)))</code></p> <p>Arguments</p> <ul> <li><code>f::Function</code>: A stateless and parameterless function</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: s.t <code>hasmethod(f, (typeof(x),))</code> is <code>true</code></li> </ul> <p>Returns</p> <ul> <li>Output of <code>f(x)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#normalization-layers","title":"Normalization Layers","text":"<p># <code>Lux.BatchNorm</code> \u2014 Type.</p> <pre><code>BatchNorm(chs::Integer, activation=identity; init_bias=zeros32, init_scale=ones32,\naffine=true, track_stats=true, epsilon=1f-5, momentum=0.1f0,\nallow_fast_activation::Bool=true)\n</code></pre> <p>Batch Normalization layer.</p> <p><code>BatchNorm</code> computes the mean and variance for each \\(D_1 \u00d7 ... \u00d7 D_{N-2} \u00d7 1 \u00d7 D_N\\) input slice and normalises the input accordingly.</p> <p>Arguments</p> <ul> <li><code>chs</code>: Size of the channel dimension in your data. Given an array with <code>N</code> dimensions, call the <code>N-1</code>th the channel dimension. For a batch of feature vectors this is just the data dimension, for <code>WHCN</code> images it's the usual channel dimension.</li> <li><code>activation</code>: After normalization, elementwise activation <code>activation</code> is applied.</li> </ul> <p>Keyword Arguments</p> <ul> <li>If <code>track_stats=true</code>, accumulates mean and variance statistics in training phase that will be used to renormalize the input in test phase.</li> <li><code>epsilon</code>: a value added to the denominator for numerical stability</li> <li><code>momentum</code>:  the value used for the <code>running_mean</code> and <code>running_var</code> computation</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> <li> <p>If <code>affine=true</code>, it also applies  a shift and a rescale to the input through to learnable per-channel bias and scale parameters.</p> <ul> <li><code>init_bias</code>: Controls how the <code>bias</code> is initiliazed</li> <li><code>init_scale</code>: Controls how the <code>scale</code> is initiliazed</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Array where <code>size(x, N - 1) = chs</code> and <code>ndims(x) &gt; 2</code></li> </ul> <p>Returns</p> <ul> <li><code>y</code>: Normalized Array</li> <li>Update model state</li> </ul> <p>Parameters</p> <ul> <li> <p><code>affine=true</code></p> <ul> <li><code>bias</code>: Bias of shape <code>(chs,)</code></li> <li><code>scale</code>: Scale of shape <code>(chs,)</code></li> <li><code>affine=false</code> - Empty <code>NamedTuple()</code></li> </ul> </li> </ul> <p>States</p> <ul> <li> <p>Statistics if <code>track_stats=true</code></p> <ul> <li><code>running_mean</code>: Running mean of shape <code>(chs,)</code></li> <li><code>running_var</code>: Running variance of shape <code>(chs,)</code></li> <li> <p>Statistics if <code>track_stats=false</code></p> </li> <li> <p><code>running_mean</code>: nothing</p> </li> <li><code>running_var</code>: nothing</li> <li><code>training</code>: Used to check if training/inference mode</li> </ul> </li> </ul> <p>Use <code>Lux.testmode</code> during inference.</p> <p>Example</p> <pre><code>m = Chain(Dense(784 =&gt; 64), BatchNorm(64, relu), Dense(64 =&gt; 10), BatchNorm(10))\n</code></pre> <p>Warning</p> <p>Passing a batch size of 1, during training will result in NaNs.</p> <p>See also <code>BatchNorm</code>, <code>InstanceNorm</code>, <code>LayerNorm</code>, <code>WeightNorm</code></p> <p>source</p> <p># <code>Lux.GroupNorm</code> \u2014 Type.</p> <pre><code>GroupNorm(chs::Integer, groups::Integer, activation=identity; init_bias=zeros32,\ninit_scale=ones32, affine=true, track_stats=true, epsilon=1f-5,\nmomentum=0.1f0, allow_fast_activation::Bool=true)\n</code></pre> <p>Group Normalization layer.</p> <p>Arguments</p> <ul> <li><code>chs</code>: Size of the channel dimension in your data. Given an array with <code>N</code> dimensions, call the <code>N-1</code>th the channel dimension. For a batch of feature vectors this is just the data dimension, for <code>WHCN</code> images it's the usual channel dimension.</li> <li><code>groups</code> is the number of groups along which the statistics are computed. The number of channels must be an integer multiple of the number of groups.</li> <li><code>activation</code>: After normalization, elementwise activation <code>activation</code> is applied.</li> </ul> <p>Keyword Arguments</p> <ul> <li>If <code>track_stats=true</code>, accumulates mean and variance statistics in training phase that will be used to renormalize the input in test phase. (This feature has been deprecated and will be removed in v0.5)</li> <li><code>epsilon</code>: a value added to the denominator for numerical stability</li> <li><code>momentum</code>:  the value used for the <code>running_mean</code> and <code>running_var</code> computation (This feature has been deprecated and will be removed in v0.5)</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> <li> <p>If <code>affine=true</code>, it also applies  a shift and a rescale to the input through to learnable per-channel bias and scale parameters.</p> <ul> <li><code>init_bias</code>: Controls how the <code>bias</code> is initiliazed</li> <li><code>init_scale</code>: Controls how the <code>scale</code> is initiliazed</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Array where <code>size(x, N - 1) = chs</code> and <code>ndims(x) &gt; 2</code></li> </ul> <p>Returns</p> <ul> <li><code>y</code>: Normalized Array</li> <li>Update model state</li> </ul> <p>Parameters</p> <ul> <li> <p><code>affine=true</code></p> <ul> <li><code>bias</code>: Bias of shape <code>(chs,)</code></li> <li><code>scale</code>: Scale of shape <code>(chs,)</code></li> <li><code>affine=false</code> - Empty <code>NamedTuple()</code></li> </ul> </li> </ul> <p>States</p> <ul> <li> <p>Statistics if <code>track_stats=true</code> (DEPRECATED)</p> <ul> <li><code>running_mean</code>: Running mean of shape <code>(groups,)</code></li> <li><code>running_var</code>: Running variance of shape <code>(groups,)</code></li> <li> <p>Statistics if <code>track_stats=false</code></p> </li> <li> <p><code>running_mean</code>: nothing</p> </li> <li><code>running_var</code>: nothing</li> <li><code>training</code>: Used to check if training/inference mode</li> </ul> </li> </ul> <p>Use <code>Lux.testmode</code> during inference.</p> <p>Example</p> <pre><code>m = Chain(Dense(784 =&gt; 64), GroupNorm(64, 4, relu), Dense(64 =&gt; 10), GroupNorm(10, 5))\n</code></pre> <p>Warning</p> <p>GroupNorm doesn't have CUDNN support. The GPU fallback is not very efficient.</p> <p>See also <code>GroupNorm</code>, <code>InstanceNorm</code>, <code>LayerNorm</code>, <code>WeightNorm</code></p> <p>source</p> <p># <code>Lux.LayerNorm</code> \u2014 Type.</p> <pre><code>LayerNorm(shape::NTuple{N, Int}, activation=identity; epsilon=1f-5, dims=Colon(),\naffine::Bool=false, init_bias=zeros32, init_scale=ones32,)\n</code></pre> <p>Computes mean and standard deviation over the whole input array, and uses these to normalize the whole array. Optionally applies an elementwise affine transformation afterwards.</p> <p>Given an input array \\(x\\), this layer computes</p> \\[ y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta \\] <p>where \\(\\gamma\\) &amp; \\(\\beta\\) are trainable parameters if <code>affine=true</code>.</p> <p>Arguments</p> <ul> <li><code>shape</code>: Broadcastable shape of input array excluding the batch dimension.</li> <li><code>activation</code>: After normalization, elementwise activation <code>activation</code> is applied.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> <li><code>epsilon</code>: a value added to the denominator for numerical stability.</li> <li><code>dims</code>: Dimensions to normalize the array over.</li> <li> <p>If <code>affine=true</code>, it also applies  a shift and a rescale to the input through to learnable per-channel bias and scale parameters.</p> <ul> <li><code>init_bias</code>: Controls how the <code>bias</code> is initiliazed</li> <li><code>init_scale</code>: Controls how the <code>scale</code> is initiliazed</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray</li> </ul> <p>Returns</p> <ul> <li><code>y</code>: Normalized Array</li> <li>Empty NamedTuple()</li> </ul> <p>Parameters</p> <ul> <li><code>affine=false</code>: Empty <code>NamedTuple()</code></li> <li> <p><code>affine=true</code></p> <ul> <li><code>bias</code>: Bias of shape <code>(shape..., 1)</code></li> <li><code>scale</code>: Scale of shape <code>(shape..., 1)</code></li> </ul> </li> </ul> <p>source</p> <p># <code>Lux.WeightNorm</code> \u2014 Type.</p> <pre><code>WeightNorm(layer::AbstractExplicitLayer, which_params::NTuple{N,Symbol},\ndims::Union{Tuple,Nothing}=nothing)\n</code></pre> <p>Applies weight normalization to a parameter in the given layer.</p> <p>\\(w = g\\frac{v}{\\|v\\|}\\)</p> <p>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This updates the parameters in <code>which_params</code> (e.g. <code>weight</code>) using two parameters: one specifying the magnitude (e.g. <code>weight_g</code>) and one specifying the direction (e.g. <code>weight_v</code>).</p> <p>Arguments</p> <ul> <li><code>layer</code> whose parameters are being reparameterized</li> <li><code>which_params</code>: parameter names for the parameters being reparameterized</li> <li>By default, a norm over the entire array is computed. Pass <code>dims</code> to modify the dimension.</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Should be of valid type for input to <code>layer</code></li> </ul> <p>Returns</p> <ul> <li>Output from <code>layer</code></li> <li>Updated model state of <code>layer</code></li> </ul> <p>Parameters</p> <ul> <li><code>normalized</code>: Parameters of <code>layer</code> that are being normalized</li> <li><code>unnormalized</code>: Parameters of <code>layer</code> that are not being normalized</li> </ul> <p>States</p> <ul> <li>Same as that of <code>layer</code></li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#upsampling","title":"Upsampling","text":"<p># <code>Lux.PixelShuffle</code> \u2014 Function.</p> <pre><code>PixelShuffle(r::Int)\n</code></pre> <p>Pixel shuffling layer with upscale factor <code>r</code>. Usually used for generating higher resolution images while upscaling them.</p> <p>See <code>NNlib.pixel_shuffle</code> for more details.</p> <p>PixelShuffle is not a Layer, rather it returns a <code>WrappedFunction</code> with the function set to <code>Base.Fix2(pixel_shuffle, r)</code></p> <p>Arguments</p> <ul> <li><code>r</code>: Upscale factor</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: For 4D-arrays representing N images, the operation converts input size(x) == (W, H, r^2 x C, N) to output of size (r x W, r x H, C, N). For D-dimensional data, it expects ndims(x) == D+2 with channel and batch dimensions, and divides the number of channels by r^D.</li> </ul> <p>Returns</p> <ul> <li>Output of size <code>(r x W, r x H, C, N)</code> for 4D-arrays, and <code>(r x W, r x H, ..., C, N)</code> for D-dimensional data, where <code>D = ndims(x) - 2</code></li> </ul> <p>source</p> <p># <code>Lux.Upsample</code> \u2014 Type.</p> <pre><code>Upsample(mode = :nearest; [scale, size]) Upsample(scale, mode = :nearest)\n</code></pre> <p>Upsampling Layer.</p> <p>Layer Construction</p> <p>Option 1</p> <ul> <li><code>mode</code>: Set to <code>:nearest</code>, <code>:linear</code>, <code>:bilinear</code> or <code>:trilinear</code></li> </ul> <p>Exactly one of two keywords must be specified:</p> <ul> <li>If <code>scale</code> is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually.</li> <li>Alternatively, keyword <code>size</code> accepts a tuple, to directly specify the leading dimensions of the output.</li> </ul> <p>Option 2</p> <ul> <li>If <code>scale</code> is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually.</li> <li><code>mode</code>: Set to <code>:nearest</code>, <code>:bilinear</code> or <code>:trilinear</code></li> </ul> <p>Currently supported upsampling <code>mode</code>s and corresponding NNlib's methods are:</p> <ul> <li><code>:nearest</code> -&gt; <code>NNlib.upsample_nearest</code></li> <li><code>:bilinear</code> -&gt; <code>NNlib.upsample_bilinear</code></li> <li><code>:trilinear</code> -&gt; <code>NNlib.upsample_trilinear</code></li> </ul> <p>Inputs</p> <ul> <li> <p><code>x</code>: For the input dimensions look into the documentation for the corresponding <code>NNlib</code> function</p> <ul> <li>As a rule of thumb, <code>:nearest</code> should work with arrays of arbitrary dimensions</li> <li><code>:bilinear</code> works with 4D Arrays</li> <li><code>:trilinear</code> works with 5D Arrays</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>Upsampled Input of size <code>size</code> or of size <code>(I_1 x scale[1], ..., I_N x scale[N], C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#index","title":"Index","text":"<ul> <li><code>Lux.AdaptiveMaxPool</code></li> <li><code>Lux.AdaptiveMeanPool</code></li> <li><code>Lux.BatchNorm</code></li> <li><code>Lux.Bilinear</code></li> <li><code>Lux.BranchLayer</code></li> <li><code>Lux.Chain</code></li> <li><code>Lux.Conv</code></li> <li><code>Lux.ConvTranspose</code></li> <li><code>Lux.CrossCor</code></li> <li><code>Lux.Dense</code></li> <li><code>Lux.Dropout</code></li> <li><code>Lux.Embedding</code></li> <li><code>Lux.FlattenLayer</code></li> <li><code>Lux.GRUCell</code></li> <li><code>Lux.GlobalMaxPool</code></li> <li><code>Lux.GlobalMeanPool</code></li> <li><code>Lux.GroupNorm</code></li> <li><code>Lux.LSTMCell</code></li> <li><code>Lux.LayerNorm</code></li> <li><code>Lux.MaxPool</code></li> <li><code>Lux.Maxout</code></li> <li><code>Lux.MeanPool</code></li> <li><code>Lux.NoOpLayer</code></li> <li><code>Lux.PairwiseFusion</code></li> <li><code>Lux.Parallel</code></li> <li><code>Lux.RNNCell</code></li> <li><code>Lux.Recurrence</code></li> <li><code>Lux.ReshapeLayer</code></li> <li><code>Lux.Scale</code></li> <li><code>Lux.SelectDim</code></li> <li><code>Lux.SkipConnection</code></li> <li><code>Lux.StatefulRecurrentCell</code></li> <li><code>Lux.Upsample</code></li> <li><code>Lux.VariationalHiddenDropout</code></li> <li><code>Lux.WeightNorm</code></li> <li><code>Lux.WrappedFunction</code></li> <li><code>Lux.ActivationFunction</code></li> <li><code>Lux.PixelShuffle</code></li> </ul>"},{"location":"api/utilities/","title":"Utilities","text":""},{"location":"api/utilities/#data-transfer","title":"Data Transfer","text":"<p># <code>Lux.cpu</code> \u2014 Function.</p> <pre><code>cpu(x)\n</code></pre> <p>Transfer <code>x</code> to CPU</p> <p>source</p> <p># <code>Lux.gpu</code> \u2014 Function.</p> <pre><code>gpu(x)\n</code></pre> <p>Transfer <code>x</code> to GPU</p> <p>source</p> <p></p> <p></p>"},{"location":"api/utilities/#initialization","title":"Initialization","text":"<p># <code>Lux.glorot_normal</code> \u2014 Function.</p> <pre><code>glorot_normal(rng::AbstractRNG, size...; gain = 1)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a normal distribution with standard deviation <code>gain * sqrt(2 / (fan_in + fan_out))</code>. This method is described in [1] and also known as Xavier initialization.</p> <p>References</p> <p>[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.</p> <p>source</p> <p># <code>Lux.glorot_uniform</code> \u2014 Function.</p> <pre><code>glorot_uniform(rng::AbstractRNG, size...; gain = 1)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a uniform distribution on the interval \\([-x, x]\\), where <code>x = gain * sqrt(6 / (fan_in + fan_out))</code>. This method is described in [1] and also known as Xavier initialization.</p> <p>References</p> <p>[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.</p> <p>source</p> <p># <code>Lux.kaiming_normal</code> \u2014 Function.</p> <pre><code>kaiming_normal(rng::AbstractRNG, size...; gain = \u221a2f0)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers taken from a normal distribution standard deviation <code>gain / sqrt(fan_in)</code></p> <p>References</p> <p>[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.</p> <p>source</p> <p># <code>Lux.kaiming_uniform</code> \u2014 Function.</p> <pre><code>kaiming_uniform(rng::AbstractRNG, size...; gain = \u221a2f0)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a uniform distribution on the interval <code>[-x, x]</code>, where <code>x = gain * sqrt(3/fan_in)</code>.</p> <p>References</p> <p>[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.</p> <p>source</p> <p># <code>Lux.ones32</code> \u2014 Function.</p> <pre><code>ones32(rng::AbstractRNG, size...) = ones(Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of ones of the given <code>size</code>. (<code>rng</code> is ignored)</p> <p>source</p> <p># <code>Lux.rand32</code> \u2014 Function.</p> <pre><code>rand32(rng::AbstractRNG, size...) = rand(rng, Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of random numbers from a uniform distribution of the given <code>size</code>.</p> <p>source</p> <p># <code>Lux.randn32</code> \u2014 Function.</p> <pre><code>randn32(rng::AbstractRNG, size...) = randn(rng, Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of random numbers from a standard normal distribution of the given <code>size</code>.</p> <p>source</p> <p># <code>Lux.zeros32</code> \u2014 Function.</p> <pre><code>zeros32(rng::AbstractRNG, size...) = zeros(Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of zeros of the given <code>size</code>. (<code>rng</code> is ignored)</p> <p>source</p> <p></p> <p></p>"},{"location":"api/utilities/#miscellaneous-utilities","title":"Miscellaneous Utilities","text":"<p># <code>Lux.applyactivation</code> \u2014 Function.</p> <pre><code>applyactivation(f::Function, x::AbstractArray)\n</code></pre> <p>Apply the function <code>f</code> on <code>x</code> elementwise, i.e. <code>f.(x)</code>. Dispatches to CUDNN if possible.</p> <p>Warning</p> <p>This function has been deprecated. Use <code>f.(x)</code> instead.</p> <p>source</p> <p># <code>Lux.elementwise_add</code> \u2014 Function.</p> <pre><code>elementwise_add(x, y)\n</code></pre> <p>Computes <code>x .+ y</code>. Dispatches to CUDNN if possible.</p> <p>Warning</p> <p>This function has been deprecated. Use <code>x .+ y</code> instead.</p> <p>source</p> <p># <code>Lux.elementwise_mul</code> \u2014 Function.</p> <pre><code>elementwise_mul(x, y)\n</code></pre> <p>Computes <code>x .* y</code>. Dispatches to CUDNN if possible.</p> <p>Warning</p> <p>This function has been deprecated. Use <code>x .* y</code> instead.</p> <p>source</p> <p># <code>Lux.istraining</code> \u2014 Function.</p> <pre><code>istraining(::Val{training})\nistraining(st::NamedTuple)\n</code></pre> <p>Returns <code>true</code> if <code>training</code> is <code>true</code> or if <code>st</code> contains a <code>training</code> field with value <code>true</code>. Else returns <code>false</code>.</p> <p>Method undefined if <code>st.training</code> is not of type <code>Val</code>.</p> <p>source</p> <p># <code>Lux.multigate</code> \u2014 Function.</p> <pre><code>multigate(x::AbstractArray, ::Val{N})\n</code></pre> <p>Split up <code>x</code> into <code>N</code> equally sized chunks (along dimension <code>1</code>).</p> <p>source</p> <p># <code>Lux.replicate</code> \u2014 Function.</p> <pre><code>replicate(rng::AbstractRNG)\nreplicate(rng::CUDA.RNG)\n</code></pre> <p>Creates a copy of the <code>rng</code> state depending on its type.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/utilities/#index","title":"Index","text":"<ul> <li><code>Lux.applyactivation</code></li> <li><code>Lux.cpu</code></li> <li><code>Lux.elementwise_add</code></li> <li><code>Lux.elementwise_mul</code></li> <li><code>Lux.glorot_normal</code></li> <li><code>Lux.glorot_uniform</code></li> <li><code>Lux.gpu</code></li> <li><code>Lux.istraining</code></li> <li><code>Lux.kaiming_normal</code></li> <li><code>Lux.kaiming_uniform</code></li> <li><code>Lux.multigate</code></li> <li><code>Lux.ones32</code></li> <li><code>Lux.rand32</code></li> <li><code>Lux.randn32</code></li> <li><code>Lux.replicate</code></li> <li><code>Lux.zeros32</code></li> </ul>"},{"location":"devdocs/layer_implementation/","title":"Layer Implementation","text":""},{"location":"devdocs/layer_implementation/#layer-implementation","title":"Layer Implementation","text":""},{"location":"devdocs/layer_implementation/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":""},{"location":"devdocs/layer_implementation/#cell-implementations","title":"Cell Implementations","text":""},{"location":"devdocs/layer_implementation/#explicit-management-on-end-user-side","title":"Explicit Management on End-User Side","text":"<p>Note</p> <p>We currently use this implementation</p> <p>User is responsible for managing the memory and hidden states.</p> <p></p> <p></p>"},{"location":"devdocs/layer_implementation/#pros","title":"Pros","text":"<ol> <li>Simple Design and Implementation.</li> <li> <p>Hard for the User to mess up, i.e. there is no explicit requirement to call things like <code>Flux.reset!</code>.</p> <ul> <li>In the first call user passes the <code>input</code>.</li> <li>In the subsequent calls, the user passes a tuple containing the <code>input</code>, <code>hidden_state</code> and <code>memory</code> (if needed).</li> </ul> </li> </ol> <p></p> <p></p>"},{"location":"devdocs/layer_implementation/#cons","title":"Cons","text":"<ol> <li>Requires more explicit management from the user which might make it harder to use.</li> <li>Currently the call order convention is not enforced which could lead to sneaky errors. (Implementing a check is quite trivial if we store a call counter in the model <code>state</code>).</li> </ol>"},{"location":"devdocs/layer_implementation/#store-hidden-state-and-memory-in-model-state","title":"Store Hidden State and Memory in Model State","text":"<p>Storing the memory and hidden state in <code>st</code> would allow user to just pass <code>x</code> without varying how calls are made at different timesteps.</p> <p></p>"},{"location":"devdocs/layer_implementation/#pros_1","title":"Pros","text":"<ol> <li>Easier for the end-user.</li> </ol>"},{"location":"devdocs/layer_implementation/#cons_1","title":"Cons","text":"<ol> <li> <p><code>reset</code>ing the hidden-state and memory is slightly tricky.</p> <ol> <li>One way would be to store a <code>initial_hidden_state</code> and <code>initial_memory</code> in the state alongside the <code>hidden_state</code> and <code>memory</code>.</li> </ol> </li> </ol>"},{"location":"devdocs/style_guide/","title":"Style Guide","text":""},{"location":"devdocs/style_guide/#style-guide","title":"Style Guide","text":"<p>We strictly enforce a style guide across the repository. For the most part we rely on SciMLStyle. However, any additional guideline mentioned in this document takes precedence.</p> <p>How to auto-format your code?</p> <p>Firstly, install <code>JuliaFormatter</code> by running <code>julia -e 'using Pkg; Pkg.add(PackageSpec(name=\"JuliaFormatter\"))'</code>. Next, from the root directory of the project, simply run <code>julia -e 'using JuliaFormatter; format(\".\")'</code>.</p> <p>We do have automatic formatter, which opens PR after fixing common style issues, however, we strictly don't merge PRs without a green style check.</p> <p>Note</p> <p>If you find any existing code which doesn't adhere to these guidelines, open an issue so that we can fix that.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#code-styling","title":"Code Styling","text":"<ul> <li>Keyword Arguments must be separated using a semicolon <code>;</code></li> <li>Functions must use <code>return</code>. Returning the last value is quite ambiguous \u2013 did the author actually want it returned?</li> <li>Format docstrings as you would format regular code. If the docstring constains LaTeX in multiple lines, use <code>math</code> block.</li> <li>No avoiding multiply symbol \u2013 so <code>2x</code> is invalid instead do it like other languages <code>2 * x</code>.</li> </ul>"},{"location":"devdocs/style_guide/#testing","title":"Testing","text":"<p>Note</p> <p>Unfortunately we haven't yet tested all the functionality in the base library using these guidelines.</p> <ul> <li>The file structure of the <code>test</code> folder should mirror that of the <code>src</code> folder. Every file in src should have a complementary file in the test folder, containing tests relevant to that file's contents.</li> <li>Add generic utilities for testing in <code>test/test_utils.jl</code> and include them in the relevant files.</li> <li>Use JET.jl to test for dynamic dispatch in the functionality you added, specifically use <code>run_JET_tests</code> from <code>test/test_utils.jl</code>.</li> <li>Always test for gradient correctness. Zygote can be notorious for incorrect gradients, so add tests using <code>test_gradient_correctness_fdm</code> for finite differencing or use any other AD framework and tally the results.</li> </ul> <p></p> <p></p>"},{"location":"devdocs/style_guide/#try-adding-to-backend-packages","title":"Try adding to backend packages","text":"<p>Lux is mostly a frontend for defining Neural Networks. As such, if an optimization needs to be applied to lets say <code>NNlib.jl</code>, it is better to open a PR there since all frameworks using <code>NNlib.jl</code> get to benefit from these fixes.</p> <p>Similarly, if a bug comes to the forefront from one of the backend packages, make sure to  open a corresponding issue there to ensure they are appropriately tracked.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#mutability","title":"Mutability","text":"<p>This is strictly enforced, i.e. all layers/functions provided as part of the external API must be pure functions, even if they come with a performance penalty.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#branching-generated-functions","title":"Branching \u2013 Generated Functions","text":"<p>Zygote doesn't like branches in code. Like it or not, we are stuck with it for the near future. Even if julia is able to optimize branches away, Zygote will most certainly throw away those optimizations (these can be tested via <code>Zygote.@code_ir</code>).</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#writing-efficient-non-branching-code-to-make-zygote-happy","title":"Writing efficient non-branching code to make Zygote happy","text":"<ul> <li> <p>Rely on <code>@generated</code> functions to remove most runtime branching. Certain examples:</p> <ul> <li>Layers behaving differently during training and inference \u2013 we know at compile-time whether a layer is being run in training/inference mode via <code>istraining(st)</code>.</li> <li>Composite Layers relying on a variable number of internal layers \u2013 Again we know the length of the number of internal layers at compile time. Hence we can manually unroll the loops. See <code>Parallel</code>, <code>Chain</code>, etc.</li> <li>Pass around <code>Val</code> in state. <code>Flux.jl</code> sets <code>training</code> to be <code>(:auto, true, false)</code>. Hence, which branch will be evaluated, will have to be determined at runtime time (bad). Instead if we pass <code>Val(true)</code>, we will be able to specialize functions directly based on <code>true</code>, <code>false</code>, etc. ensuring there is no runtime cost for these operations. See <code>BatchNorm</code>, <code>Dropout</code>, etc.</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"devdocs/style_guide/#deprecation","title":"Deprecation","text":"<p>Deprecations should be handled according to SemVer recommendations, i.e. there should be atleast one version where we throw a deprecation warning. This ensures users know how to modify their code for upcoming releases.</p> <p>This blog details the process of deprecating functionalities in Julia packages. We follow the same process. Some additional guidelines are:</p> <ul> <li>Add tests using <code>Test.@test_deprecated</code> to ensure that deprecations are indeed working as expected.</li> <li>Add a warning to the documentation about deprecations (and how to use the new recommended functionality).</li> <li>Add <code># Deprecated Functionality (Remove in &lt;VERSION NUMBER&gt;)</code> before the tests and deprecated functionality not placed in <code>src/deprecated.jl</code> (like kwarg deprecations). This makes it easier to search and delete the functionalities before making a breaking release.</li> </ul> <p></p> <p></p>"},{"location":"devdocs/style_guide/#documentation","title":"Documentation","text":"<p>We use <code>Documenter.jl</code> + <code>mkdocs</code> for our documentation.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#adding-tutorials","title":"Adding Tutorials","text":"<p>Add tutorials must be added to the <code>examples</code> directory. Then add an entry for the path and tutorial name in <code>docs/make.jl</code>. Finally, update the navigation <code>nav</code> in <code>docs/mkdocs.yml</code></p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#documentation-for-layers","title":"Documentation for Layers","text":"<p>The first line must be indented by 4 spaces and should contain the possible ways to construct the layer. This should be followed up with a description about what the layer does. If mathematical equations are needed to explain what the layer does, go for it. Often times we fuse parameters to make computation faster, this should be reflected in the equations being used, i.e. equations and the internal code must be consistent. (See <code>LSTMCell</code>, <code>GRUCell</code> for some examples.)</p> <p>Note</p> <p>There is no need to document how the layers are being called since they must adhere to <code>layer(x, ps, st)</code>. Any deviation from that and the PR will not be accepted.</p> <p>Next, we will have certain subsections (though all of them might not be necessary for all layers).</p> <ul> <li> <p>Arguments: This section should be present unless the layer is constructed without any arguments (See <code>NoOpLayer</code>). All the arguments and their explicit constraints must be explained.</p> <ul> <li>It is recommended to separate out the Keyword Arguments in their own section.</li> <li>Inputs: This section should always be present. List out the requirements <code>x</code> needs to satisfy. (Don't write about <code>ps</code> and <code>st</code> since that is expected by default.)</li> <li>Returns: What will the layer return? We know the second element will be a state but is that updated in any form or not?</li> <li>Parameters: What are the properties of the NamedTuple returned from <code>initialparameters</code>? Omit if the layer is parameterless.</li> <li>States: What are the properties of the NamedTuple returned from <code>initialstates</code>? Omit if the layer is stateless.</li> </ul> </li> </ul>"},{"location":"devdocs/subpackages/","title":"Sub Packages","text":""},{"location":"devdocs/subpackages/#subpackages","title":"SubPackages","text":"<p><code>Lux.jl</code> operates somewhat like a monorepo having a weird inter-dependency structure. So adding new subpackages in <code>lib</code> can be somewhat complicated. Here are the guidelines that need to be followed:</p> <p></p> <p></p>"},{"location":"devdocs/subpackages/#package-structure","title":"Package Structure","text":"<ul> <li>Each subpackage should be in its own directory in <code>lib</code>.</li> <li>Don't have a <code>docs</code> directory (see the Documentation Section for details).</li> <li>Add a <code>LICENSE</code> file (needed to register the package independently).</li> </ul>"},{"location":"devdocs/subpackages/#workflows","title":"Workflows","text":"<ul> <li>All workflows should go in the <code>.github/workflows</code> directory (in the project root).</li> <li> <p>For <code>CI.yml</code> and <code>CINightly.yml</code></p> <ul> <li>add the project name to <code>group</code> matrix.</li> <li>Under <code>directories</code> for <code>julia-actions/julia-processcoverage@v1</code> add <code>lib/&lt;project name&gt;/src</code>.</li> <li>For <code>CompatHelper.yml</code> add <code>lib/&lt;project name&gt;</code> to the list of <code>subdirs</code>.</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"devdocs/subpackages/#documentation","title":"Documentation","text":"<ul> <li>Create a directory for the package: <code>docs/src/lib/&lt;project name&gt;</code>.</li> <li>Optionally, if you want the <code>index.md</code> page for your subpackage to be same as the <code>README.md</code> file, add the package name to <code>_setup_subdir_pkgs_index_file.([...])</code> in <code>docs/make.jl</code>.</li> <li> <p>For generating documentation (say from docstrings) for your package, you need to update the documentation pipeline:</p> <ul> <li>In <code>.github/workflows/Documentation.yml</code> under <code>Install documentation dependencies</code> install the package using <code>Pkg.develop(PackageSpec(path=joinpath(pwd(), \"lib/&lt;project name&gt;\")))</code>.</li> <li>Add a dependency in <code>docs/Project.toml</code> (don't add compat entries).</li> <li>Add <code>using &lt;Project Name&gt;</code> in <code>docs/make.jl</code>.</li> <li>Add the package name to <code>modules</code> in <code>docs/make.jl</code>.</li> <li>For every new page that you have added (including <code>index.md</code> if using <code>README.md</code> file) update <code>docs/mkdocs.yml</code> <code>nav</code> field.</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"devdocs/subpackages/#testing","title":"Testing","text":"<ul> <li>For testing, always use <code>Project.toml</code> in the <code>lib/&lt;project name&gt;/test</code> directory.</li> <li>Write the tests as you would for a normal package.</li> <li>In <code>test/runtests.jl</code> add the package name to <code>groups</code> if <code>GROUP == \"All\"</code>.</li> <li> <p>Next list any cross-dependency. When CI is run, it uses the local version of the package instead of the registered version.</p> <ul> <li>For example, <code>Lux</code> depends on <code>LuxLib</code> so <code>\"Lux\" =&gt; [_get_lib_path(\"LuxLib\")]</code></li> <li><code>Boltz</code> depends on both <code>Lux</code> and <code>LuxLib</code> (via <code>Lux</code>) so <code>\"Boltz\" =&gt; [_get_lib_path(\"LuxLib\"), dirname(@__DIR__)]</code></li> <li>If there are no cross-dependencies, remember to add an empty vector.</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"devdocs/subpackages/#registration","title":"Registration","text":"<p>Registration is simply, just run <code>@JuliaRegistrator register subdir=\"lib/&lt;project name&gt;\"</code></p> <p></p> <p></p>"},{"location":"devdocs/subpackages/#code-coverage","title":"Code Coverage","text":"<p>Add the following to <code>.codecov.yml</code>:</p> <pre><code>    - name: &lt;package name&gt;\npaths: - lib/&lt;package name&gt;/**\n</code></pre> <p>If you have performed all the steps correctly the code coverage for the subpackage will be available under the flag <code>&lt;project name&gt;</code>.</p>"},{"location":"examples/examples/","title":"Home","text":""},{"location":"examples/examples/#tutorials-examples-using-lux","title":"Tutorials &amp; Examples using Lux","text":""},{"location":"examples/examples/#tutorials","title":"Tutorials","text":"<ul> <li>Julia &amp; Lux for the Uninitiated</li> <li>Fitting a Simple Polynomial</li> <li>Training a Simple LSTM</li> <li>MNIST Classification using NeuralODE</li> <li>Bayesian Neural Network</li> <li>Hyper Network on MNIST and FashionMNIST</li> </ul>"},{"location":"examples/examples/#scipts","title":"Scipts","text":"<ul> <li>ImageNet Classification</li> </ul>"},{"location":"examples/examples/#packages","title":"Packages","text":"<p>See Ecosystem for more details.</p>"},{"location":"examples/generated/beginner/Basics/main/","title":"Julia & Lux for the Uninitiated","text":""},{"location":"examples/generated/beginner/Basics/main/#julia-lux-for-the-uninitiated","title":"Julia &amp; Lux for the Uninitiated","text":"<p>This is a quick intro to Lux loosely based on:</p> <ol> <li>PyTorch's tutorial.</li> <li>Flux's tutorial.</li> <li>Flax's tutorial.</li> </ol> <p>It introduces basic Julia programming, as well <code>Zygote</code>, a source-to-source automatic differentiation (AD) framework in Julia. We'll use these tools to build a very simple neural network. Let's start with importing <code>Lux.jl</code></p> <pre><code>using Lux, Random\n</code></pre> <pre><code>  Activating project at `~/work/Lux.jl/Lux.jl/examples`\n</code></pre> <p>Now let us control the randomness in our code using proper Pseudo Random Number Generator (PRNG)</p> <pre><code>rng = Random.default_rng()\nRandom.seed!(rng, 0)\n</code></pre> <pre><code>Random.TaskLocalRNG()\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#arrays","title":"Arrays","text":"<p>The starting point for all of our models is the <code>Array</code> (sometimes referred to as a <code>Tensor</code> in other frameworks). This is really just a list of numbers, which might be arranged into a shape like a square. Let's write down an array with three elements.</p> <pre><code>x = [1, 2, 3]\n</code></pre> <pre><code>3-element Vector{Int64}:\n 1\n 2\n 3\n</code></pre> <p>Here's a matrix \u2013 a square array with four elements.</p> <pre><code>x = [1 2; 3 4]\n</code></pre> <pre><code>2\u00d72 Matrix{Int64}:\n 1  2\n 3  4\n</code></pre> <p>We often work with arrays of thousands of elements, and don't usually write them down by hand. Here's how we can create an array of 5\u00d73 = 15 elements, each a random number from zero to one.</p> <pre><code>x = rand(rng, 5, 3)\n</code></pre> <pre><code>5\u00d73 Matrix{Float64}:\n 0.455238   0.746943   0.193291\n 0.547642   0.746801   0.116989\n 0.773354   0.97667    0.899766\n 0.940585   0.0869468  0.422918\n 0.0296477  0.351491   0.707534\n</code></pre> <p>There's a few functions like this; try replacing <code>rand</code> with <code>ones</code>, <code>zeros</code>, or <code>randn</code>.</p> <p>By default, Julia works stores numbers is a high-precision format called <code>Float64</code>. In ML we often don't need all those digits, and can ask Julia to work with <code>Float32</code> instead. We can even ask for more digits using <code>BigFloat</code>.</p> <pre><code>x = rand(BigFloat, 5, 3)\n</code></pre> <pre><code>5\u00d73 Matrix{BigFloat}:\n 0.981339    0.793159  0.459019\n 0.043883    0.624384  0.56055\n 0.164786    0.524008  0.0355555\n 0.414769    0.577181  0.621958\n 0.00823197  0.30215   0.655881\n</code></pre> <pre><code>x = rand(Float32, 5, 3)\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 0.567794   0.369178   0.342539\n 0.0985227  0.201145   0.587206\n 0.776598   0.148248   0.0851708\n 0.723731   0.0770206  0.839303\n 0.404728   0.230954   0.679087\n</code></pre> <p>We can ask the array how many elements it has.</p> <pre><code>length(x)\n</code></pre> <pre><code>15\n</code></pre> <p>Or, more specifically, what size it has.</p> <pre><code>size(x)\n</code></pre> <pre><code>(5, 3)\n</code></pre> <p>We sometimes want to see some elements of the array on their own.</p> <pre><code>x\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 0.567794   0.369178   0.342539\n 0.0985227  0.201145   0.587206\n 0.776598   0.148248   0.0851708\n 0.723731   0.0770206  0.839303\n 0.404728   0.230954   0.679087\n</code></pre> <pre><code>x[2, 3]\n</code></pre> <pre><code>0.58720636f0\n</code></pre> <p>This means get the second row and the third column. We can also get every row of the third column.</p> <pre><code>x[:, 3]\n</code></pre> <pre><code>5-element Vector{Float32}:\n 0.34253937\n 0.58720636\n 0.085170805\n 0.8393034\n 0.67908657\n</code></pre> <p>We can add arrays, and subtract them, which adds or subtracts each element of the array.</p> <pre><code>x + x\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 1.13559   0.738356  0.685079\n 0.197045  0.40229   1.17441\n 1.5532    0.296496  0.170342\n 1.44746   0.154041  1.67861\n 0.809456  0.461908  1.35817\n</code></pre> <pre><code>x - x\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n</code></pre> <p>Julia supports a feature called broadcasting, using the <code>.</code> syntax. This tiles small arrays (or single numbers) to fill bigger ones.</p> <pre><code>x .+ 1\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 1.56779  1.36918  1.34254\n 1.09852  1.20114  1.58721\n 1.7766   1.14825  1.08517\n 1.72373  1.07702  1.8393\n 1.40473  1.23095  1.67909\n</code></pre> <p>We can see Julia tile the column vector <code>1:5</code> across all rows of the larger array.</p> <pre><code>zeros(5, 5) .+ (1:5)\n</code></pre> <pre><code>5\u00d75 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 2.0  2.0  2.0  2.0  2.0\n 3.0  3.0  3.0  3.0  3.0\n 4.0  4.0  4.0  4.0  4.0\n 5.0  5.0  5.0  5.0  5.0\n</code></pre> <p>The x' syntax is used to transpose a column <code>1:5</code> into an equivalent row, and Julia will tile that across columns.</p> <pre><code>zeros(5, 5) .+ (1:5)'\n</code></pre> <pre><code>5\u00d75 Matrix{Float64}:\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n</code></pre> <p>We can use this to make a times table.</p> <pre><code>(1:5) .* (1:5)'\n</code></pre> <pre><code>5\u00d75 Matrix{Int64}:\n 1   2   3   4   5\n 2   4   6   8  10\n 3   6   9  12  15\n 4   8  12  16  20\n 5  10  15  20  25\n</code></pre> <p>Finally, and importantly for machine learning, we can conveniently do things like matrix multiply.</p> <pre><code>W = randn(5, 10)\nx = rand(10)\nW * x\n</code></pre> <pre><code>5-element Vector{Float64}:\n  1.2197981041108443\n -2.6262587710059595\n -2.8573820474674845\n -2.4319346874291305\n  1.010866857715021\n</code></pre> <p>Julia's arrays are very powerful, and you can learn more about what they can do here.</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#cuda-arrays","title":"CUDA Arrays","text":"<p>CUDA functionality is provided separately by the CUDA.jl package. If you have a GPU and CUDA available, <code>Lux</code> will automatically build the required CUDA dependencies using <code>CUDA.jl</code>.</p> <p>You can manually add <code>CUDA</code>. Once CUDA is loaded you can move any array to the GPU with the <code>cu</code> function (or the <code>gpu</code> function exported by `Lux``), and it supports all of the above operations with the same syntax.</p> <pre><code># using CUDA\n# x = cu(rand(5, 3))\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#immutability","title":"(Im)mutability","text":"<p>Lux as you might have read is Immutable by convention which means that the core library is built without any form of mutation and all functions are pure. However, we don't enforce it in any form. We do strongly recommend that users extending this framework for their respective applications don't mutate their arrays.</p> <pre><code>x = reshape(1:8, 2, 4)\n</code></pre> <pre><code>2\u00d74 reshape(::UnitRange{Int64}, 2, 4) with eltype Int64:\n 1  3  5  7\n 2  4  6  8\n</code></pre> <p>To update this array, we should first copy the array.</p> <pre><code>x_copy = copy(x)\nview(x_copy, :, 1) .= 0\n\nprintln(\"Original Array \", x)\nprintln(\"Mutated Array \", x_copy)\n</code></pre> <pre><code>Original Array [1 3 5 7; 2 4 6 8]\nMutated Array [0 3 5 7; 0 4 6 8]\n</code></pre> <p>Note that our current default AD engine (Zygote) is unable to differentiate through this mutation, however, for these specialized cases it is quite trivial to write custom backward passes. (This problem will be fixed once we move towards Enzyme.jl)</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#managing-randomness","title":"Managing Randomness","text":"<p>We rely on the Julia StdLib <code>Random</code> for managing the randomness in our execution. First, we create an PRNG (pseudorandom number generator) and seed it.</p> <pre><code>rng = Random.default_rng() # Creates a Xoshiro PRNG\nRandom.seed!(rng, 0)\n</code></pre> <pre><code>Random.TaskLocalRNG()\n</code></pre> <p>If we call any function that relies on <code>rng</code> and uses it via <code>randn</code>, <code>rand</code>, etc. <code>rng</code> will be mutated. As we have already established we care a lot about immutability, hence we should use <code>Lux.replicate</code> on PRNGs before using them.</p> <p>First, let us run a random number generator 3 times with the <code>replicate</code>d rng.</p> <pre><code>for i in 1:3\nprintln(\"Iteration $i \", rand(Lux.replicate(rng), 10))\nend\n</code></pre> <pre><code>Iteration 1 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\nIteration 2 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\nIteration 3 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\n</code></pre> <p>As expected we get the same output. We can remove the <code>replicate</code> call and we will get different outputs.</p> <pre><code>for i in 1:3\nprintln(\"Iteration $i \", rand(rng, 10))\nend\n</code></pre> <pre><code>Iteration 1 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\nIteration 2 [0.018743665453639813, 0.8601828553599953, 0.6556360448565952, 0.7746656838366666, 0.7817315740767116, 0.5553797706980106, 0.1261990389976131, 0.4488101521328277, 0.624383955429775, 0.05657739601024536]\nIteration 3 [0.19597391412112541, 0.6830945313415872, 0.6776220912718907, 0.6456416023530093, 0.6340362477836592, 0.5595843665394066, 0.5675557670686644, 0.34351700231383653, 0.7237308297251812, 0.3691778381831775]\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#automatic-differentiation","title":"Automatic Differentiation","text":"<p>Julia has quite a few (maybe too many) AD tools. For the purpose of this tutorial, we will use AbstractDifferentiation.jl which provides a uniform API across multiple AD backends. For the backends we will use:</p> <ol> <li>ForwardDiff.jl \u2013 For Jacobian-Vector Product (JVP)</li> <li>Zygote.jl \u2013 For Vector-Jacobian Product (VJP)</li> </ol> <p>Slight Detour: We have had several questions regarding if we will be considering any other AD system for the reverse-diff backend. For now we will stick to Zygote.jl, however once Enzyme.jl has support for custom rules and we have tested Lux extensively with it, we will make the switch.</p> <p>Even though, theoretically, a VJP (Vector-Jacobian product - reverse autodiff) and a JVP (Jacobian-Vector product - forward-mode autodiff) are similar\u2014they compute a product of a Jacobian and a vector\u2014they differ by the computational complexity of the operation. In short, when you have a large number of parameters (hence a wide matrix), a JVP is less efficient computationally than a VJP, and, conversely, a JVP is more efficient when the Jacobian matrix is a tall matrix.</p> <pre><code>using ForwardDiff, Zygote, AbstractDifferentiation\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#gradients","title":"Gradients","text":"<p>For our first example, consider a simple function computing \\(f(x) = \\frac{1}{2}x^T x\\), where \\(\\nabla f(x) = x\\)</p> <pre><code>f(x) = x' * x / 2\n\u2207f(x) = x  # `\u2207` can be typed as `\\nabla&lt;TAB&gt;`\nv = randn(rng, Float32, 4)\n</code></pre> <pre><code>4-element Vector{Float32}:\n -0.4051151\n -0.4593922\n  0.92155594\n  1.1871622\n</code></pre> <p>Let's use AbstractDifferentiation and Zygote to compute the gradients.</p> <pre><code>println(\"Actual Gradient: \", \u2207f(v))\nprintln(\"Computed Gradient via Reverse Mode AD (Zygote): \",\nAD.gradient(AD.ZygoteBackend(), f, v)[1])\nprintln(\"Computed Gradient via Forward Mode AD (ForwardDiff): \",\nAD.gradient(AD.ForwardDiffBackend(), f, v)[1])\n</code></pre> <pre><code>Actual Gradient: Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]\nComputed Gradient via Reverse Mode AD (Zygote): Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]\nComputed Gradient via Forward Mode AD (ForwardDiff): Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]\n</code></pre> <p>Note that <code>AD.gradient</code> will only work for scalar valued outputs.</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#jacobian-vector-product","title":"Jacobian-Vector Product","text":"<p>I will defer the discussion on forward-mode AD to https://book.sciml.ai/notes/08/. Here let us just look at a mini example on how to use it.</p> <pre><code>f(x) = x .* x ./ 2\nx = randn(rng, Float32, 5)\nv = ones(Float32, 5)\n</code></pre> <pre><code>5-element Vector{Float32}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n</code></pre> <p>Construct the pushforward function.</p> <pre><code>pf_f = AD.value_and_pushforward_function(AD.ForwardDiffBackend(), f, x)\n</code></pre> <pre><code>#17 (generic function with 1 method)\n</code></pre> <p>Compute the jvp.</p> <pre><code>val, jvp = pf_f(v)\nprintln(\"Computed Value: f(\", x, \") = \", val)\nprintln(\"JVP: \", jvp[1])\n</code></pre> <pre><code>Computed Value: f(Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]) = Float32[0.3850005, 0.71437216, 0.0016247969, 0.031389393, 0.0043726736]\nJVP: Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#vector-jacobian-product","title":"Vector-Jacobian Product","text":"<p>Using the same function and inputs, let us compute the VJP.</p> <pre><code>pb_f = AD.value_and_pullback_function(AD.ZygoteBackend(), f, x)\n</code></pre> <pre><code>#25 (generic function with 1 method)\n</code></pre> <p>Compute the vjp.</p> <pre><code>val, vjp = pb_f(v)\nprintln(\"Computed Value: f(\", x, \") = \", val)\nprintln(\"VJP: \", vjp[1])\n</code></pre> <pre><code>Computed Value: f(Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]) = Float32[0.3850005, 0.71437216, 0.0016247969, 0.031389393, 0.0043726736]\nVJP: Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#linear-regression","title":"Linear Regression","text":"<p>Finally, now let us consider a linear regression problem. From a set of data-points \\(\\left\\{ (x_i, y_i), i \\in \\left\\{ 1, \\dots, k \\right\\}, x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}^m \\right\\}\\), we try to find a set of parameters \\(W\\) and \\(b\\), s.t. \\(f_{W,b}(x) = Wx + b\\), which minimizes the mean squared error:</p> \\[ L(W, b) \\longrightarrow \\sum_{i = 1}^{k} \\frac{1}{2} \\| y_i - f_{W,b}(x_i) \\|_2^2 \\] <p>We can write <code>f</code> from scratch, but to demonstrate <code>Lux</code>, let us use the <code>Dense</code> layer.</p> <pre><code>model = Dense(10 =&gt; 5)\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n</code></pre> <pre><code>Random.TaskLocalRNG()\n</code></pre> <p>Let us initialize the parameters and states (in this case it is empty) for the model.</p> <pre><code>ps, st = Lux.setup(rng, model)\nps = ps |&gt; Lux.ComponentArray\n</code></pre> <pre><code>ComponentVector{Float32}(weight = Float32[-0.5583162 0.3457679 \u2026 -0.35419345 0.039559156; -0.05661944 -0.4899126 \u2026 0.22614014 0.27704597; \u2026 ; 0.06026341 -0.11202827 \u2026 0.42526972 -0.3576447; 0.23414856 -0.5949539 \u2026 0.08254115 -0.5224755], bias = Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;])\n</code></pre> <p>Set problem dimensions.</p> <pre><code>n_samples = 20\nx_dim = 10\ny_dim = 5\n</code></pre> <pre><code>5\n</code></pre> <p>Generate random ground truth W and b.</p> <pre><code>W = randn(rng, Float32, y_dim, x_dim)\nb = randn(rng, Float32, y_dim)\n</code></pre> <pre><code>5-element Vector{Float32}:\n  0.68468636\n -0.57578707\n  0.0594993\n -0.9436797\n  1.5164032\n</code></pre> <p>Generate samples with additional noise.</p> <pre><code>x_samples = randn(rng, Float32, x_dim, n_samples)\ny_samples = W * x_samples .+ b .+ 0.01f0 .* randn(rng, Float32, y_dim, n_samples)\nprintln(\"x shape: \", size(x_samples), \"; y shape: \", size(y_samples))\n</code></pre> <pre><code>x shape: (10, 20); y shape: (5, 20)\n</code></pre> <p>For updating our parameters let's use Optimisers.jl. We will use Stochastic Gradient Descent (SGD) with a learning rate of <code>0.01</code>.</p> <pre><code>using Optimisers\n\nopt = Optimisers.Descent(0.01f0)\n</code></pre> <pre><code>Optimisers.Descent{Float32}(0.01f0)\n</code></pre> <p>Initialize the initial state of the optimiser</p> <pre><code>opt_state = Optimisers.setup(opt, ps)\n</code></pre> <pre><code>Leaf(Descent{Float32}(0.01), nothing)\n</code></pre> <p>Define the loss function</p> <pre><code>mse(model, ps, st, X, y) = sum(abs2, model(X, ps, st)[1] .- y)\nmse(weight, bias, X, y) = sum(abs2, weight * X .+ bias .- y)\nloss_function(ps, X, y) = mse(model, ps, st, X, y)\n\nprintln(\"Loss Value with ground true parameters: \", mse(W, b, x_samples, y_samples))\n\nfor i in 1:100\n# In actual code, don't use globals. But here I will simply for the sake of\n# demonstration\nglobal ps, st, opt_state\n# Compute the gradient\ngs = gradient(loss_function, ps, x_samples, y_samples)[1]\n# Update model parameters\nopt_state, ps = Optimisers.update(opt_state, ps, gs)\nif i % 10 == 1 || i == 100\nprintln(\"Loss Value after $i iterations: \",\nmse(model, ps, st, x_samples, y_samples))\nend\nend\n</code></pre> <pre><code>Loss Value with ground true parameters: 0.009175307\nLoss Value after 1 iterations: 165.57005\nLoss Value after 11 iterations: 4.351237\nLoss Value after 21 iterations: 0.6856849\nLoss Value after 31 iterations: 0.15421417\nLoss Value after 41 iterations: 0.041469414\nLoss Value after 51 iterations: 0.014032223\nLoss Value after 61 iterations: 0.006883738\nLoss Value after 71 iterations: 0.004938521\nLoss Value after 81 iterations: 0.004391277\nLoss Value after 91 iterations: 0.0042331247\nLoss Value after 100 iterations: 0.0041888584\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/","title":"Fitting a Polynomial","text":""},{"location":"examples/generated/beginner/PolynomialFitting/main/#fitting-a-polynomial-using-mlp","title":"Fitting a Polynomial using MLP","text":"<p>In this tutorial we will fit a MultiLayer Perceptron (MLP) on data generated from a polynomial.</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing NNlib, Optimisers, Random, Statistics, Zygote, CairoMakie, MakiePublication\n</code></pre> <pre><code>  Activating project at `~/work/Lux.jl/Lux.jl/examples`\n</code></pre>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#dataset","title":"Dataset","text":"<p>Generate 128 datapoints from the polynomial \\(y = x^2 - 2x\\).</p> <pre><code>function generate_data(rng::AbstractRNG)\nx = reshape(collect(range(-2.0f0, 2.0f0, 128)), (1, 128))\ny = evalpoly.(x, ((0, -2, 1),)) .+ randn(rng, (1, 128)) .* 0.1f0\nreturn (x, y)\nend\n</code></pre> <pre><code>generate_data (generic function with 1 method)\n</code></pre> <p>Initialize the random number generator and fetch the dataset.</p> <pre><code>rng = MersenneTwister()\nRandom.seed!(rng, 12345)\n\n(x, y) = generate_data(rng)\n</code></pre> <pre><code>(Float32[-2.0 -1.968504 \u2026 1.968504 2.0], [8.11723579535073 7.8972862806322315 \u2026 -0.21213293699653427 0.049985105882301])\n</code></pre> <p>Let's visualize the dataset</p> <pre><code>with_theme(theme_web()) do\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n\nl = lines!(ax, x[1, :], x -&gt; evalpoly(x, (0, -2, 1)); linewidth=3)\ns = scatter!(ax, x[1, :], y[1, :]; markersize=8, color=:orange, strokecolor=:black,\nstrokewidth=1)\n\naxislegend(ax, [l, s], [\"True Quadratic Function\", \"Data Points\"])\n\nreturn fig\nend\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#neural-network","title":"Neural Network","text":"<p>For this problem, you should not be using a neural network. But let's still do that!</p> <pre><code>model = Chain(Dense(1 =&gt; 16, relu), Dense(16 =&gt; 1))\n</code></pre> <pre><code>Chain(\n    layer_1 = Dense(1 =&gt; 16, relu),     # 32 parameters\n    layer_2 = Dense(16 =&gt; 1),           # 17 parameters\n)         # Total: 49 parameters,\n          #        plus 0 states, summarysize 32 bytes.\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#optimizer","title":"Optimizer","text":"<p>We will use Adam from Optimisers.jl</p> <pre><code>opt = Adam(0.03f0)\n</code></pre> <pre><code>Optimisers.Adam{Float32}(0.03f0, (0.9f0, 0.999f0), 1.1920929f-7)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#loss-function","title":"Loss Function","text":"<p>We will use the <code>Lux.Training</code> API so we need to ensure that our loss function takes 4 inputs \u2013 model, parameters, states and data. The function must return 3 values \u2013 loss, updated_state, and any computed statistics.</p> <pre><code>function loss_function(model, ps, st, data)\ny_pred, st = Lux.apply(model, data[1], ps, st)\nmse_loss = mean(abs2, y_pred .- data[2])\nreturn mse_loss, st, ()\nend\n</code></pre> <pre><code>loss_function (generic function with 1 method)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#training","title":"Training","text":"<p>First we will create a <code>Lux.Training.TrainState</code> which is essentially a convenience wrapper over parameters, states and optimizer states.</p> <pre><code>tstate = Lux.Training.TrainState(rng, model, opt; transform_variables=gpu)\n</code></pre> <pre><code>Lux.Training.TrainState{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:weight, :bias), Tuple{Matrix{Float32}, Matrix{Float32}}}, NamedTuple{(:weight, :bias), Tuple{Matrix{Float32}, Matrix{Float32}}}}}, NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}}}, NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:weight, :bias), Tuple{Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}}, NamedTuple{(:weight, :bias), Tuple{Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}}}}, Chain{NamedTuple{(:layer_1, :layer_2), Tuple{Dense{true, typeof(NNlib.relu), typeof(Lux.glorot_uniform), typeof(Lux.zeros32)}, Dense{true, typeof(identity), typeof(Lux.glorot_uniform), typeof(Lux.zeros32)}}}}}(Chain(), (layer_1 = (weight = Float32[0.36222202; 0.23371002; \u2026 ; 0.5260752; -0.07562564;;], bias = Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;]), layer_2 = (weight = Float32[-0.14330137 -0.39328107 \u2026 -0.34761065 -0.05758927], bias = Float32[0.0;;])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), (layer_1 = (weight = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], (0.9, 0.999))), bias = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], (0.9, 0.999)))), layer_2 = (weight = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0 0.0 \u2026 0.0 0.0], Float32[0.0 0.0 \u2026 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0;;], Float32[0.0;;], (0.9, 0.999))))), 0)\n</code></pre> <p>Now we will use Zygote for our AD requirements.</p> <pre><code>vjp_rule = Lux.Training.ZygoteVJP()\n</code></pre> <pre><code>Lux.Training.ZygoteVJP()\n</code></pre> <p>Finally the training loop.</p> <pre><code>function main(tstate::Lux.Training.TrainState, vjp::Lux.Training.AbstractVJP, data::Tuple,\nepochs::Int)\ndata = data .|&gt; gpu\nfor epoch in 1:epochs\ngrads, loss, stats, tstate = Lux.Training.compute_gradients(vjp, loss_function,\ndata, tstate)\n@info epoch=epoch loss=loss\ntstate = Lux.Training.apply_gradients(tstate, grads)\nend\nreturn tstate\nend\n\ntstate = main(tstate, vjp_rule, (x, y), 250)\ny_pred = cpu(Lux.apply(tstate.model, gpu(x), tstate.parameters, tstate.states)[1])\n</code></pre> <pre><code>1\u00d7128 Matrix{Float32}:\n 7.93183  7.76661  7.60138  7.43616  \u2026  -0.305276  -0.280904  -0.256532\n</code></pre> <p>Let's plot the results</p> <pre><code>with_theme(theme_web()) do\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n\nl = lines!(ax, x[1, :], x -&gt; evalpoly(x, (0, -2, 1)); linewidth=3)\ns1 = scatter!(ax, x[1, :], y[1, :]; markersize=8, color=:orange, strokecolor=:black,\nstrokewidth=1)\ns2 = scatter!(ax, x[1, :], y_pred[1, :]; markersize=8, color=:green, strokecolor=:black,\nstrokewidth=1)\n\naxislegend(ax, [l, s1, s2], [\"True Quadratic Function\", \"Actual Data\", \"Predictions\"])\n\nreturn fig\nend\n</code></pre> <p></p> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/beginner/SimpleRNN/main/","title":"Training a Simple LSTM","text":""},{"location":"examples/generated/beginner/SimpleRNN/main/#training-a-simple-lstm","title":"Training a Simple LSTM","text":"<p>In this tutorial we will go over using a recurrent neural network to classify clockwise and anticlockwise spirals. By the end of this tutorial you will be able to:</p> <ol> <li>Create custom Lux models.</li> <li>Become familiar with the Lux recurrent neural network API.</li> <li>Training using Optimisers.jl and Zygote.jl.</li> </ol> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing MLUtils, Optimisers, Zygote, NNlib, Random, Statistics\n</code></pre> <pre><code>  Activating project at `~/work/Lux.jl/Lux.jl/examples`\n</code></pre>"},{"location":"examples/generated/beginner/SimpleRNN/main/#dataset","title":"Dataset","text":"<p>We will use MLUtils to generate 500 (noisy) clockwise and 500 (noisy) anticlockwise spirals. Using this data we will create a <code>MLUtils.DataLoader</code>. Our dataloader will give us sequences of size 2 \u00d7 seqlen \u00d7 batchsize and we need to predict a binary value whether the sequence is clockwise or anticlockwise.</p> <pre><code>function get_dataloaders(; dataset_size=1000, sequence_length=50)\n# Create the spirals\ndata = [MLUtils.Datasets.make_spiral(sequence_length) for _ in 1:dataset_size]\n# Get the labels\nlabels = vcat(repeat([0.0f0], dataset_size \u00f7 2), repeat([1.0f0], dataset_size \u00f7 2))\nclockwise_spirals = [reshape(d[1][:, 1:sequence_length], :, sequence_length, 1)\nfor d in data[1:(dataset_size \u00f7 2)]]\nanticlockwise_spirals = [reshape(d[1][:, (sequence_length + 1):end], :, sequence_length,\n1) for d in data[((dataset_size \u00f7 2) + 1):end]]\nx_data = Float32.(cat(clockwise_spirals..., anticlockwise_spirals...; dims=3))\n# Split the dataset\n(x_train, y_train), (x_val, y_val) = splitobs((x_data, labels); at=0.8, shuffle=true)\n# Create DataLoaders\nreturn (\n# Use DataLoader to automatically minibatch and shuffle the data\nDataLoader(collect.((x_train, y_train)); batchsize=128, shuffle=true),\n# Don't shuffle the validation data\nDataLoader(collect.((x_val, y_val)); batchsize=128, shuffle=false))\nend\n</code></pre> <pre><code>get_dataloaders (generic function with 1 method)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#creating-a-classifier","title":"Creating a Classifier","text":"<p>We will be extending the <code>Lux.AbstractExplicitContainerLayer</code> type for our custom model since it will contain a lstm block and a classifier head.</p> <p>We pass the fieldnames <code>lstm_cell</code> and <code>classifier</code> to the type to ensure that the parameters and states are automatically populated and we don't have to define <code>Lux.initialparameters</code> and <code>Lux.initialstates</code>.</p> <p>To understand more about container layers, please look at Container Layer.</p> <pre><code>struct SpiralClassifier{L, C} &lt;:\nLux.AbstractExplicitContainerLayer{(:lstm_cell, :classifier)}\nlstm_cell::L\nclassifier::C\nend\n</code></pre> <p>We won't define the model from scratch but rather use the <code>Lux.LSTMCell</code> and <code>Lux.Dense</code>.</p> <pre><code>function SpiralClassifier(in_dims, hidden_dims, out_dims)\nreturn SpiralClassifier(LSTMCell(in_dims =&gt; hidden_dims),\nDense(hidden_dims =&gt; out_dims, sigmoid))\nend\n</code></pre> <pre><code>Main.SpiralClassifier\n</code></pre> <p>We can use default Lux blocks \u2013 <code>Recurrence(LSTMCell(in_dims =&gt; hidden_dims)</code> \u2013 instead of defining the following. But let's still do it for the sake of it.</p> <p>Now we need to define the behavior of the Classifier when it is invoked.</p> <pre><code>function (s::SpiralClassifier)(x::AbstractArray{T, 3}, ps::NamedTuple,\nst::NamedTuple) where {T}\n# First we will have to run the sequence through the LSTM Cell\n# The first call to LSTM Cell will create the initial hidden state\n# See that the parameters and states are automatically populated into a field called\n# `lstm_cell` We use `eachslice` to get the elements in the sequence without copying,\n# and `Iterators.peel` to split out the first element for LSTM initialization.\nx_init, x_rest = Iterators.peel(eachslice(x; dims=2))\n(y, carry), st_lstm = s.lstm_cell(x_init, ps.lstm_cell, st.lstm_cell)\n# Now that we have the hidden state and memory in `carry` we will pass the input and\n# `carry` jointly\nfor x in x_rest\n(y, carry), st_lstm = s.lstm_cell((x, carry), ps.lstm_cell, st_lstm)\nend\n# After running through the sequence we will pass the output through the classifier\ny, st_classifier = s.classifier(y, ps.classifier, st.classifier)\n# Finally remember to create the updated state\nst = merge(st, (classifier=st_classifier, lstm_cell=st_lstm))\nreturn vec(y), st\nend\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#defining-accuracy-loss-and-optimiser","title":"Defining Accuracy, Loss and Optimiser","text":"<p>Now let's define the binarycrossentropy loss. Typically it is recommended to use <code>logitbinarycrossentropy</code> since it is more numerically stable, but for the sake of simplicity we will use <code>binarycrossentropy</code>.</p> <pre><code>function xlogy(x, y)\nresult = x * log(y)\nreturn ifelse(iszero(x), zero(result), result)\nend\n\nfunction binarycrossentropy(y_pred, y_true)\ny_pred = y_pred .+ eps(eltype(y_pred))\nreturn mean(@. -xlogy(y_true, y_pred) - xlogy(1 - y_true, 1 - y_pred))\nend\n\nfunction compute_loss(x, y, model, ps, st)\ny_pred, st = model(x, ps, st)\nreturn binarycrossentropy(y_pred, y), y_pred, st\nend\n\nmatches(y_pred, y_true) = sum((y_pred .&gt; 0.5) .== y_true)\naccuracy(y_pred, y_true) = matches(y_pred, y_true) / length(y_pred)\n</code></pre> <pre><code>accuracy (generic function with 1 method)\n</code></pre> <p>Finally lets create an optimiser given the model parameters.</p> <pre><code>function create_optimiser(ps)\nopt = Optimisers.ADAM(0.01f0)\nreturn Optimisers.setup(opt, ps)\nend\n</code></pre> <pre><code>create_optimiser (generic function with 1 method)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#training-the-model","title":"Training the Model","text":"<pre><code>function main()\n# Get the dataloaders\n(train_loader, val_loader) = get_dataloaders()\n\n# Create the model\nmodel = SpiralClassifier(2, 8, 1)\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\nps, st = Lux.setup(rng, model)\n\n# Create the optimiser\nopt_state = create_optimiser(ps)\n\nfor epoch in 1:25\n# Train the model\nfor (x, y) in train_loader\n(loss, y_pred, st), back = pullback(p -&gt; compute_loss(x, y, model, p, st), ps)\ngs = back((one(loss), nothing, nothing))[1]\nopt_state, ps = Optimisers.update(opt_state, ps, gs)\n\nprintln(\"Epoch [$epoch]: Loss $loss\")\nend\n\n# Validate the model\nst_ = Lux.testmode(st)\nfor (x, y) in val_loader\n(loss, y_pred, st_) = compute_loss(x, y, model, ps, st_)\nacc = accuracy(y_pred, y)\nprintln(\"Validation: Loss $loss Accuracy $acc\")\nend\nend\nend\n\nmain()\n</code></pre> <pre><code>Epoch [1]: Loss 0.563091\nEpoch [1]: Loss 0.50031674\nEpoch [1]: Loss 0.46993724\nEpoch [1]: Loss 0.44585252\nEpoch [1]: Loss 0.42853004\nEpoch [1]: Loss 0.42057058\nEpoch [1]: Loss 0.38664216\nValidation: Loss 0.36785078 Accuracy 1.0\nValidation: Loss 0.36884576 Accuracy 1.0\nEpoch [2]: Loss 0.35990417\nEpoch [2]: Loss 0.35025194\nEpoch [2]: Loss 0.3402797\nEpoch [2]: Loss 0.32155994\nEpoch [2]: Loss 0.30208215\nEpoch [2]: Loss 0.28757554\nEpoch [2]: Loss 0.270198\nValidation: Loss 0.2579093 Accuracy 1.0\nValidation: Loss 0.25842893 Accuracy 1.0\nEpoch [3]: Loss 0.26158288\nEpoch [3]: Loss 0.24571519\nEpoch [3]: Loss 0.23363028\nEpoch [3]: Loss 0.22559838\nEpoch [3]: Loss 0.20717819\nEpoch [3]: Loss 0.19713923\nEpoch [3]: Loss 0.19214839\nValidation: Loss 0.18044133 Accuracy 1.0\nValidation: Loss 0.1806515 Accuracy 1.0\nEpoch [4]: Loss 0.17823191\nEpoch [4]: Loss 0.1720218\nEpoch [4]: Loss 0.16498649\nEpoch [4]: Loss 0.15748107\nEpoch [4]: Loss 0.1486668\nEpoch [4]: Loss 0.14235476\nEpoch [4]: Loss 0.13705496\nValidation: Loss 0.12927093 Accuracy 1.0\nValidation: Loss 0.1293802 Accuracy 1.0\nEpoch [5]: Loss 0.12876134\nEpoch [5]: Loss 0.1251984\nEpoch [5]: Loss 0.1158484\nEpoch [5]: Loss 0.11394659\nEpoch [5]: Loss 0.10709124\nEpoch [5]: Loss 0.104397945\nEpoch [5]: Loss 0.101136886\nValidation: Loss 0.09433798 Accuracy 1.0\nValidation: Loss 0.094438806 Accuracy 1.0\nEpoch [6]: Loss 0.09438712\nEpoch [6]: Loss 0.091230996\nEpoch [6]: Loss 0.084929906\nEpoch [6]: Loss 0.08464088\nEpoch [6]: Loss 0.079778954\nEpoch [6]: Loss 0.07585856\nEpoch [6]: Loss 0.071348645\nValidation: Loss 0.06973383 Accuracy 1.0\nValidation: Loss 0.069844626 Accuracy 1.0\nEpoch [7]: Loss 0.07121022\nEpoch [7]: Loss 0.06445556\nEpoch [7]: Loss 0.06639002\nEpoch [7]: Loss 0.061034955\nEpoch [7]: Loss 0.059253994\nEpoch [7]: Loss 0.056935493\nEpoch [7]: Loss 0.052991237\nValidation: Loss 0.05204758 Accuracy 1.0\nValidation: Loss 0.052176777 Accuracy 1.0\nEpoch [8]: Loss 0.051847912\nEpoch [8]: Loss 0.05031281\nEpoch [8]: Loss 0.047190532\nEpoch [8]: Loss 0.046727914\nEpoch [8]: Loss 0.044947643\nEpoch [8]: Loss 0.042604756\nEpoch [8]: Loss 0.041868933\nValidation: Loss 0.039163005 Accuracy 1.0\nValidation: Loss 0.039298177 Accuracy 1.0\nEpoch [9]: Loss 0.03914217\nEpoch [9]: Loss 0.038595587\nEpoch [9]: Loss 0.035607126\nEpoch [9]: Loss 0.034708958\nEpoch [9]: Loss 0.032869816\nEpoch [9]: Loss 0.033551455\nEpoch [9]: Loss 0.031799775\nValidation: Loss 0.029896418 Accuracy 1.0\nValidation: Loss 0.030038469 Accuracy 1.0\nEpoch [10]: Loss 0.031013332\nEpoch [10]: Loss 0.028027952\nEpoch [10]: Loss 0.026980385\nEpoch [10]: Loss 0.028156633\nEpoch [10]: Loss 0.026798887\nEpoch [10]: Loss 0.024615299\nEpoch [10]: Loss 0.023793802\nValidation: Loss 0.023475038 Accuracy 1.0\nValidation: Loss 0.023611596 Accuracy 1.0\nEpoch [11]: Loss 0.024837341\nEpoch [11]: Loss 0.022281062\nEpoch [11]: Loss 0.021435138\nEpoch [11]: Loss 0.02122792\nEpoch [11]: Loss 0.020889627\nEpoch [11]: Loss 0.021179266\nEpoch [11]: Loss 0.018595144\nValidation: Loss 0.01908924 Accuracy 1.0\nValidation: Loss 0.019210886 Accuracy 1.0\nEpoch [12]: Loss 0.019503364\nEpoch [12]: Loss 0.018652998\nEpoch [12]: Loss 0.018224034\nEpoch [12]: Loss 0.017759353\nEpoch [12]: Loss 0.017773928\nEpoch [12]: Loss 0.016257102\nEpoch [12]: Loss 0.017025184\nValidation: Loss 0.016035926 Accuracy 1.0\nValidation: Loss 0.01614127 Accuracy 1.0\nEpoch [13]: Loss 0.015943551\nEpoch [13]: Loss 0.015461239\nEpoch [13]: Loss 0.015734075\nEpoch [13]: Loss 0.015533627\nEpoch [13]: Loss 0.015109714\nEpoch [13]: Loss 0.014514977\nEpoch [13]: Loss 0.01250788\nValidation: Loss 0.013837993 Accuracy 1.0\nValidation: Loss 0.01393335 Accuracy 1.0\nEpoch [14]: Loss 0.013931633\nEpoch [14]: Loss 0.013579331\nEpoch [14]: Loss 0.013571744\nEpoch [14]: Loss 0.013855472\nEpoch [14]: Loss 0.012291621\nEpoch [14]: Loss 0.012526935\nEpoch [14]: Loss 0.013246046\nValidation: Loss 0.012196861 Accuracy 1.0\nValidation: Loss 0.0122803245 Accuracy 1.0\nEpoch [15]: Loss 0.012557793\nEpoch [15]: Loss 0.011444303\nEpoch [15]: Loss 0.012170338\nEpoch [15]: Loss 0.0116443\nEpoch [15]: Loss 0.012070053\nEpoch [15]: Loss 0.011057332\nEpoch [15]: Loss 0.010814285\nValidation: Loss 0.010909624 Accuracy 1.0\nValidation: Loss 0.010984318 Accuracy 1.0\nEpoch [16]: Loss 0.011304823\nEpoch [16]: Loss 0.01015573\nEpoch [16]: Loss 0.011124623\nEpoch [16]: Loss 0.010359224\nEpoch [16]: Loss 0.010782347\nEpoch [16]: Loss 0.010069233\nEpoch [16]: Loss 0.009509276\nValidation: Loss 0.009867142 Accuracy 1.0\nValidation: Loss 0.009936609 Accuracy 1.0\nEpoch [17]: Loss 0.010023102\nEpoch [17]: Loss 0.009115362\nEpoch [17]: Loss 0.010314796\nEpoch [17]: Loss 0.00975236\nEpoch [17]: Loss 0.008773214\nEpoch [17]: Loss 0.00954038\nEpoch [17]: Loss 0.0101304315\nValidation: Loss 0.00900226 Accuracy 1.0\nValidation: Loss 0.009066705 Accuracy 1.0\nEpoch [18]: Loss 0.008912059\nEpoch [18]: Loss 0.009301419\nEpoch [18]: Loss 0.008914042\nEpoch [18]: Loss 0.00886388\nEpoch [18]: Loss 0.008316556\nEpoch [18]: Loss 0.008328475\nEpoch [18]: Loss 0.009232808\nValidation: Loss 0.008267567 Accuracy 1.0\nValidation: Loss 0.008323971 Accuracy 1.0\nEpoch [19]: Loss 0.0089094825\nEpoch [19]: Loss 0.00752693\nEpoch [19]: Loss 0.008469941\nEpoch [19]: Loss 0.008529839\nEpoch [19]: Loss 0.007631449\nEpoch [19]: Loss 0.007866326\nEpoch [19]: Loss 0.0065536415\nValidation: Loss 0.007628181 Accuracy 1.0\nValidation: Loss 0.0076849023 Accuracy 1.0\nEpoch [20]: Loss 0.007379742\nEpoch [20]: Loss 0.0077172033\nEpoch [20]: Loss 0.00788015\nEpoch [20]: Loss 0.007860287\nEpoch [20]: Loss 0.006961119\nEpoch [20]: Loss 0.0069962675\nEpoch [20]: Loss 0.0078713335\nValidation: Loss 0.007080801 Accuracy 1.0\nValidation: Loss 0.0071310825 Accuracy 1.0\nEpoch [21]: Loss 0.007929705\nEpoch [21]: Loss 0.007458208\nEpoch [21]: Loss 0.006595021\nEpoch [21]: Loss 0.0068095145\nEpoch [21]: Loss 0.0061412784\nEpoch [21]: Loss 0.006820177\nEpoch [21]: Loss 0.006913939\nValidation: Loss 0.0065935333 Accuracy 1.0\nValidation: Loss 0.006642794 Accuracy 1.0\nEpoch [22]: Loss 0.0065368414\nEpoch [22]: Loss 0.0063316287\nEpoch [22]: Loss 0.0066916468\nEpoch [22]: Loss 0.006592747\nEpoch [22]: Loss 0.0062900716\nEpoch [22]: Loss 0.006324705\nEpoch [22]: Loss 0.0070462665\nValidation: Loss 0.006165847 Accuracy 1.0\nValidation: Loss 0.006210735 Accuracy 1.0\nEpoch [23]: Loss 0.0061641424\nEpoch [23]: Loss 0.0062597105\nEpoch [23]: Loss 0.006334014\nEpoch [23]: Loss 0.0058441227\nEpoch [23]: Loss 0.005930262\nEpoch [23]: Loss 0.0061859014\nEpoch [23]: Loss 0.004930711\nValidation: Loss 0.00578142 Accuracy 1.0\nValidation: Loss 0.0058240104 Accuracy 1.0\nEpoch [24]: Loss 0.0054906006\nEpoch [24]: Loss 0.0057746614\nEpoch [24]: Loss 0.0058690687\nEpoch [24]: Loss 0.0057370253\nEpoch [24]: Loss 0.0056166565\nEpoch [24]: Loss 0.0057003833\nEpoch [24]: Loss 0.0057470636\nValidation: Loss 0.005440268 Accuracy 1.0\nValidation: Loss 0.0054809535 Accuracy 1.0\nEpoch [25]: Loss 0.0054254434\nEpoch [25]: Loss 0.0059194616\nEpoch [25]: Loss 0.0052186153\nEpoch [25]: Loss 0.0050898637\nEpoch [25]: Loss 0.0052764104\nEpoch [25]: Loss 0.0052530323\nEpoch [25]: Loss 0.005522846\nValidation: Loss 0.005130264 Accuracy 1.0\nValidation: Loss 0.0051692827 Accuracy 1.0\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/intermediate/BayesianNN/main/","title":"Bayesian Neural Network","text":""},{"location":"examples/generated/intermediate/BayesianNN/main/#bayesian-neural-network","title":"Bayesian Neural Network","text":"<p>We borrow this tutorial from the official Turing Docs. We will show how the explicit parameterization of Lux enables first-class composability with packages which expect flattened out parameter vectors.</p> <p>We will use Turing.jl with Lux.jl to implement implementing a classification algorithm. Lets start by importing the relevant libraries.</p> <pre><code># Import libraries\nusing Lux\nusing Turing, CairoMakie, Random, ReverseDiff, NNlib, Functors, MakiePublication\n\n# Hide sampling progress\nTuring.setprogress!(false);\n\n# Use reverse_diff due to the number of parameters in neural networks\nTuring.setadbackend(:reversediff)\n</code></pre> <pre><code>:reversediff\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/intermediate/BayesianNN/main/#generating-data","title":"Generating data","text":"<p>Our goal here is to use a Bayesian neural network to classify points in an artificial dataset. The code below generates data points arranged in a box-like pattern and displays a graph of the dataset we'll be working with.</p> <pre><code># Number of points to generate\nN = 80\nM = round(Int, N / 4)\nrng = Random.default_rng()\nRandom.seed!(rng, 1234)\n\n# Generate artificial data\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt1s = Array([[x1s[i] + 0.5f0; x2s[i] + 0.5f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt1s, Array([[x1s[i] - 5.0f0; x2s[i] - 5.0f0] for i in 1:M]))\n\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt0s = Array([[x1s[i] + 0.5f0; x2s[i] - 5.0f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt0s, Array([[x1s[i] - 5.0f0; x2s[i] + 0.5f0] for i in 1:M]))\n\n# Store all the data for later\nxs = [xt1s; xt0s]\nts = [ones(2 * M); zeros(2 * M)]\n\n# Plot data points\n</code></pre> <pre><code>80-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n \u22ee\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n</code></pre> <p>withtheme(themeweb()) do     fig = Figure()     ax = Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")</p> <pre><code>l = lines!(ax, x[1, :], x -&gt; evalpoly(x, (0, -2, 1)); linewidth=3)\ns = scatter!(ax, x[1, :], y[1, :]; markersize=8, color=:orange, strokecolor=:black,\n             strokewidth=1)\n\naxislegend(ax, [l, s], [\"True Quadratic Function\", \"Data Points\"])\n\nreturn fig\n</code></pre> <p>end</p> <pre><code>function plot_data()\nx1 = first.(xt1s)\ny1 = last.(xt1s)\nx2 = first.(xt0s)\ny2 = last.(xt0s)\n\nfig = with_theme(theme_web()) do\nfig = Figure()\nax = CairoMakie.Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n\nscatter!(ax, x1, y1; markersize=8, color=:red, strokecolor=:black, strokewidth=1)\nscatter!(ax, x2, y2; markersize=8, color=:blue, strokecolor=:black, strokewidth=1)\n\nreturn fig\nend\n\nreturn fig\nend\n\nplot_data()\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"examples/generated/intermediate/BayesianNN/main/#building-the-neural-network","title":"Building the Neural Network","text":"<p>The next step is to define a feedforward neural network where we express our parameters as distributions, and not single points as with traditional neural networks. For this we will use <code>Dense</code> to define liner layers and compose them via <code>Chain</code>, both are neural network primitives from <code>Lux</code>. The network <code>nn</code> we will create will have two hidden layers with <code>tanh</code> activations and one output layer with <code>sigmoid</code> activation, as shown below.</p> <p>The <code>nn</code> is an instance that acts as a function and can take data, parameters and current state as inputs and output predictions. We will define distributions on the neural network parameters.</p> <pre><code># Construct a neural network using Lux\nnn = Chain(Dense(2 =&gt; 3, tanh), Dense(3 =&gt; 2, tanh), Dense(2 =&gt; 1, sigmoid))\n\n# Initialize the model weights and state\nps, st = Lux.setup(rng, nn)\n\nLux.parameterlength(nn) # number of paraemters in NN\n</code></pre> <pre><code>20\n</code></pre> <p>The probabilistic model specification below creates a parameters variable, which has IID normal variables. The parameters represents all parameters of our neural net (weights and biases).</p> <pre><code># Create a regularization term and a Gaussian prior variance term.\nalpha = 0.09\nsig = sqrt(1.0 / alpha)\n</code></pre> <pre><code>3.3333333333333335\n</code></pre> <p>Construct named tuple from a sampled parameter vector. We could also use ComponentArrays here and simply broadcast to avoid doing this. But let's do it this way to avoid dependencies.</p> <pre><code>function vector_to_parameters(ps_new::AbstractVector, ps::NamedTuple)\n@assert length(ps_new) == Lux.parameterlength(ps)\ni = 1\nfunction get_ps(x)\nz = reshape(view(ps_new, i:(i + length(x) - 1)), size(x))\ni += length(x)\nreturn z\nend\nreturn fmap(get_ps, ps)\nend\n\n# Specify the probabilistic model.\n@model function bayes_nn(xs, ts)\nglobal st\n\n# Sample the parameters\nnparameters = Lux.parameterlength(nn)\nparameters ~ MvNormal(zeros(nparameters), sig .* ones(nparameters))\n\n# Forward NN to make predictions\npreds, st = nn(xs, vector_to_parameters(parameters, ps), st)\n\n# Observe each prediction.\nfor i in 1:length(ts)\nts[i] ~ Bernoulli(preds[i])\nend\nend\n</code></pre> <pre><code>bayes_nn (generic function with 2 methods)\n</code></pre> <p>Inference can now be performed by calling sample. We use the HMC sampler here.</p> <pre><code># Perform inference.\nN = 5000\nch = sample(bayes_nn(hcat(xs...), ts), HMC(0.05, 4), N)\n</code></pre> <pre><code>Chains MCMC chain (5000\u00d729\u00d71 Array{Float64, 3}):\n\nIterations        = 1:1:5000\nNumber of chains  = 1\nSamples per chain = 5000\nWall duration     = 76.18 seconds\nCompute duration  = 76.18 seconds\nparameters        = parameters[1], parameters[2], parameters[3], parameters[4], parameters[5], parameters[6], parameters[7], parameters[8], parameters[9], parameters[10], parameters[11], parameters[12], parameters[13], parameters[14], parameters[15], parameters[16], parameters[17], parameters[18], parameters[19], parameters[20]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, step_size, nom_step_size\n\nSummary Statistics\n      parameters      mean       std   naive_se      mcse       ess      rhat  \u22ef\n          Symbol   Float64   Float64    Float64   Float64   Float64   Float64  \u22ef\n\n   parameters[1]    1.4675    2.3083     0.0326    0.2693   11.7077    2.0767  \u22ef\n   parameters[2]    5.5552    2.7594     0.0390    0.3231   12.4782    1.1899  \u22ef\n   parameters[3]    0.0273    0.7531     0.0107    0.0774   18.5830    1.1024  \u22ef\n   parameters[4]   -1.8129    1.6025     0.0227    0.1796   15.6021    1.0448  \u22ef\n   parameters[5]    0.7135    1.3562     0.0192    0.1557   13.5247    1.1842  \u22ef\n   parameters[6]    4.7089    1.7599     0.0249    0.1976   18.8575    1.0080  \u22ef\n   parameters[7]   -5.0105    3.4342     0.0486    0.4005   12.7656    1.0072  \u22ef\n   parameters[8]    0.1951    2.3736     0.0336    0.2763   12.0269    1.4253  \u22ef\n   parameters[9]    0.8814    1.6137     0.0228    0.1798   19.0441    1.0278  \u22ef\n  parameters[10]   -0.9196    4.1529     0.0587    0.4896   10.7257    2.1730  \u22ef\n  parameters[11]   -0.0995    2.6126     0.0369    0.3022   12.2529    1.2131  \u22ef\n  parameters[12]   -2.0922    3.0141     0.0426    0.3552   11.4024    1.6551  \u22ef\n  parameters[13]    4.3457    1.7685     0.0250    0.2022   14.9482    1.0385  \u22ef\n  parameters[14]   -2.9048    1.6752     0.0237    0.1877   15.8116    1.0980  \u22ef\n  parameters[15]    2.1551    1.8145     0.0257    0.2082   13.7058    1.2178  \u22ef\n  parameters[16]   -3.2932    1.4081     0.0199    0.1561   19.8059    1.0165  \u22ef\n  parameters[17]   -3.4255    2.6896     0.0380    0.3158   12.5991    1.2372  \u22ef\n        \u22ee             \u22ee         \u22ee         \u22ee          \u22ee         \u22ee         \u22ee     \u22f1\n                                                     1 column and 3 rows omitted\n\nQuantiles\n      parameters       2.5%     25.0%     50.0%     75.0%     97.5%\n          Symbol    Float64   Float64   Float64   Float64   Float64\n\n   parameters[1]    -2.8737   -0.2060    1.2397    3.2028    6.3763\n   parameters[2]     0.7006    3.3812    5.8509    7.5387   11.0436\n   parameters[3]    -2.0910   -0.2111    0.1039    0.4571    1.0501\n   parameters[4]    -5.8984   -2.5130   -1.6280   -0.7613    0.8443\n   parameters[5]    -0.7348   -0.0564    0.4240    0.9659    5.2736\n   parameters[6]     1.6909    3.4942    4.5379    5.9346    8.2718\n   parameters[7]   -11.1144   -7.4680   -5.2027   -2.8930    1.9571\n   parameters[8]    -4.3334   -1.5037    0.2996    2.0048    4.4496\n   parameters[9]    -1.9076   -0.1331    0.6428    1.7236    4.4656\n  parameters[10]    -8.4372   -4.2836    0.1960    2.2765    5.9196\n  parameters[11]    -5.1766   -1.7353    0.2017    1.6669    4.5451\n  parameters[12]    -6.9702   -4.6678   -2.4274    0.6276    3.0518\n  parameters[13]     1.2283    3.0997    4.1223    5.5624    8.0027\n  parameters[14]    -6.2349   -4.0604   -2.9481   -1.8547    0.5993\n  parameters[15]    -1.9406    1.1370    2.4385    3.4656    5.0130\n  parameters[16]    -5.6563   -4.2682   -3.5360   -2.4030    0.0173\n  parameters[17]    -9.2556   -4.9700   -3.1804   -1.7896    1.5551\n        \u22ee             \u22ee          \u22ee         \u22ee         \u22ee         \u22ee\n                                                       3 rows omitted\n</code></pre> <p>Now we extract the parameter samples from the sampled chain as theta (this is of size <code>5000 x 20</code> where <code>5000</code> is the number of iterations and <code>20</code> is the number of parameters). We'll use these primarily to determine how good our model's classifier is.</p> <pre><code># Extract all weight and bias parameters.\ntheta = MCMCChains.group(ch, :parameters).value;\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/intermediate/BayesianNN/main/#prediction-visualization","title":"Prediction Visualization","text":"<pre><code># A helper to run the nn through data `x` using parameters `theta`\nnn_forward(x, theta) = nn(x, vector_to_parameters(theta, ps), st)[1]\n\n# Plot the data we have.\nfig = plot_data()\n\n# Find the index that provided the highest log posterior in the chain.\n_, i = findmax(ch[:lp])\n\n# Extract the max row value from i.\ni = i.I[1]\n\n# Plot the posterior distribution with a contour plot\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)\nfig\n</code></pre> <p>The contour plot above shows that the MAP method is not too bad at classifying our data. Now we can visualize our predictions.</p> \\[ p(\\tilde{x} | X, \\alpha) = \\int_{\\theta} p(\\tilde{x} | \\theta) p(\\theta | X, \\alpha) \\approx \\sum_{\\theta \\sim p(\\theta | X, \\alpha)}f_{\\theta}(\\tilde{x}) \\] <p>The <code>nn_predict</code> function takes the average predicted value from a network parameterized by weights drawn from the MCMC chain.</p> <pre><code># Return the average predicted value across multiple weights.\nfunction nn_predict(x, theta, num)\nreturn mean([nn_forward(x, view(theta, i, :))[1] for i in 1:10:num])\nend\n</code></pre> <pre><code>nn_predict (generic function with 1 method)\n</code></pre> <p>Next, we use the <code>nn_predict</code> function to predict the value at a sample of points where the x1 and x2 coordinates range between -6 and 6. As we can see below, we still have a satisfactory fit to our data, and more importantly, we can also see where the neural network is uncertain about its predictions much easier\u2013-those regions between cluster boundaries.</p> <p>Plot the average prediction.</p> <pre><code>fig = plot_data()\n\nn_end = 1500\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_predict([x1, x2], theta, n_end)[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)\nfig\n</code></pre> <p></p> <p>Suppose we are interested in how the predictive power of our Bayesian neural network evolved between samples. In that case, the following graph displays an animation of the contour plot generated from the network weights in samples 1 to 1,000.</p> <pre><code># Number of iterations to plot.\nn_end = 1000\n\nfig = plot_data()\nZ = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\nc = contour!(x1_range, x2_range, Z)\ncurrent_axis(fig).title = \"Iteration 1\"\n\nCairoMakie.record(fig, joinpath(@__DIR__, \"animationbayesiannn.mp4\"), 1:5:n_end;\nframerate=60) do i\nZ = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\nc[3] = Z\nreturn current_axis(fig).title = \"Iteration $i\"\nend\n</code></pre> <pre><code>\"/home/runner/work/Lux.jl/Lux.jl/docs/docs/examples/generated/intermediate/BayesianNN/animationbayesiannn.mp4\"\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/intermediate/HyperNet/main/","title":"Training a Hyper Network","text":""},{"location":"examples/generated/intermediate/HyperNet/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing ComponentArrays, CUDA, MLDatasets, MLUtils, NNlib, OneHotArrays, Optimisers, Random,\nSetfield, Statistics, Zygote\nCUDA.allowscalar(false)\n</code></pre> <pre><code>  Activating project at `~/work/Lux.jl/Lux.jl/examples`\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#loading-datasets","title":"Loading Datasets","text":"<pre><code>function _load_dataset(dset, n_train::Int, n_eval::Int, batchsize::Int)\nimgs, labels = dset(:train)[1:n_train]\nx_train, y_train = reshape(imgs, 28, 28, 1, n_train), onehotbatch(labels, 0:9)\n\nimgs, labels = dset(:test)[1:n_eval]\nx_test, y_test = reshape(imgs, 28, 28, 1, n_eval), onehotbatch(labels, 0:9)\n\nreturn (DataLoader((x_train, y_train); batchsize=min(batchsize, n_train), shuffle=true),\nDataLoader((x_test, y_test); batchsize=min(batchsize, n_eval), shuffle=false))\nend\n\nfunction load_datasets(n_train=1024, n_eval=32, batchsize=256)\nreturn _load_dataset.((MNIST, FashionMNIST), n_train, n_eval, batchsize)\nend\n</code></pre> <pre><code>load_datasets (generic function with 4 methods)\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#implement-a-hypernet-layer","title":"Implement a HyperNet Layer","text":"<pre><code>struct HyperNet{W &lt;: Lux.AbstractExplicitLayer, C &lt;: Lux.AbstractExplicitLayer, A} &lt;:\nLux.AbstractExplicitContainerLayer{(:weight_generator, :core_network)}\nweight_generator::W\ncore_network::C\nca_axes::A\nend\n\nfunction HyperNet(w::Lux.AbstractExplicitLayer, c::Lux.AbstractExplicitLayer)\nca_axes = Lux.initialparameters(Random.default_rng(), c) |&gt; ComponentArray |&gt; getaxes\nreturn HyperNet(w, c, ca_axes)\nend\n\nfunction Lux.initialparameters(rng::AbstractRNG, h::HyperNet)\nreturn (weight_generator=Lux.initialparameters(rng, h.weight_generator),)\nend\n\nfunction (hn::HyperNet)(x, ps, st::NamedTuple) where {T &lt;: Tuple}\nps_new, st_ = hn.weight_generator(x, ps.weight_generator, st.weight_generator)\n@set! st.weight_generator = st_\nreturn ComponentArray(vec(ps_new), hn.ca_axes), st\nend\n\nfunction (hn::HyperNet)((x, y)::T, ps, st::NamedTuple) where {T &lt;: Tuple}\nps_ca, st = hn(x, ps, st)\npred, st_ = hn.core_network(y, ps_ca, st.core_network)\n@set! st.core_network = st_\nreturn pred, st\nend\n</code></pre> <pre><code>WARNING: method definition for HyperNet at main.md:54 declares type variable T but does not use it.\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#create-and-initialize-the-hypernet","title":"Create and Initialize the HyperNet","text":"<pre><code>function create_model()\n# Doesn't need to be a MLP can have any Lux Layer\ncore_network = Chain(FlattenLayer(), Dense(784, 256, relu), Dense(256, 10))\nweight_generator = Chain(Embedding(2 =&gt; 32), Dense(32, 64, relu),\nDense(64, Lux.parameterlength(core_network)))\n\nmodel = HyperNet(weight_generator, core_network)\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps, st = Lux.setup(rng, model) .|&gt; gpu\n\nreturn model, ps, st\nend\n</code></pre> <pre><code>create_model (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#define-utility-functions","title":"Define Utility Functions","text":"<pre><code>logitcrossentropy(y_pred, y) = mean(-sum(y .* logsoftmax(y_pred); dims=1))\n\nfunction loss(data_idx, x, y, model, ps, st)\ny_pred, st = model((data_idx, x), ps, st)\nreturn logitcrossentropy(y_pred, y), st\nend\n\nfunction accuracy(model, ps, st, dataloader, data_idx)\ntotal_correct, total = 0, 0\nst = Lux.testmode(st)\nfor (x, y) in dataloader\nx = x |&gt; gpu\ny = y |&gt; gpu\ntarget_class = onecold(cpu(y))\npredicted_class = onecold(cpu(model((data_idx, x), ps, st)[1]))\ntotal_correct += sum(target_class .== predicted_class)\ntotal += length(target_class)\nend\nreturn total_correct / total\nend\n</code></pre> <pre><code>accuracy (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#training","title":"Training","text":"<pre><code>function train()\nmodel, ps, st = create_model()\n\n# Training\ndataloaders = load_datasets()\n\nopt = Adam(0.001f0)\nst_opt = Optimisers.setup(opt, ps)\n\n### Warmup the Model\nimg, lab = gpu(dataloaders[1][1].data[1][:, :, :, 1:1]),\ngpu(dataloaders[1][1].data[2][:, 1:1])\nloss(1, img, lab, model, ps, st)\n(l, _), back = pullback(p -&gt; loss(1, img, lab, model, p, st), ps)\nback((one(l), nothing))\n\n### Lets train the model\nnepochs = 9\nfor epoch in 1:nepochs\nfor data_idx in 1:2\ntrain_dataloader, test_dataloader = dataloaders[data_idx]\n\nstime = time()\nfor (x, y) in train_dataloader\nx = x |&gt; gpu\ny = y |&gt; gpu\n(l, st), back = pullback(p -&gt; loss(data_idx, x, y, model, p, st), ps)\ngs = back((one(l), nothing))[1]\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\nend\nttime = time() - stime\n\ntrain_acc = round(accuracy(model, ps, st, train_dataloader, data_idx) * 100;\ndigits=2)\ntest_acc = round(accuracy(model, ps, st, test_dataloader, data_idx) * 100;\ndigits=2)\n\ndata_name = data_idx == 1 ? \"MNIST\" : \"FashionMNIST\"\n\nprintln(\"[$epoch/$nepochs] \\t $data_name Time $(round(ttime; digits=2))s \\t \" *\n\"Training Accuracy: $(train_acc)% \\t Test Accuracy: $(test_acc)%\")\nend\nend\n\nfor data_idx in 1:2\ntrain_dataloader, test_dataloader = dataloaders[data_idx]\ntrain_acc = round(accuracy(model, ps, st, train_dataloader, data_idx) * 100;\ndigits=2)\ntest_acc = round(accuracy(model, ps, st, test_dataloader, data_idx) * 100; digits=2)\n\ndata_name = data_idx == 1 ? \"MNIST\" : \"FashionMNIST\"\n\nprintln(\"[FINAL] \\t $data_name Training Accuracy: $(train_acc)% \\t \" *\n\"Test Accuracy: $(test_acc)%\")\nend\nend\n\ntrain()\n</code></pre> <pre><code>[1/9]    MNIST Time 3.88s    Training Accuracy: 72.66%   Test Accuracy: 78.12%\n[1/9]    FashionMNIST Time 1.04s     Training Accuracy: 58.01%   Test Accuracy: 59.38%\n[2/9]    MNIST Time 0.9s     Training Accuracy: 65.53%   Test Accuracy: 62.5%\n[2/9]    FashionMNIST Time 1.84s     Training Accuracy: 63.18%   Test Accuracy: 53.12%\n[3/9]    MNIST Time 1.59s    Training Accuracy: 75.68%   Test Accuracy: 78.12%\n[3/9]    FashionMNIST Time 0.9s      Training Accuracy: 60.45%   Test Accuracy: 53.12%\n[4/9]    MNIST Time 0.9s     Training Accuracy: 86.82%   Test Accuracy: 81.25%\n[4/9]    FashionMNIST Time 0.9s      Training Accuracy: 65.33%   Test Accuracy: 62.5%\n[5/9]    MNIST Time 0.9s     Training Accuracy: 87.6%    Test Accuracy: 84.38%\n[5/9]    FashionMNIST Time 0.9s      Training Accuracy: 66.31%   Test Accuracy: 62.5%\n[6/9]    MNIST Time 0.9s     Training Accuracy: 90.14%   Test Accuracy: 87.5%\n[6/9]    FashionMNIST Time 0.9s      Training Accuracy: 72.75%   Test Accuracy: 65.62%\n[7/9]    MNIST Time 0.9s     Training Accuracy: 92.58%   Test Accuracy: 87.5%\n[7/9]    FashionMNIST Time 0.9s      Training Accuracy: 73.44%   Test Accuracy: 56.25%\n[8/9]    MNIST Time 0.9s     Training Accuracy: 92.77%   Test Accuracy: 87.5%\n[8/9]    FashionMNIST Time 0.9s      Training Accuracy: 77.15%   Test Accuracy: 68.75%\n[9/9]    MNIST Time 0.9s     Training Accuracy: 95.31%   Test Accuracy: 87.5%\n[9/9]    FashionMNIST Time 0.9s      Training Accuracy: 79.98%   Test Accuracy: 65.62%\n[FINAL]      MNIST Training Accuracy: 90.43%     Test Accuracy: 87.5%\n[FINAL]      FashionMNIST Training Accuracy: 79.98%      Test Accuracy: 65.62%\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/intermediate/NeuralODE/main/","title":"MNIST Classification using NeuralODE","text":""},{"location":"examples/generated/intermediate/NeuralODE/main/#mnist-classification-using-neural-odes","title":"MNIST Classification using Neural ODEs","text":"<p>To understand Neural ODEs, users should look up these lecture notes. We recommend users to directly use DiffEqFlux.jl, instead of implementing Neural ODEs from scratch.</p> <p></p> <p></p>"},{"location":"examples/generated/intermediate/NeuralODE/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing ComponentArrays, CUDA, SciMLSensitivity, NNlib, Optimisers, OrdinaryDiffEq, Random,\nStatistics, Zygote, OneHotArrays\nimport MLDatasets: MNIST\nimport MLUtils: DataLoader, splitobs\nCUDA.allowscalar(false)\n</code></pre> <pre><code>  Activating project at `~/work/Lux.jl/Lux.jl/examples`\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#loading-mnist","title":"Loading MNIST","text":"<pre><code>function loadmnist(batchsize, train_split)\n# Load MNIST: Only 1500 for demonstration purposes\nN = 1500\ndataset = MNIST(; split=:train)\nimgs = dataset.features[:, :, 1:N]\nlabels_raw = dataset.targets[1:N]\n\n# Process images into (H,W,C,BS) batches\nx_data = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3)))\ny_data = onehotbatch(labels_raw, 0:9)\n(x_train, y_train), (x_test, y_test) = splitobs((x_data, y_data); at=train_split)\n\nreturn (\n# Use DataLoader to automatically minibatch and shuffle the data\nDataLoader(collect.((x_train, y_train)); batchsize=batchsize, shuffle=true),\n# Don't shuffle the test data\nDataLoader(collect.((x_test, y_test)); batchsize=batchsize, shuffle=false))\nend\n</code></pre> <pre><code>loadmnist (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#define-the-neural-ode-layer","title":"Define the Neural ODE Layer","text":"<p>The NeuralODE is a ContainerLayer, which stores a <code>model</code>. The parameters and states of the NeuralODE are same as those of the underlying model.</p> <pre><code>struct NeuralODE{M &lt;: Lux.AbstractExplicitLayer, So, Se, T, K} &lt;:\nLux.AbstractExplicitContainerLayer{(:model,)}\nmodel::M\nsolver::So\nsensealg::Se\ntspan::T\nkwargs::K\nend\n\nfunction NeuralODE(model::Lux.AbstractExplicitLayer; solver=Tsit5(),\nsensealg=InterpolatingAdjoint(; autojacvec=ZygoteVJP()),\ntspan=(0.0f0, 1.0f0), kwargs...)\nreturn NeuralODE(model, solver, sensealg, tspan, kwargs)\nend\n\nfunction (n::NeuralODE)(x, ps, st)\nfunction dudt(u, p, t)\nu_, st = n.model(u, p, st)\nreturn u_\nend\nprob = ODEProblem{false}(ODEFunction{false}(dudt), x, n.tspan, ps)\nreturn solve(prob, n.solver; sensealg=n.sensealg, n.kwargs...), st\nend\n\nfunction diffeqsol_to_array(x::ODESolution{T, N, &lt;:AbstractVector{&lt;:CuArray}}) where {T, N}\nreturn dropdims(gpu(x); dims=3)\nend\ndiffeqsol_to_array(x::ODESolution) = dropdims(Array(x); dims=3)\n</code></pre> <pre><code>diffeqsol_to_array (generic function with 2 methods)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/intermediate/NeuralODE/main/#create-and-initialize-the-neural-ode-layer","title":"Create and Initialize the Neural ODE Layer","text":"<pre><code>function create_model()\n# Construct the Neural ODE Model\nmodel = Chain(FlattenLayer(), Dense(784, 20, tanh),\nNeuralODE(Chain(Dense(20, 10, tanh), Dense(10, 10, tanh),\nDense(10, 20, tanh)); save_everystep=false, reltol=1.0f-3,\nabstol=1.0f-3, save_start=false), diffeqsol_to_array,\nDense(20, 10))\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps, st = Lux.setup(rng, model)\nps = ComponentArray(ps) |&gt; gpu\nst = st |&gt; gpu\n\nreturn model, ps, st\nend\n</code></pre> <pre><code>create_model (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#define-utility-functions","title":"Define Utility Functions","text":"<pre><code>logitcrossentropy(y_pred, y) = mean(-sum(y .* logsoftmax(y_pred); dims=1))\n\nfunction loss(x, y, model, ps, st)\ny_pred, st = model(x, ps, st)\nreturn logitcrossentropy(y_pred, y), st\nend\n\nfunction accuracy(model, ps, st, dataloader)\ntotal_correct, total = 0, 0\nst = Lux.testmode(st)\niterator = CUDA.functional() ? CuIterator(dataloader) : dataloader\nfor (x, y) in iterator\ntarget_class = onecold(cpu(y))\npredicted_class = onecold(cpu(model(x, ps, st)[1]))\ntotal_correct += sum(target_class .== predicted_class)\ntotal += length(target_class)\nend\nreturn total_correct / total\nend\n</code></pre> <pre><code>accuracy (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#training","title":"Training","text":"<pre><code>function train()\nmodel, ps, st = create_model()\n\n# Training\ntrain_dataloader, test_dataloader = loadmnist(128, 0.9)\n\nopt = Optimisers.ADAM(0.001f0)\nst_opt = Optimisers.setup(opt, ps)\n\n### Warmup the Model\nimg, lab = gpu(train_dataloader.data[1][:, :, :, 1:1]),\ngpu(train_dataloader.data[2][:, 1:1])\nloss(img, lab, model, ps, st)\n(l, _), back = pullback(p -&gt; loss(img, lab, model, p, st), ps)\nback((one(l), nothing))\n\n### Lets train the model\nnepochs = 9\nfor epoch in 1:nepochs\nstime = time()\niterator = CUDA.functional() ? CuIterator(train_dataloader) : train_dataloader\nfor (x, y) in iterator\n(l, st), back = pullback(p -&gt; loss(x, y, model, p, st), ps)\n### We need to add `nothing`s equal to the number of returned values - 1\ngs = back((one(l), nothing))[1]\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\nend\nttime = time() - stime\n\nprintln(\"[$epoch/$nepochs] \\t Time $(round(ttime; digits=2))s \\t Training Accuracy: \" *\n\"$(round(accuracy(model, ps, st, train_dataloader) * 100; digits=2))% \\t \" *\n\"Test Accuracy: $(round(accuracy(model, ps, st, test_dataloader) * 100; digits=2))%\")\nend\nend\n\ntrain()\n</code></pre> <pre><code>[1/9]    Time 1.77s      Training Accuracy: 50.96%   Test Accuracy: 43.33%\n[2/9]    Time 0.13s      Training Accuracy: 69.63%   Test Accuracy: 66.0%\n[3/9]    Time 0.15s      Training Accuracy: 77.93%   Test Accuracy: 71.33%\n[4/9]    Time 0.25s      Training Accuracy: 80.74%   Test Accuracy: 76.67%\n[5/9]    Time 0.11s      Training Accuracy: 82.52%   Test Accuracy: 78.0%\n[6/9]    Time 0.11s      Training Accuracy: 84.07%   Test Accuracy: 78.67%\n[7/9]    Time 0.22s      Training Accuracy: 85.33%   Test Accuracy: 80.67%\n[8/9]    Time 0.11s      Training Accuracy: 86.59%   Test Accuracy: 81.33%\n[9/9]    Time 0.11s      Training Accuracy: 87.7%    Test Accuracy: 82.0%\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"introduction/ecosystem/","title":"Ecosystem","text":""},{"location":"introduction/ecosystem/#ecosystem","title":"Ecosystem","text":""},{"location":"introduction/ecosystem/#frameworks-extending-lux","title":"Frameworks extending Lux","text":"<ul> <li>Boltz.jl \u2013 Prebuilt deep learning models for image classification tasks</li> <li>DeepEquilibriumNetworks.jl \u2013 Continuous and Discrete Deep Equilibrium Networks</li> <li>DiffEqFlux.jl \u2013 Neural Differential Equations, Continuous Normalizing Flows, etc.</li> </ul>"},{"location":"introduction/ecosystem/#extended-julia-ecosystem","title":"Extended Julia Ecosystem","text":"<p>As you might have noticed we don't do much apart from Neural Networks. All other parts of the DL training/evaluation pipeline should be offloaded to:</p> <p></p> <p></p>"},{"location":"introduction/ecosystem/#automatic-differentiation","title":"Automatic Differentiation","text":"<ul> <li>Zygote.jl \u2013 Currently the default and recommended AD library</li> <li>Enzyme.jl \u2013 Experimental Support (but will most likely become the future default)</li> <li>ForwardDiff.jl \u2013 For forward mode AD support</li> <li>ReverseDiff.jl \u2013 Tape based reverse mode AD (mostly untested)</li> </ul>"},{"location":"introduction/ecosystem/#data-manipulation-and-loading","title":"Data Manipulation and Loading","text":"<ul> <li>Augmentor.jl</li> <li>DataLoaders.jl</li> <li>Images.jl</li> <li>DataAugmentation.jl</li> </ul>"},{"location":"introduction/ecosystem/#distributed-dataparallel-training","title":"Distributed DataParallel Training","text":"<ul> <li>FluxMPI.jl</li> </ul>"},{"location":"introduction/ecosystem/#neural-network-primitives","title":"Neural Network Primitives","text":"<ul> <li>NNlib.jl</li> </ul>"},{"location":"introduction/ecosystem/#optimisation","title":"Optimisation","text":"<ul> <li>Optimisers.jl</li> <li>ParameterSchedulers.jl</li> <li>Optimization.jl</li> </ul>"},{"location":"introduction/ecosystem/#parameter-manipulation","title":"Parameter Manipulation","text":"<ul> <li>Functors.jl</li> </ul>"},{"location":"introduction/ecosystem/#serialization","title":"Serialization","text":"<ul> <li>Serialization.jl</li> <li>JLD2.jl</li> </ul>"},{"location":"introduction/ecosystem/#testing-utilities","title":"Testing Utilities","text":"<ul> <li>FiniteDifferences.jl \u2013 Finite Differencing. Useful for testing gradient correctness</li> <li>JET.jl</li> </ul>"},{"location":"introduction/ecosystem/#training-visualization-logging","title":"Training Visualization &amp; Logging","text":"<ul> <li>Wandb.jl</li> <li>TensorBoardLogger.jl</li> </ul>"},{"location":"introduction/overview/","title":"All about Lux","text":""},{"location":"introduction/overview/#why-we-wrote-lux","title":"Why we wrote Lux?","text":"<p>Julia already has quite a few well established Neural Network Frameworks \u2013 Flux &amp; KNet. However, certain design elements \u2013 Coupled Model and Parameters &amp; Internal Mutations \u2013 associated with these frameworks make them less compiler and user friendly. Making changes to address these problems in the respective frameworks would be too disruptive for users. Here comes in <code>Lux</code>: a neural network framework built completely using pure functions to make it both compiler and autodiff friendly.</p> <p></p> <p></p>"},{"location":"introduction/overview/#design-principles","title":"Design Principles","text":"<ul> <li>Layers must be immutable \u2013 cannot store any parameter/state but rather store the information to construct them</li> <li>Layers are pure functions</li> <li>Layers return a Tuple containing the result and the updated state</li> <li>Given same inputs the outputs must be same \u2013 yes this must hold true even for stochastic functions. Randomness must be controlled using <code>rng</code>s passed in the state.</li> <li>Easily extensible</li> </ul>"},{"location":"introduction/overview/#why-use-lux-over-flux","title":"Why use Lux over Flux?","text":"<ul> <li>Neural Networks for SciML: For SciML Applications (Neural ODEs, Deep Equilibrium Models) solvers typically expect a monolithic parameter vector. Flux enables this via its <code>destructure</code> mechanism, but <code>destructure</code> comes with various edge cases and limitations. Lux forces users to make an explicit distinction between state variables and parameter variables to avoid these issues. Also, it comes battery-included for distributed training using FluxMPI.jl (I know :P the naming)</li> <li>Sensible display of Custom Layers \u2013 Ever wanted to see Pytorch like Network printouts or wondered how to extend the pretty printing of Flux's layers? Lux handles all of that by default.</li> <li>Truly immutable models - No unexpected internal mutations since all layers are implemented as pure functions. All layers are also deterministic given the parameters and state: if a layer is supposed to be stochastic (say <code>Dropout</code>), the state must contain a seed which is then updated after the function call.</li> <li>Easy Parameter Manipulation \u2013 By separating parameter data and layer structures, Lux makes implementing <code>WeightNorm</code>, <code>SpectralNorm</code>, etc. downright trivial. Without this separation, it is much harder to pass such parameters around without mutations which AD systems don't like.</li> </ul>"},{"location":"introduction/overview/#why-not-use-lux","title":"Why not use Lux?","text":"<ul> <li>Small Neural Networks on CPU \u2013 Lux is developed for training large neural networks. For smaller architectures, we recommend using SimpleChains.jl.</li> <li>Lux won't magically speed up your code (yet) \u2013 Lux shares the same backend with Flux and so if your primary desire to shift is driven by performance, you will be disappointed.</li> <li>Special Architecture Support \u2013 Unfortunately, we currently don't support Cloud TPUs and even AMD GPUs are not well tested. (We do plan to support these in the nearish future)</li> </ul>"},{"location":"lib/Boltz/","title":"Boltz","text":""},{"location":"lib/Boltz/#boltz","title":"Boltz \u26a1","text":"<p>Accelerate \u26a1 your ML research using pre-built Deep Learning Models with Lux</p> <p></p> <p></p>"},{"location":"lib/Boltz/#installation","title":"Installation","text":"<pre><code>using Pkg\nPkg.add(\"Boltz\")\n</code></pre>"},{"location":"lib/Boltz/#getting-started","title":"Getting Started","text":"<pre><code>using Boltz, Lux\n\nmodel, ps, st = resnet(:resnet18; pretrained=true)\n</code></pre>"},{"location":"lib/Boltz/#classification-models","title":"Classification Models","text":"MODEL NAME FUNCTION NAME PRETRAINED TOP 1 ACCURACY (%) TOP 5 ACCURACY (%) AlexNet <code>alexnet</code> <code>:alexnet</code> \u2705 54.48 77.72 ResNet <code>resnet</code> <code>:resnet18</code> \u2705 68.08 88.44 ResNet <code>resnet</code> <code>:resnet34</code> \u2705 72.13 90.91 ResNet <code>resnet</code> <code>:resnet50</code> \u2705 74.55 92.36 ResNet <code>resnet</code> <code>:resnet101</code> \u2705 74.81 92.36 ResNet <code>resnet</code> <code>:resnet152</code> \u2705 77.63 93.84 VGG <code>vgg</code> <code>:vgg11</code> \u2705 67.35 87.91 VGG <code>vgg</code> <code>:vgg13</code> \u2705 68.40 88.48 VGG <code>vgg</code> <code>:vgg16</code> \u2705 70.24 89.80 VGG <code>vgg</code> <code>:vgg19</code> \u2705 71.09 90.27 VGG <code>vgg</code> <code>:vgg11_bn</code> \u2705 69.09 88.94 VGG <code>vgg</code> <code>:vgg13_bn</code> \u2705 69.66 89.49 VGG <code>vgg</code> <code>:vgg16_bn</code> \u2705 72.11 91.02 VGG <code>vgg</code> <code>:vgg19_bn</code> \u2705 72.95 91.32 ConvMixer <code>convmixer</code> <code>:small</code> \ud83d\udeab ConvMixer <code>convmixer</code> <code>:base</code> \ud83d\udeab ConvMixer <code>convmixer</code> <code>:large</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet121</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet161</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet169</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet201</code> \ud83d\udeab GoogleNet <code>googlenet</code> <code>:googlenet</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v1</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v2</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v3_small</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v3_large</code> \ud83d\udeab ResNeXT <code>resnext</code> <code>:resnext50</code> \ud83d\udeab ResNeXT <code>resnext</code> <code>:resnext101</code> \ud83d\udeab ResNeXT <code>resnext</code> <code>:resnext152</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:tiny</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:small</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:base</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:large</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:huge</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:giant</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:gigantic</code> \ud83d\udeab <p>These models can be created using <code>&lt;FUNCTION&gt;(&lt;NAME&gt;; pretrained = &lt;PRETRAINED&gt;)</code></p> <p></p> <p></p>"},{"location":"lib/Boltz/#preprocessing","title":"Preprocessing","text":"<p>All the pretrained models require that the images be normalized with the parameters <code>mean = [0.485f0, 0.456f0, 0.406f0]</code> and <code>std = [0.229f0, 0.224f0, 0.225f0]</code>.</p>"},{"location":"lib/Flux2Lux/","title":"Introduction","text":""},{"location":"lib/Flux2Lux/#flux2lux","title":"Flux2Lux","text":"<p>Flux2Lux is a package that allows you to convert Flux.jl models to Lux.jl.</p> <p></p> <p></p>"},{"location":"lib/Flux2Lux/#difference-from-luxtransform","title":"Difference from <code>Lux.transform</code>","text":"<p><code>Lux.transform</code> has been deprecated in favor of <code>Flux2Lux.jl</code>. This package is a strict superset of its predecessor. It provides additional features like <code>preserve_ps_st</code> and <code>force_transform</code>. See the documentation of <code>Flux2Lux.transform</code> for more details.</p>"},{"location":"lib/Flux2Lux/api/","title":"API Reference","text":""},{"location":"lib/Flux2Lux/api/#functions","title":"Functions","text":"<p># <code>Flux2Lux.transform</code> \u2014 Function.</p> <pre><code>transform(l; preserve_ps_st::Bool=false, force_preserve::Bool=false)\n</code></pre> <p>Convert a Flux Model to Lux Model.</p> <p>Warning</p> <p><code>transform</code> always ingores the <code>active</code> field of some of the Flux layers. This is almost never going to be supported on Flux2Lux.</p> <p>Arguments</p> <ul> <li><code>l</code>: Flux l or any generic Julia function / object.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>preserve_ps_st</code>: Set to <code>true</code> to preserve the states and parameters of the l. This attempts the best possible way to preserve the original model. But it might fail. If you need to override possible failures, set <code>force_preserve</code> to <code>true</code>.</li> <li><code>force_preserve</code>: Some of the transformations with state and parameters preservation haven't been implemented yet, in these cases, if <code>force_transform</code> is <code>false</code> a warning will be printed and a core Lux layer will be returned. Else, it will create a <code>FluxLayer</code>.</li> </ul> <p>Examples</p> <pre><code>using Flux2Lux, Lux, Metalhead, Random\n\nm = ResNet(18)\nm2 = transform(m.layers)\n\nx = randn(Float32, 224, 224, 3, 1);\n\nps, st = Lux.setup(Random.default_rng(), m2);\n\nm2(x, ps, st)\n</code></pre> <p>source</p> <p></p> <p></p>"},{"location":"lib/Flux2Lux/api/#layers","title":"Layers","text":"<p># <code>Flux2Lux.FluxLayer</code> \u2014 Type.</p> <pre><code>FluxLayer(layer)\n</code></pre> <p>Serves as a compatibility layer between Flux and Lux. This uses <code>Optimisers.destructure</code> API internally.</p> <p>Warning</p> <p>Lux was written to overcome the limitations of <code>destructure</code> + <code>Flux</code>. It is recommended to rewrite your l in Lux instead of using this layer.</p> <p>Warning</p> <p>Introducing this Layer in your model will lead to type instabilities, given the way <code>Optimisers.destructure</code> works.</p> <p>Arguments</p> <ul> <li><code>layer</code>: Flux layer</li> </ul> <p>Parameters</p> <ul> <li><code>p</code>: Flattened parameters of the <code>layer</code></li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"lib/Flux2Lux/api/#index","title":"Index","text":"<ul> <li><code>Flux2Lux.FluxLayer</code></li> <li><code>Flux2Lux.transform</code></li> </ul>"},{"location":"lib/LuxCore/","title":"Introduction","text":""},{"location":"lib/LuxCore/#luxcore","title":"LuxCore","text":"<p><code>LuxCore.jl</code> defines the abstract layers for Lux. Allows users to be compatible with the entirely of <code>Lux.jl</code> without having such a heavy dependency. If you are depending on <code>Lux.jl</code> directly, you do not need to depend on <code>LuxCore.jl</code> (all the functionality is exported via <code>Lux.jl</code>).</p>"},{"location":"lib/LuxCore/api/","title":"API Reference","text":""},{"location":"lib/LuxCore/api/#abstract-types","title":"Abstract Types","text":"<p># <code>LuxCore.AbstractExplicitLayer</code> \u2014 Type.</p> <pre><code>AbstractExplicitLayer\n</code></pre> <p>Abstract Type for all Lux Layers</p> <p>Users implementing their custom layer, must implement</p> <ul> <li><code>initialparameters(rng::AbstractRNG, layer::CustomAbstractExplicitLayer)</code> \u2013 This returns a <code>NamedTuple</code> containing the trainable parameters for the layer.</li> <li><code>initialstates(rng::AbstractRNG, layer::CustomAbstractExplicitLayer)</code> \u2013 This returns a NamedTuple containing the current state for the layer. For most layers this is typically empty. Layers that would potentially contain this include <code>BatchNorm</code>, <code>LSTM</code>, <code>GRU</code> etc.</li> </ul> <p>Optionally:</p> <ul> <li><code>parameterlength(layer::CustomAbstractExplicitLayer)</code> \u2013 These can be automatically calculated, but it is recommended that the user defines these.</li> <li><code>statelength(layer::CustomAbstractExplicitLayer)</code> \u2013 These can be automatically calculated, but it is recommended that the user defines these.</li> </ul> <p>See also <code>AbstractExplicitContainerLayer</code></p> <p>source</p> <p># <code>LuxCore.AbstractExplicitContainerLayer</code> \u2014 Type.</p> <pre><code>AbstractExplicitContainerLayer{layers} &lt;: AbstractExplicitLayer\n</code></pre> <p>Abstract Container Type for certain Lux Layers. <code>layers</code> is a tuple containing fieldnames for the layer, and constructs the parameters and states using those.</p> <p>Users implementing their custom layer can extend the same functions as in <code>AbstractExplicitLayer</code>.</p> <p>Tip</p> <p>Advanced structure manipulation of these layers post construction is possible via <code>Functors.fmap</code>. For a more flexible interface, we recommend using the experimental feature <code>Lux.@layer_map</code>.</p> <p>source</p> <p></p> <p></p>"},{"location":"lib/LuxCore/api/#general","title":"General","text":"<p># <code>LuxCore.apply</code> \u2014 Function.</p> <pre><code>apply(model::AbstractExplicitLayer, x, ps, st::NamedTuple)\n</code></pre> <p>Simply calls <code>model(x, ps, st)</code></p> <p>source</p> <p># <code>LuxCore.setup</code> \u2014 Function.</p> <pre><code>setup(rng::AbstractRNG, l::AbstractExplicitLayer)\n</code></pre> <p>Shorthand for getting the parameters and states of the layer <code>l</code>. Is equivalent to <code>(initialparameters(rng, l), initialstates(rng, l))</code>.</p> <p>Warning</p> <p>This function is not pure, it mutates <code>rng</code>.</p> <p>source</p> <p></p> <p></p>"},{"location":"lib/LuxCore/api/#parameters","title":"Parameters","text":"<p># <code>LuxCore.initialparameters</code> \u2014 Function.</p> <pre><code>initialparameters(rng::AbstractRNG, l)\n</code></pre> <p>Generate the initial parameters of the layer <code>l</code>.</p> <p>source</p> <p># <code>LuxCore.parameterlength</code> \u2014 Function.</p> <pre><code>parameterlength(l)\n</code></pre> <p>Return the total number of parameters of the layer <code>l</code>.</p> <p>source</p> <p></p> <p></p>"},{"location":"lib/LuxCore/api/#states","title":"States","text":"<p># <code>LuxCore.initialstates</code> \u2014 Function.</p> <pre><code>initialstates(rng::AbstractRNG, l)\n</code></pre> <p>Generate the initial states of the layer <code>l</code>.</p> <p>source</p> <p># <code>LuxCore.statelength</code> \u2014 Function.</p> <pre><code>statelength(l)\n</code></pre> <p>Return the total number of states of the layer <code>l</code>.</p> <p>source</p> <p># <code>LuxCore.testmode</code> \u2014 Function.</p> <pre><code>testmode(st::NamedTuple)\n</code></pre> <p>Make all occurances of <code>training</code> in state <code>st</code> \u2013 <code>Val(false)</code>.</p> <p>source</p> <p># <code>LuxCore.trainmode</code> \u2014 Function.</p> <pre><code>trainmode(st::NamedTuple)\n</code></pre> <p>Make all occurances of <code>training</code> in state <code>st</code> \u2013 <code>Val(true)</code>.</p> <p>source</p> <p># <code>LuxCore.update_state</code> \u2014 Function.</p> <pre><code>update_state(st::NamedTuple, key::Symbol, value; layer_check=_default_layer_check(key))\n</code></pre> <p>Recursively update all occurances of the <code>key</code> in the state <code>st</code> with the <code>value</code>.</p> <p>source</p> <p></p> <p></p>"},{"location":"lib/LuxCore/api/#index","title":"Index","text":"<ul> <li><code>LuxCore.AbstractExplicitContainerLayer</code></li> <li><code>LuxCore.AbstractExplicitLayer</code></li> <li><code>LuxCore.apply</code></li> <li><code>LuxCore.initialparameters</code></li> <li><code>LuxCore.initialstates</code></li> <li><code>LuxCore.parameterlength</code></li> <li><code>LuxCore.setup</code></li> <li><code>LuxCore.statelength</code></li> <li><code>LuxCore.testmode</code></li> <li><code>LuxCore.trainmode</code></li> <li><code>LuxCore.update_state</code></li> </ul>"},{"location":"lib/LuxLib/","title":"Introduction","text":""},{"location":"lib/LuxLib/#luxlib","title":"LuxLib","text":"<p>Backend for Lux.jl.</p> <p></p> <p></p>"},{"location":"lib/LuxLib/#tutorials","title":"Tutorials","text":"<p>This is a developer-facing project and most users should not depend on it directly. As such, we don't have tutorials for this package. Instead, we recommend you check out the Lux tutorials.</p> <p></p> <p></p>"},{"location":"lib/LuxLib/#whats-the-distinction-from-nnlibjl","title":"What's the distinction from NNlib.jl?","text":"<p>Think of this package as a temporary location for functionalities that will move into NNlib.jl. At the moment, this is supposed to be a heavier dependency than NNlib.jl, and it makes no attempt to separate code across different architectures.</p>"},{"location":"lib/LuxLib/api/","title":"API Reference","text":""},{"location":"lib/LuxLib/api/#dropout","title":"Dropout","text":"<p># <code>LuxLib.alpha_dropout</code> \u2014 Function.</p> <pre><code>alpha_dropout(rng::AbstractRNG, x, p, ::Val{training})\nalpha_dropout(rng::AbstractRNG, x, p, ::Val{training}, \u03b1, A, B)\n</code></pre> <p>Alpha Dropout: Dropout ensuring that the mean and variance of the output remains same as the input. For details see [1]. Use the second call signature to avoid recomputing the constants for a fixed dropout probability.</p> <p>Arguments</p> <ul> <li><code>rng</code>: Random number generator</li> <li><code>x</code>: Input Array</li> <li><code>p</code>: Probability of an element to be dropped out</li> <li><code>Val(training)</code>: If <code>true</code> then dropout is applied on <code>x</code> with probability <code>p</code>. Else, <code>x</code> is returned</li> <li><code>\u03b1</code>: -1.7580993408473766. Computed at limit x tends to infinity, <code>selu(x) = -\u03bb\u03b2 = \u03b1</code></li> <li><code>A</code>: Scaling factor for the mean</li> <li><code>B</code>: Scaling factor for the variance</li> </ul> <p>Returns</p> <ul> <li>Output Array after applying alpha dropout</li> <li>Updated state for the random number generator</li> </ul> <p>References</p> <p>[1] Klambauer, G\u00fcnter, et al. \"Self-normalizing neural networks.\" Advances in neural     information processing systems 30 (2017).</p> <p>source</p> <p># <code>LuxLib.dropout</code> \u2014 Function.</p> <pre><code>dropout(rng::AbstractRNG, x, p, ::Val{training}; dims, invp=inv(p))\ndropout(rng::AbstractRNG, x, mask, p, ::Val{training}, ::Val{update_mask}; dims,\ninvp=inv(p))\n</code></pre> <p>Dropout: Simple Way to prevent Neural Networks for Overfitting. For details see [1].</p> <p>Arguments</p> <ul> <li><code>rng</code>: Random number generator</li> <li><code>x</code>: Input Array</li> <li><code>mask</code>: Dropout Mask. If not used then it is constructed automatically</li> <li><code>p</code>: Probability of an element to be dropped out</li> <li><code>Val(training)</code>: If <code>true</code> then dropout is applied on <code>x</code> with probability <code>p</code> along <code>dims</code>. Else, <code>x</code> is returned</li> <li><code>Val(update_mask)</code>: If <code>true</code> then the mask is generated and used. Else, the <code>mask</code> provided is directly used</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>dims</code>: Dimensions along which dropout is applied</li> <li><code>invp</code>: Inverse of the probability (\\(\\frac{1}{p}\\))</li> </ul> <p>Returns</p> <ul> <li>Output Array after applying dropout</li> <li>Dropout Mask (if <code>training == false</code>, the returned value is meaningless)</li> <li>Updated state for the random number generator</li> </ul> <p>References</p> <p>[1] Srivastava, Nitish, et al. \"Dropout: a simple way to prevent neural networks from     overfitting.\" The journal of machine learning research 15.1 (2014): 1929-1958.</p> <p>source</p> <p></p> <p></p>"},{"location":"lib/LuxLib/api/#normalization","title":"Normalization","text":"<p># <code>LuxLib.batchnorm</code> \u2014 Function.</p> <pre><code>batchnorm(x, scale, bias, running_mean, running_var; momentum, epsilon, training)\n</code></pre> <p>Batch Normalization. For details see [1].</p> <p>Batch Normalization computes the mean and variance for each \\(D_1 \\times ... \\times D_{N - 2} \\times 1 \\times D_N\\) input slice and normalises the input accordingly.</p> <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> <li><code>running_mean</code>: Running mean (can be <code>nothing</code>)</li> <li><code>running_var</code>: Running variance (can be <code>nothing</code>)</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>momentum</code>: Momentum for updating running mean and variance</li> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> <li><code>training</code>: Set to <code>Val(true)</code> if running in training mode</li> </ul> <p>Returns</p> <p>Normalized Array of same size as <code>x</code>. And a Named Tuple containing the updated running mean and variance.</p> <p>Performance Considerations</p> <p>If the input array is <code>2D</code>, <code>4D</code>, or <code>5D</code> <code>CuArray</code> with element types <code>Float16</code>, <code>Float32</code> and <code>Float64</code>, then the CUDNN code path will be used. In all other cases, a broadcasting fallback is used which is not highly optimized.</p> <p>References</p> <p>[1] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network     training by reducing internal covariate shift.\" International conference on machine     learning. PMLR, 2015.</p> <p>source</p> <p># <code>LuxLib.groupnorm</code> \u2014 Function.</p> <pre><code>groupnorm(x, scale, bias; groups, epsilon)\ngroupnorm(x, scale, bias, running_mean, running_var; groups, momentum, training,\nepsilon)\n</code></pre> <p>Group Normalization. For details see [1].</p> <p>This op is similar to batch normalization, but statistics are shared across equally-sized groups of channels and not shared across batch dimension. Thus, group normalization does not depend on the batch composition and does not require maintaining internal state for storing statistics.</p> <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> <li><code>running_mean</code>: Running mean of the inputs. Must be an <code>AbstractVector</code> or <code>nothing</code>.</li> <li><code>running_var</code>: Running variance of the inputs. Must be an <code>AbstractVector</code> or <code>nothing</code>.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>groups</code>: Number of groups</li> <li><code>momentum</code>: Momentum for updating running mean and variance.</li> <li><code>training</code>: Set to <code>Val(true)</code> if running in training mode.</li> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> </ul> <p>Returns</p> <p>If using the first function signature, then the only the normalized array is returned.</p> <p>Otherwise, the normalized array and a named tuple containing updated running mean and updated running variance are returned.</p> <p>Additional Notes</p> <p><code>running_mean</code>, <code>running_var</code>, <code>momentum</code>, and <code>training</code> exist only for backwards compatibility reasons. There is no well documented evidence in literature that tracking statistics for group normalization actually helps. It is recommended to not use these arguments at all.</p> <p>Performance Considerations</p> <p>The most common case of this Op \u2013 <code>x</code> is a 4D array and there is no statistics tracking \u2013 is optimized using KernelAbstractions and has a fast custom backwards pass implemented. All other cases have a fallback implementation which is not especially optimized.</p> <p>Additionally, if the element types of <code>x</code>, <code>scale</code>, and <code>bias</code> are not same and not one of <code>Float32</code> and <code>Float64</code>, then the Op uses the slower fallback implementation. We have tested the code path for <code>Float16</code> and it works, but gradient accumulation is extremely fragile. Hence, for <code>Float16</code> inputs, it uses the fallback implementation.</p> <p>If the batch size is small (&lt; 16), then the fallback implementation will be faster than the KA version. However, this customization is not possible using the direct <code>groupnorm</code> interface.</p> <p>References</p> <p>[1] Wu, Yuxin, and Kaiming He. \"Group normalization.\" Proceedings of the European conference     on computer vision (ECCV). 2018.</p> <p>source</p> <p># <code>LuxLib.instancenorm</code> \u2014 Function.</p> <pre><code>instancenorm(x, scale, bias; epsilon, training)\n</code></pre> <p>Instance Normalization. For details see [1].</p> <p>Instance Normalization computes the mean and variance for each \\(D_1 \\times ... \\times D_{N - 2} \\times 1 \\times 1\\)` input slice and normalises the input accordingly.</p> <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized (must be atleast 3D)</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> <li><code>training</code>: Set to <code>Val(true)</code> if running in training mode</li> </ul> <p>Returns</p> <p>Normalized Array of same size as <code>x</code>. And a Named Tuple containing the updated running mean and variance.</p> <p>References</p> <p>[1] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Instance normalization: The     missing ingredient for fast stylization.\" arXiv preprint arXiv:1607.08022 (2016).</p> <p>source</p> <p># <code>LuxLib.layernorm</code> \u2014 Function.</p> <pre><code>layernorm(x, scale, bias; dims, epsilon)\n</code></pre> <p>Layer Normalization. For details see [1].</p> <p>Given an input array \\(x\\), this layer computes</p> \\[ y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta \\] <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>dims</code>: Dimensions along which the mean and std of <code>x</code> is computed</li> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> </ul> <p>Returns</p> <p>Normalized Array of same size as <code>x</code>.</p> <p>References</p> <p>[1] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer normalization.\" arXiv     preprint arXiv:1607.06450 (2016).</p> <p>source</p> <p></p> <p></p>"},{"location":"lib/LuxLib/api/#index","title":"Index","text":"<ul> <li><code>LuxLib.alpha_dropout</code></li> <li><code>LuxLib.batchnorm</code></li> <li><code>LuxLib.dropout</code></li> <li><code>LuxLib.groupnorm</code></li> <li><code>LuxLib.instancenorm</code></li> <li><code>LuxLib.layernorm</code></li> </ul>"},{"location":"manual/dispatch_custom_inputs/","title":"Dispatch on Custom Inputs","text":""},{"location":"manual/dispatch_custom_inputs/#dispatching-on-custom-input-types","title":"Dispatching on Custom Input Types","text":""},{"location":"manual/dispatch_custom_inputs/#which-function-should-participate-in-dispatch","title":"Which function should participate in dispatch?","text":"<ul> <li>Defining a dispatch on <code>(::Layer)(x::MyInputType, ps, st::NamedTuple)</code> is inconvenient, since it requires the user to define a new method for every layer type.</li> <li><code>(::AbstractExplicitLayer)(x::MyInputType, ps, st::NamedTuple)</code> doesn't work.</li> <li>Instead, we need to define the dispatch on <code>Lux.apply(::AbstractExplicitLayer, x::MyInputType, ps, st::NamedTuple)</code>.</li> </ul>"},{"location":"manual/dispatch_custom_inputs/#concrete-example","title":"Concrete Example","text":"<p>Consider Neural ODEs. In these models, often time we want to every iteration of the neural network to take the current time as input. Here, we won't go through implementing an entire Neural ODE model. Instead we will define a time dependent version of <code>Chain</code>.</p> <p></p> <p></p>"},{"location":"manual/dispatch_custom_inputs/#time-dependent-chain-implementation","title":"Time-Dependent Chain Implementation","text":"<pre><code>using Lux, Random\n\nstruct TDChain{L &lt;: NamedTuple} &lt;: Lux.AbstractExplicitContainerLayer{(:layers,)}\nlayers::L\nend\n\nfunction (l::TDChain)((x, t)::Tuple, ps, st::NamedTuple)\n# Concatenate along the 2nd last dimension\nsz = ntuple(i -&gt; i == ndims(x) - 1 ? 1 : size(x, i), ndims(x))\nt_ = ones(eltype(x), sz) .* t  # Needs to be modified for GPU\nfor name in keys(l.layers)\nx, st_ = Lux.apply(getfield(l.layers, name), cat(x, t_; dims=ndims(x) - 1),\ngetfield(ps, name), getfield(st, name))\nst = merge(st, NamedTuple{(name,)}((st_,)))\nend\nreturn x, st\nend\n\nmodel = Chain(Dense(3, 4), TDChain((; d1=Dense(5, 4), d2=Dense(5, 4))), Dense(4, 1))\n</code></pre> <pre><code>Chain(\n    layer_1 = Dense(3 =&gt; 4),            # 16 parameters\n    layer_2 = TDChain(\n        layers = NamedTuple(\n            d1 = Dense(5 =&gt; 4),         # 24 parameters\n            d2 = Dense(5 =&gt; 4),         # 24 parameters\n        ),\n    ),\n    layer_3 = Dense(4 =&gt; 1),            # 5 parameters\n)         # Total: 69 parameters,\n          #        plus 0 states, summarysize 64 bytes.\n</code></pre>"},{"location":"manual/dispatch_custom_inputs/#running-the-tdchain","title":"Running the TDChain","text":"<pre><code>rng = MersenneTwister(0)\nps, st = Lux.setup(rng, model)\nx = randn(rng, Float32, 3, 2)\n\n# model(x, ps, st)\n</code></pre> <pre><code>3\u00d72 Matrix{Float32}:\n  0.473714  1.42305\n  0.300234  0.408387\n -0.762677  0.588621\n</code></pre> <p>The last line is commented out, since it will not work. Try uncommenting it and see what happens.</p> <p></p>"},{"location":"manual/dispatch_custom_inputs/#dispatching-on-custom-input-types_1","title":"Dispatching on Custom Input Types","text":"<ul> <li>Create a Custom Layer storing the time.</li> </ul> <pre><code>struct ArrayAndTime{A &lt;: AbstractArray, T &lt;: Real}\narray::A\ntime::T\nend\n</code></pre> <ul> <li>Define the dispatch on <code>Lux.apply(::AbstractExplicitLayer, x::ArrayAndTime, ps, st::NamedTuple)</code>.</li> </ul> <pre><code>function Lux.apply(layer::Lux.AbstractExplicitLayer, x::ArrayAndTime, ps, st::NamedTuple)\ny, st = layer(x.array, ps, st)\nreturn ArrayAndTime(y, x.time), st\nend\n\nfunction Lux.apply(layer::TDChain, x::ArrayAndTime, ps, st::NamedTuple)\ny, st = layer((x.array, x.time), ps, st)\nreturn ArrayAndTime(y, x.time), st\nend\n</code></pre> <ul> <li>Run the model.</li> </ul> <pre><code>xt = ArrayAndTime(x, 10.0f0)\n\nmodel(xt, ps, st)[1]\n</code></pre> <pre><code>Main.ArrayAndTime{Matrix{Float32}, Float32}(Float32[4.8016562 5.174927], 10.0f0)\n</code></pre>"},{"location":"manual/dispatch_custom_inputs/#using-the-same-input-for-non-td-models","title":"Using the same input for non-TD models","text":"<p>Writing proper dispatch means we can simple replace the <code>TDChain</code> with a <code>Chain</code> (of course with dimension corrections) and the pipeline still works.</p> <pre><code>model = Chain(Dense(3, 4), Chain((; d1=Dense(4, 4), d2=Dense(4, 4))), Dense(4, 1))\n\nps, st = Lux.setup(rng, model)\n\nmodel(xt, ps, st)[1]\n</code></pre> <pre><code>Main.ArrayAndTime{Matrix{Float32}, Float32}(Float32[-0.08124366 -1.1121564], 10.0f0)\n</code></pre>"},{"location":"manual/freezing_parameters/","title":"Freezing Model Parameters","text":""},{"location":"manual/freezing_parameters/#freezing-model-parameters","title":"Freezing Model Parameters","text":"<p>Warning</p> <p>API for freezing parameters should be considered experimental at this point.</p> <p>In this manual we will go over how to freeze certain parameters in a model.</p> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-layers-of-a-particular-kind","title":"Freezing layers of a particular kind","text":"<p>To freeze a particular kind of layer, let's say <code>Dense</code> in the following example. We can use <code>Lux.@layer_map</code> and freeze layers if they are of type <code>Dense</code>.</p> <pre><code>using Lux, Random\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nmodel = Chain(Dense(3, 4), Chain(Dense(4, 4), Dropout(0.5f0), BatchNorm(4)),\nDense(4, 1); disable_optimizations=true)\n\nps, st = Lux.setup(rng, model)\n\nx = randn(rng, Float32, 3, 2)\n\nmodel(x, ps, st)\n\nfunction freeze_dense(d::Lux.Dense, ps, st, name::String)\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nend\nfreeze_dense(l, ps, st, name) = (l, ps, st)\n\nmodel_frozen, ps_frozen, st_frozen = Lux.@layer_map freeze_dense model ps st\n\nmodel_frozen(x, ps_frozen, st_frozen)\n</code></pre> <pre><code>(Float32[1.7641534 -1.7641534], (layer_1 = (frozen_params = (weight = Float32[-0.026350189 -0.5554656 -0.35653266; -0.17461072 0.6705545 0.29924855; -0.8935247 -0.42453378 -0.3020351; -0.7988979 -0.7666331 -0.7104237], bias = Float32[0.0; 0.0; 0.0; 0.0;;]), states = NamedTuple()), layer_2 = (layer_1 = (frozen_params = (weight = Float32[-0.47289538 -0.680748 0.1764085 0.34383082; 0.42747158 -0.13819042 -0.109261915 -0.6143286; -0.35790488 -0.20881107 0.70390546 0.48137343; 0.82561636 0.38187847 0.05779423 -0.35181466], bias = Float32[0.0; 0.0; 0.0; 0.0;;]), states = NamedTuple()), layer_2 = (rng = Random.Xoshiro(0x87711e5ce1a49ffe, 0xa210b60ecab6b8c5, 0x436c749552fc8172, 0x03e9c7d813a9f096), training = Val{true}()), layer_3 = (running_mean = Float32[-0.04517859, 0.03484953, -0.004917746, 0.0074841487], running_var = Float32[0.94082206, 0.92428976, 0.90048367, 0.90112025], training = Val{true}())), layer_3 = (frozen_params = (weight = Float32[0.3981135 0.45468387 -0.07694905 0.8353388], bias = Float32[0.0;;]), states = NamedTuple())))\n</code></pre> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-by-layer-name","title":"Freezing by layer name","text":"<p>When the function in <code>layer_map</code> is called, the 4th argument is the name of the layer. For example, if you want to freeze the 1st layer inside the inner Chain. The name for this would be <code>&lt;model&gt;.layer_2.layer_1</code>.</p> Freezing by layer nameFreezing by layer type <pre><code>function freeze_by_name(d, ps, st, name::String)\nif name == \"model.layer_2.layer_1\"\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nelse\nreturn d, ps, st\nend\nend\n</code></pre> <pre><code>function freeze_dense(d::Dense, ps, st, name::String)\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nend\nfreeze_dense(l, ps, st, name) = (l, ps, st)\n</code></pre> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-part-of-the-parameters","title":"Freezing part of the parameters","text":"<p>Instead of freezing all the parameters, we can simple specify <code>(:weight,)</code> to freeze only the <code>weight</code> parameter while training the <code>bias</code> parameter.</p> Freezing some parameters of a layerFreezing all parameters of a layer <pre><code>function freeze_by_name(d, ps, st, name::String)\nif name == \"model.layer_2.layer_1\"\nreturn Lux.freeze(d, ps, st, (:weight,))\nelse\nreturn d, ps, st\nend\nend\n</code></pre> <pre><code>function freeze_by_name(d, ps, st, name::String)\nif name == \"model.layer_2.layer_1\"\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nelse\nreturn d, ps, st\nend\nend\n</code></pre> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-part-of-a-chain","title":"Freezing part of a Chain","text":"<p>Starting <code>v0.4.22</code>, we can directly index into a <code>Chain</code>. So freezing a part of a <code>Chain</code>, is extremely easy.</p> <pre><code>using Lux, Random\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nmodel = Chain(Dense(3, 4), Dense(4, 4), Dropout(0.5f0), BatchNorm(4), Dense(4, 1))\n\nmodel_frozen = Chain(model[1:2], Lux.freeze(model[3:4]), model[5])\nps, st = Lux.setup(rng, model_frozen)\n\nx = randn(rng, Float32, 3, 2)\n\nmodel_frozen(x, ps, st)\n</code></pre> <pre><code>(Float32[1.7641534 -1.7641534], (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (frozen_params = (layer_3 = NamedTuple(), layer_4 = (scale = Float32[1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0])), states = (layer_3 = (rng = Random.Xoshiro(0x87711e5ce1a49ffe, 0xa210b60ecab6b8c5, 0x436c749552fc8172, 0x03e9c7d813a9f096), training = Val{true}()), layer_4 = (running_mean = Float32[-0.04517859, 0.03484953, -0.004917746, 0.0074841487], running_var = Float32[0.94082206, 0.92428976, 0.90048367, 0.90112025], training = Val{true}()))), layer_4 = NamedTuple()))\n</code></pre>"},{"location":"manual/interface/","title":"Lux Interface","text":""},{"location":"manual/interface/#lux-interface","title":"Lux Interface","text":"<p>Tip</p> <p>If you just want to define compatibility with Lux without actually using any of the other functionality provided by Lux (like layers), it is recommended to depend on <code>LuxCore.jl</code> instead of <code>Lux.jl</code>. <code>LuxCore.jl</code> is a significantly lighter dependency.</p> <p>First let's set the expectations straight.</p> <ul> <li>Do you have to follow the interface? No.</li> <li>Should you follow it? Probably yes.</li> <li>Why? It provides the ability for frameworks built on top of Lux to be cross compatible. Additionally, any new functionality built into Lux, will just work for your framework.</li> </ul> <p>Warning</p> <p>The interface is optional for frameworks being developed independent of Lux. All functionality in the core library (and officially supported ones) must adhere to the interface</p> <p></p> <p></p>"},{"location":"manual/interface/#layer-interface","title":"Layer Interface","text":""},{"location":"manual/interface/#singular-layer","title":"Singular Layer","text":"<p>If the layer doesn't contain any other Lux layer, then it is a <code>Singular Layer</code>. This means it should optionally subtype <code>Lux.AbstractExplicitLayer</code> but mandatorily define all the necessary functions mentioned in the docstrings. Consider a simplified version of <code>Dense</code> called <code>Linear</code>.</p> <p>First, setup the architectural details for this layer. Note, that the architecture doesn't contain any mutable structure like arrays. When in doubt, remember, once constructed a model architecture cannot change.</p> <p>Tip</p> <p>For people coming from Flux.jl background this might be weird. We recommend checking out the Flux to Lux migration guide first before proceeding.</p> <pre><code>using Lux, Random\n\nstruct Linear{F1, F2} &lt;: Lux.AbstractExplicitLayer\nin_dims::Int\nout_dims::Int\ninit_weight::F1\ninit_bias::F2\nend\n\nfunction Linear(in_dims::Int, out_dims::Int; init_weight=Lux.glorot_uniform,\ninit_bias=Lux.zeros32)\nreturn Linear{typeof(init_weight), typeof(init_bias)}(in_dims, out_dims, init_weight,\ninit_bias)\nend\n\nl = Linear(2, 4)\n</code></pre> <pre><code>Linear()\n</code></pre> <p>Next, we need to implement functions which return the parameters and states for the layer. In case of <code>Linear</code>, the parameters are <code>weight</code> and <code>bias</code> while the states are empty. States become important when defining layers like <code>BatchNorm</code>, <code>WeightNorm</code>, etc. The recommended data structure for returning parameters is a NamedTuple, though anything satisfying the Parameter Interface is valid.</p> <pre><code>function Lux.initialparameters(rng::AbstractRNG, l::Linear)\nreturn (weight=l.init_weight(rng, l.out_dims, l.in_dims),\nbias=l.init_bias(rng, l.out_dims, 1))\nend\n\nLux.initialstates(::AbstractRNG, ::Linear) = NamedTuple()\n</code></pre> <p>You could also implement <code>Lux.parameterlength</code> and <code>Lux.statelength</code> to prevent wasteful reconstruction of the parameters and states.</p> <pre><code># This works\nprintln(\"Parameter Length: \", Lux.parameterlength(l), \"; State Length: \",\nLux.statelength(l))\n\n# But still recommened to define these\nLux.parameterlength(l::Linear) = l.out_dims * l.in_dims + l.out_dims\n\nLux.statelength(::Linear) = 0\n</code></pre> <pre><code>Parameter Length: 12; State Length: 0\n</code></pre> <p>Tip</p> <p>You might notice that we don't pass in a <code>PRNG</code> for these functions. If your parameter length and/or state length depend on a random number generator, you should think really hard about what you are trying to do and why.</p> <p>Now, we need to define how the layer works. For this you make your layer a function with exactly 3 arguments \u2013 <code>x</code> the input, <code>ps</code> the parameters, and <code>st</code> the states. This function must return two things \u2013 <code>y</code> the output, and <code>st_new</code> the updated state.</p> <pre><code>function (l::Linear)(x::AbstractMatrix, ps, st::NamedTuple)\ny = ps.weight * x .+ ps.bias\nreturn y, st\nend\n</code></pre> <p>Finally, let's run this layer. If you have made this far into the documentation, we don't feel you need a refresher on that.</p> <pre><code>rng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps, st = Lux.setup(rng, l)\n\nprintln(\"Parameter Length: \", Lux.parameterlength(l), \"; State Length: \",\nLux.statelength(l))\n\nx = randn(rng, Float32, 2, 1)\n\nLux.apply(l, x, ps, st) # or `l(x, ps, st)`\n</code></pre> <pre><code>(Float32[-0.15276335; 0.45325348; 1.0207279; 0.78226817;;], NamedTuple())\n</code></pre> <p></p> <p></p>"},{"location":"manual/interface/#container-layer","title":"Container Layer","text":"<p>If your layer comprises of other Lux layers, then it is a <code>Container Layer</code>. Note that you could treat it as a <code>Singular Layer</code>, and it is still fine. FWIW, if you cannot subtype your layer with <code>Lux.AbstractExplicitContainerLayer</code> then you should go down the <code>Singular Layer</code> route. But subtyping allows us to bypass some of these common definitions. Let us now define a layer, which is basically a composition of two linear layers.</p> <pre><code>struct ComposedLinear{L1, L2} &lt;: Lux.AbstractExplicitContainerLayer{(:linear_1, :linear_2)}\nlinear_1::L1\nlinear_2::L2\nend\n\nfunction (cl::ComposedLinear)(x::AbstractMatrix, ps, st::NamedTuple)\n# To access the parameters and states for `linear_1` we do `ps.linear_1` and\n# `st.linear_1`. Similarly for `linear_2`\ny, st_l1 = cl.linear_1(x, ps.linear_1, st.linear_1)\ny, st_l2 = cl.linear_2(y, ps.linear_2, st.linear_2)\n# Finally, we need to return the new state which has the exact structure as `st`\nreturn y, (linear_1 = st_l1, linear_2 = st_l2)\nend\n</code></pre> <p>Here, you will notice we have passed <code>(:linear_1, :linear_2)</code> to the supertype. It essentially informs the type that, <code>&lt;obj&gt;.linear_1</code> and <code>&lt;obj&gt;.linear_2</code> are Lux layers and we need to construct parameters and states for those. Let's construct these and see:</p> <pre><code>model = ComposedLinear(Linear(2, 4), Linear(4, 2))\ndisplay(model)\n\nps, st = Lux.setup(rng, model)\n\nprintln(\"Parameters: \", ps)\nprintln(\"States: \", st)\n\nprintln(\"Parameter Length: \", Lux.parameterlength(model), \"; State Length: \",\nLux.statelength(model))\n\nx = randn(rng, Float32, 2, 1)\n\nLux.apply(model, x, ps, st) # or `model(x, ps, st)`\n</code></pre> <pre><code>(Float32[1.3410565; 0.78000563;;], (linear_1 = NamedTuple(), linear_2 = NamedTuple()))\n</code></pre> <p></p> <p></p>"},{"location":"manual/interface/#parameter-interface","title":"Parameter Interface","text":"<p>We accept any parameter type as long as we can fetch the parameters using <code>getproperty(obj, :parameter_name)</code>. This allows us to simulaneously support <code>NamedTuple</code>s and <code>ComponentArray</code>s. Let us go through a concrete example of what it means. Consider <code>Dense</code> which expects two parameters named <code>weight</code> and <code>bias</code>.</p> <p>Note</p> <p>If you are defining your own parameter type, it is your responsibility to make sure that it works with the AutoDiff System you are using.</p> <pre><code>using Lux, Random\n\nd = Dense(2, 3)\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps_default, st = Lux.setup(rng, d)\n\nx = randn(rng, Float32, 2, 1)\n\nprintln(\"Result with `NamedTuple` parameters: \", first(d(x, ps_default, st)))\n</code></pre> <pre><code>Result with `NamedTuple` parameters: Float32[1.135916; 0.7668784; -1.0876652;;]\n</code></pre> <p>Let, us define a custom paramter type with fields <code>myweight</code> and <code>mybias</code> but if we try to access <code>weight</code> we get back <code>myweight</code>, similar for <code>bias</code>.</p> <p>Warning</p> <p>This is for demonstrative purposes, don't try this at home!</p> <pre><code>struct DenseLayerParameters{W, B}\nmyweight::W\nmybias::B\nend\n\nfunction Base.getproperty(ps::DenseLayerParameters, x::Symbol)\nif x == :weight\nreturn getfield(ps, :myweight)\nelseif x == :bias\nreturn getfield(ps, :mybias)\nend\nreturn getfield(ps, x)\nend\n\nps = DenseLayerParameters(ps_default.weight, ps_default.bias)\n\nprintln(\"Result with `DenseLayerParameters` parameters: \", first(d(x, ps, st)))\n</code></pre> <pre><code>Result with `DenseLayerParameters` parameters: Float32[1.135916; 0.7668784; -1.0876652;;]\n</code></pre> <p>The takeaway from this shouldn't be \u2013 lets define weird parameter types. Simply because you can do weird things like this doesn't mean you should, since it only leads to bugs.</p> <p>Instead this shows the flexibility you have for how your parameters can be structured.</p> <p></p> <p></p>"},{"location":"manual/interface/#state-interface","title":"State Interface","text":"<p>States are always type constrained to be <code>NamedTuple</code>. The structure of the input state must match that of the output state, i.e. <code>keys(st_in) == keys(st_out)</code>. This doesn't imply that types of the input and output state match. To generate efficient code, we often do dispatch on the state, for example, <code>Dropout</code>, <code>BatchNorm</code>, etc.</p>"},{"location":"manual/migrate_from_flux/","title":"Migrating from Flux to Lux","text":""},{"location":"manual/migrate_from_flux/#migrating-from-flux-to-lux","title":"Migrating from Flux to Lux","text":"<p>For the core library layers like <code>Dense</code>, <code>Conv</code>, etc. we have intentionlly kept the API very similar to Flux. In most cases, replacing <code>using Flux</code> with <code>using Lux</code> should be enough to get you started. We cover the additional changes that you will have to make in the following example.</p> LuxFlux <pre><code>using Lux, Random, NNlib, Zygote\nmodel = Chain(Dense(2 =&gt; 4), BatchNorm(4, relu), Dense(4 =&gt; 2))\nrng = Random.default_rng()\nx = randn(rng, Float32, 2, 4)\n\nps, st = Lux.setup(rng, model)\nmodel(x, ps, st)\ngradient(ps -&gt; sum(first(model(x, ps, st))), ps)\n</code></pre> <pre><code>using Flux, Random, NNlib, Zygote\n\nmodel = Chain(Dense(2 =&gt; 4), BatchNorm(4, relu), Dense(4 =&gt; 2))\nrng = Random.default_rng()\nx = randn(rng, Float32, 2, 4)\n\n\n\nmodel(x)\n\ngradient(model -&gt; sum(model(x)), model)\n</code></pre> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#implementing-custom-layers","title":"Implementing Custom Layers","text":"<p>Flux and Lux operate under extremely different design philosophies regarding how layers should be implemented. A summary of the differences would be:</p> <ul> <li>Flux stores everything in a single struct and relies on <code>Functors.@functor</code> and <code>Flux.trainable</code> to distinguish between trainable and non-trainable parameters.</li> <li>Lux relies on the user to define <code>Lux.initialparameters</code> and <code>Lux.initialstates</code> to distinguish between trainable parameters (called \"parameters\") and non-trainable parameters (called \"states\"). Additionally Lux layers define the model architecture, hence device transfer utilities like <code>gpu</code>, <code>cpu</code>, etc. cannot be applied on Lux layers, instead they need to be applied on the parameters and states.</li> </ul> <p>Let's work through a concrete example to demonstrate this. We will implement a very simple layer that computes \\(A \\times B \\times x\\) where \\(A\\) is not trainable and \\(B\\) is trainable.</p> LuxFlux <pre><code>using Lux, Random, NNlib, Zygote\n\nstruct LuxLinear &lt;: Lux.AbstractExplicitLayer\ninit_A\ninit_B\nend\n\nfunction LuxLinear(A::AbstractArray, B::AbstractArray)\n# Storing Arrays or any mutable structure inside a Lux Layer is not recommended\n# instead we will convert this to a function to perform lazy initialization\nreturn LuxLinear(() -&gt; copy(A), () -&gt; copy(B))\nend\n\n# `B` is a parameter\nLux.initialparameters(rng::AbstractRNG, layer::LuxLinear) = (B=layer.init_B(),)\n\n# `A` is a state\nLux.initialstates(rng::AbstractRNG, layer::LuxLinear) = (A=layer.init_A(),)\n\n(l::LuxLinear)(x, ps, st) = st.A * ps.B * x, st\n</code></pre> <pre><code>using Flux, Random, NNlib, Zygote, Optimisers\n\nstruct FluxLinear\nA\nB\nend\n\n\n\n\n\n\n\n# `A` is not trainable\nOptimisers.trainable(f::FluxLinear) = (B=f.B,)\n\n# Needed so that both `A` and `B` can be transfered between devices\nFlux.@functor FluxLinear\n\n(l::FluxLinear)(x) = l.A * l.B * x\n</code></pre> <p>Now let us run the model.</p> LuxFlux <pre><code>rng = Random.default_rng()\nmodel = LuxLinear(randn(rng, 2, 4), randn(rng, 4, 2))\nx = randn(rng, 2, 1)\n\nps, st = Lux.setup(rng, model)\nmodel(x, ps, st)\ngradient(ps -&gt; sum(first(model(x, ps, st))), ps)\n</code></pre> <pre><code>rng = Random.default_rng()\nmodel = FluxLinear(randn(rng, 2, 4), randn(rng, 4, 2))\nx = randn(rng, 2, 1)\n\n\n\nmodel(x)\n\ngradient(model -&gt; sum(model(x)), model)\n</code></pre> <p>To reiterate some of the important points:</p> <ul> <li>Don't store mutables like Arrays inside a Lux Layer.</li> <li>Parameters and States should be constructured inside the respective <code>initial*</code> functions.</li> </ul> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#certain-important-implementation-details","title":"Certain Important Implementation Details","text":""},{"location":"manual/migrate_from_flux/#traininginference-mode","title":"Training/Inference Mode","text":"<p>Flux supports a mode called <code>:auto</code> which automatically decides if the user is training the model or running inference. This is the default mode for <code>Flux.BatchNorm</code>, <code>Flux.GroupNorm</code>, <code>Flux.Dropout</code>, etc. Lux doesn't support this mode (specifically to keep code simple and do exactly what the user wants), hence our default mode is <code>training</code>. This can be changed using <code>Lux.testmode</code>.</p> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#cant-access-functions-like-relu-sigmoid-etc","title":"Can't access functions like <code>relu</code>, <code>sigmoid</code>, etc?","text":"<p>Unlike Flux we don't reexport functionality from <code>NNlib</code>, all you need to do to fix this is add <code>using NNlib</code>.</p> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#missing-some-common-layers-from-flux","title":"Missing some common layers from Flux","text":"<p>Lux is a very new framework, as such we haven't implemented all Layers that are a part of Flux. We are tracking the missing features in this issue, and hope to have them implemented soon. If you really need those functionality check out the next section.</p> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#can-we-still-use-flux-layers","title":"Can we still use Flux Layers?","text":"<p>We don't recommend this method, but here is a way to compose Flux with Lux.</p> <pre><code>using Lux, NNlib, Random, Optimisers\nimport Flux\n\n# Layer Implementation\nstruct FluxCompatLayer{L,I} &lt;: Lux.AbstractExplicitLayer\nlayer::L\ninit_parameters::I\nend\n\nfunction FluxCompatLayer(flayer)\np, re = Optimisers.destructure(flayer)\np_ = copy(p)\nreturn FluxCompatLayer(re, () -&gt; p_)\nend\n\nLux.initialparameters(rng::AbstractRNG, l::FluxCompatLayer) = (p=l.init_parameters(),)\n\n(f::FluxCompatLayer)(x, ps, st) = f.layer(ps.p)(x), st\n\n# Running the model\nfmodel = Flux.Chain(Flux.Dense(3 =&gt; 4, relu), Flux.Dense(4 =&gt; 1))\n\nlmodel = FluxCompatLayer(fmodel)\n\nrng = Random.default_rng()\nx = randn(rng, 3, 1)\n\nps, st = Lux.setup(rng, lmodel)\n\nlmodel(x, ps, st)[1] == fmodel(x)\n</code></pre>"},{"location":"manual/precompilation/","title":"Controlling Precompilation","text":""},{"location":"manual/precompilation/#controlling-snoop-precompilation","title":"Controlling Snoop Precompilation","text":"<p>Starting from <code>v0.4.27</code>, <code>Lux</code> uses SnoopPrecompile.jl to precompile certain use cases of <code>Lux</code>. This has significantly reduces Time to First Gradient (TTFG) for Zygote. However, this increases the precompilation time by a lot.</p> <p></p> <p></p>"},{"location":"manual/precompilation/#benefits-of-precompilation","title":"Benefits of Precompilation","text":"<p>Before showing how to disable precompilation, let's first make a case for snoop precompile (after all it is enabled by default).</p> <p></p> <p></p>"},{"location":"manual/precompilation/#with-snoop-precompilation-enabled","title":"With Snoop Precompilation Enabled","text":"<pre><code>julia&gt; using Boltz, Lux, Zygote\n\njulia&gt; m, ps, st = resnet(:resnet18);\n\njulia&gt; x = randn(Float32, 224, 224, 3, 2);\n\njulia&gt; @time m(x, ps, st);\n  3.437890 seconds (4.91 M allocations: 774.738 MiB, 2.67% gc time, 91.46% compilation time)\n\njulia&gt; @time Zygote.gradient(p -&gt; sum(first(m(x, p, st))), ps);\n  83.417719 seconds (80.84 M allocations: 5.780 GiB, 19.74% gc time, 98.17% compilation time: 1% of which was recompilation)\n</code></pre>"},{"location":"manual/precompilation/#with-snoop-precompilation-disabled","title":"With Snoop Precompilation Disabled","text":"<pre><code>julia&gt; using Boltz, Lux, Zygote\n\njulia&gt; m, ps, st = resnet(:resnet18);\n\njulia&gt; x = randn(Float32, 224, 224, 3, 2);\n\njulia&gt; @time m(x, ps, st);\n  5.471836 seconds (16.64 M allocations: 1.316 GiB, 6.75% gc time, 93.26% compilation time)\n\njulia&gt; @time Zygote.gradient(p -&gt; sum(first(m(x, p, st))), ps);\n  100.497466 seconds (95.42 M allocations: 6.467 GiB, 2.49% gc time, 99.05% compilation time)\n</code></pre>"},{"location":"manual/precompilation/#disabling-precompilation","title":"Disabling Precompilation","text":"<p>You can use Preferences.jl to control the \"amount\" of precompilation. If you want to completely disable precompilation, you can set <code>LuxSnoopPrecompile</code> to <code>false</code> in your <code>LocalPreferences.toml</code> file. This can be done using the following command:</p> <pre><code>using Preferences, UUIDs\n\nPreferences.@set_preferences!(UUID(\"b2108857-7c20-44ae-9111-449ecde12c47\"),\n\"LuxSnoopPrecompile\", false)\n# Preferences.@set_preferences!(UUID(\"b2108857-7c20-44ae-9111-449ecde12c47\"),\n#                               \"LuxPrecompileComponentArrays\", false)\n</code></pre> <p>If <code>LuxSnoopPrecompile</code> is set to <code>false</code>, then <code>Lux</code> will not use <code>SnoopPrecompile.jl</code>:</p> <pre><code>julia&gt; @time_imports using Lux\n\n    119.0 ms  Lux 6.41% compilation time\n</code></pre> <p>The other option is to just disable compilation of <code>ComponentArrays.jl</code> codepaths. This is desirable if you are not planning to use Lux with any of the SciML Packages. This can be done by setting <code>LuxPrecompileComponentArrays</code> to <code>false</code>:</p> <pre><code>julia&gt; @time_imports using Lux\n\n    3366.4 ms  Lux 0.22% compilation time\n</code></pre> <p>If you have both the <code>LuxSnoopPrecompile</code> and <code>LuxPrecompileComponentArrays</code> set to <code>true</code>:</p> <pre><code>julia&gt; @time_imports using Lux\n\n    5738.5 ms  Lux 0.13% compilation time\n</code></pre>"}]}